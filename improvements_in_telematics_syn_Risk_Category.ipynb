{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPELCOsGCnZaOjF0BV3BfLA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/improvements_in_telematics_syn_Risk_Category.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "x5lJ2a0T2MRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('telematics_syn_updated.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Risk_Category', axis=1)\n",
        "y = data['Risk_Category']\n",
        "\n",
        "# Identify columns with non-numerical data\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply label encoding to categorical features\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le  # Store the encoders for later use if needed\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert resampled data to a DataFrame\n",
        "balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Risk_Category'])], axis=1)\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(Counter(balanced_data['Risk_Category']))\n",
        "\n",
        "# Save the balanced dataset\n",
        "balanced_data.to_csv('balanced_dataset.csv', index=False)\n",
        "# Count the occurrences of 1's and 0's in Risk_Category\n",
        "claim_yn_counts = df['Risk_Category'].value_counts()\n",
        "\n",
        "print(claim_yn_counts)# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncAvdtmtao7d",
        "outputId": "bc04a673-bfdf-4b0a-9c62-4855d0e1a26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "# from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from collections import Counter\n",
        "import torch\n",
        "\n",
        "# Load the dataset from Google Drive\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "GvlKGGEif1eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a binary Risk Category label\n",
        "df['Risk_Category'] = np.where(df['NB_Claim'] == 0, 'Low Risk', 'High Risk')\n",
        "\n",
        "# Save the updated dataset\n",
        "df.to_csv('telematics_syn_updated.csv', index=False)\n",
        "\n",
        "# Display the first few rows to verify the new column\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "eMJbraq98Wze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7890564b-3f9c-4b11-ca5c-af1c867ce976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
            "0       366           45        Male       -1  Married  Commute         609.0   \n",
            "1       182           44      Female        3  Married  Commute         575.0   \n",
            "2       184           48      Female        6  Married  Commute         847.0   \n",
            "3       183           71        Male        6  Married  Private         842.0   \n",
            "4       183           84        Male       10  Married  Private         856.0   \n",
            "\n",
            "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity11  \\\n",
            "0  Urban             6213.71              25  ...                    0.0   \n",
            "1  Urban            12427.42              20  ...                   24.0   \n",
            "2  Urban            12427.42              14  ...                    0.0   \n",
            "3  Urban             6213.71              43  ...                    0.0   \n",
            "4  Urban             6213.71              65  ...                    0.0   \n",
            "\n",
            "   Left.turn.intensity12  Right.turn.intensity08  Right.turn.intensity09  \\\n",
            "0                    0.0                     3.0                     1.0   \n",
            "1                   11.0                  1099.0                   615.0   \n",
            "2                    0.0                     0.0                     0.0   \n",
            "3                    0.0                     0.0                     0.0   \n",
            "4                    0.0                   325.0                   111.0   \n",
            "\n",
            "   Right.turn.intensity10  Right.turn.intensity11  Right.turn.intensity12  \\\n",
            "0                     0.0                     0.0                     0.0   \n",
            "1                   219.0                   101.0                    40.0   \n",
            "2                     0.0                     0.0                     0.0   \n",
            "3                     0.0                     0.0                     0.0   \n",
            "4                    18.0                     4.0                     2.0   \n",
            "\n",
            "   NB_Claim    AMT_Claim  Risk_Category  \n",
            "0         1  5100.171753      High Risk  \n",
            "1         1   883.554840      High Risk  \n",
            "2         0     0.000000       Low Risk  \n",
            "3         0     0.000000       Low Risk  \n",
            "4         0     0.000000       Low Risk  \n",
            "\n",
            "[5 rows x 53 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('telematics_syn_updated.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('Risk_Category', axis=1)\n",
        "y = data['Risk_Category']\n",
        "\n",
        "# Identify columns with non-numerical data\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Apply label encoding to categorical features\n",
        "label_encoders = {}\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "    label_encoders[col] = le  # Store the encoders for later use if needed\n",
        "\n",
        "# Apply SMOTE to balance the dataset\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Convert resampled data to a DataFrame\n",
        "balanced_data = pd.concat([pd.DataFrame(X_resampled, columns=X.columns), pd.DataFrame(y_resampled, columns=['Risk_Category'])], axis=1)\n",
        "\n",
        "# Check the distribution of the target variable\n",
        "print(Counter(balanced_data['Risk_Category']))\n",
        "\n",
        "# Save the balanced dataset\n",
        "balanced_data.to_csv('balanced_dataset.csv', index=False)\n",
        "# Count the occurrences of 1's and 0's in Risk_Category\n",
        "claim_yn_counts = df['Risk_Category'].value_counts()\n",
        "\n",
        "print(claim_yn_counts)"
      ],
      "metadata": {
        "id": "rULBiSrZ8t-v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df2ad0b-4660-4794-db2a-7ebd6637b5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'High Risk': 95728, 'Low Risk': 95728})\n",
            "Risk_Category\n",
            "Low Risk     95728\n",
            "High Risk     4272\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the balanced dataset\n",
        "balanced_data = pd.read_csv('balanced_dataset.csv')\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory']\n",
        "numerical_cols = [col for col in balanced_data.columns if col not in categorical_cols + ['Risk_Category']]\n",
        "\n",
        "# Preprocess the data: one-hot encode categorical variables and normalize numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Separate features and labels\n",
        "X = balanced_data.drop('Risk_Category', axis=1)\n",
        "y = balanced_data['Risk_Category']\n",
        "\n",
        "# Apply preprocessing\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the data into training, validation, and testing sets (60% train, 20% validation, 20% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_preprocessed, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Check the shape of the splits to ensure they are correct\n",
        "print(f'Training set shape: {X_train.shape}, {y_train.shape}')\n",
        "print(f'Validation set shape: {X_val.shape}, {y_val.shape}')\n",
        "print(f'Testing set shape: {X_test.shape}, {y_test.shape}')\n",
        "\n",
        "# Save the preprocessed data to CSV files if needed\n",
        "train_data = pd.DataFrame(X_train, columns=preprocessor.get_feature_names_out())\n",
        "train_data['Risk_Category'] = y_train.values\n",
        "train_data.to_csv('train_data.csv', index=False)\n",
        "\n",
        "val_data = pd.DataFrame(X_val, columns=preprocessor.get_feature_names_out())\n",
        "val_data['Risk_Category'] = y_val.values\n",
        "val_data.to_csv('val_data.csv', index=False)\n",
        "\n",
        "test_data = pd.DataFrame(X_test, columns=preprocessor.get_feature_names_out())\n",
        "test_data['Risk_Category'] = y_test.values\n",
        "test_data.to_csv('test_data.csv', index=False)"
      ],
      "metadata": {
        "id": "KZ_cQ42484Jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fd9cba3-eada-4f82-f908-235644595407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape: (114873, 138), (114873,)\n",
            "Validation set shape: (38291, 138), (38291,)\n",
            "Testing set shape: (38292, 138), (38292,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0onHpbkRVVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b072852f-8e2d-4e14-92d8-330eb5ac4e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.05836 | train_accuracy: 0.99812 | valid_accuracy: 0.99815 |  0:00:05s\n",
            "epoch 10 | loss: 0.00302 | train_accuracy: 0.99993 | valid_accuracy: 0.9997  |  0:01:06s\n",
            "\n",
            "Early stopping occurred at epoch 18 with best_epoch = 8 and best_valid_accuracy = 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9998\n",
            "Validation F1 Score: 0.9998\n",
            "Successfully saved model at tabnet_model.zip\n",
            "Model saved to: tabnet_model.zip\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import torch\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('telematics_syn_updated.csv')\n",
        "\n",
        "# Create Risk_Category based on the conditions\n",
        "df['Risk_Category'] = np.where((df['NB_Claim'] > 1) & (df['AMT_Claim'] > 1000), 1, 0)\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = ['Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory']\n",
        "numerical_cols = [col for col in df.columns if col not in categorical_cols + ['Risk_Category']]\n",
        "\n",
        "# Preprocess the data: one-hot encode categorical variables and normalize numerical features\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop('Risk_Category', axis=1)\n",
        "y = df['Risk_Category']\n",
        "\n",
        "# Apply preprocessing\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Split the data into training, validation, and testing sets (60% train, 20% validation, 20% test)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X_preprocessed, y, test_size=0.4, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Initialize TabNet model\n",
        "tabnet_model = TabNetClassifier(\n",
        "    n_d=64, n_a=64, n_steps=5,\n",
        "    gamma=1.5, n_independent=2, n_shared=2,\n",
        "    lambda_sparse=1e-3, optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type=\"entmax\",\n",
        "    scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.9),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "    seed=42,\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "tabnet_model.fit(\n",
        "    X_train=X_train, y_train=y_train,\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "    eval_name=['train', 'valid'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=100,\n",
        "    patience=10,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    drop_last=False\n",
        ")\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_val_pred = tabnet_model.predict(X_val)\n",
        "\n",
        "# Calculate accuracy and F1 score\n",
        "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "val_f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "saving_path_name = \"tabnet_model\"\n",
        "saved_filepath = tabnet_model.save_model(saving_path_name)\n",
        "print(f\"Model saved to: {saved_filepath}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6hrTd_1G2ZCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Update NOV 10"
      ],
      "metadata": {
        "id": "4dMQSLOm2aiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "y-h4BVk618SW",
        "outputId": "a68c8551-9768-4904-fe08-d83acf93596b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzWJoWoRbKZm",
        "outputId": "4ef1b132-726f-4bd2-e477-589964cd0c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.5.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch_tabnet) (4.66.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch_tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch_tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch_tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch_tabnet) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch\n",
        "import joblib\n",
        "from typing import Tuple, Dict\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class InsuranceRiskClassifier:\n",
        "    def __init__(self):\n",
        "        self.categorical_cols = ['Insured.sex', 'Marital', 'Car.use', 'Region', 'Territory']\n",
        "        self.target_col = 'Risk_Category'\n",
        "        self.preprocessor = None\n",
        "        self.tabnet_model = None\n",
        "        self.svm_model = None\n",
        "        self.label_encoders = {}\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load and preprocess the dataset.\"\"\"\n",
        "        logging.info(\"Loading and preprocessing data...\")\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Create risk category based on claims\n",
        "        df[self.target_col] = np.where(\n",
        "            (df['NB_Claim'] > 1) & (df['AMT_Claim'] > 1000),\n",
        "            'High Risk',\n",
        "            'Low Risk'\n",
        "        )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Prepare features for modeling.\"\"\"\n",
        "        logging.info(\"Preparing features...\")\n",
        "\n",
        "        # Identify numerical columns\n",
        "        numerical_cols = [col for col in df.columns\n",
        "                         if col not in self.categorical_cols + [self.target_col]]\n",
        "\n",
        "        # Create preprocessor if it doesn't exist\n",
        "        if self.preprocessor is None:\n",
        "            self.preprocessor = ColumnTransformer(\n",
        "                transformers=[\n",
        "                    ('num', StandardScaler(), numerical_cols),\n",
        "                    ('cat', OneHotEncoder(drop='first'), self.categorical_cols)\n",
        "                ])\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df.drop(self.target_col, axis=1)\n",
        "        y = df[self.target_col]\n",
        "\n",
        "        # Apply preprocessing\n",
        "        X_processed = self.preprocessor.fit_transform(X)\n",
        "\n",
        "        return X_processed, y\n",
        "\n",
        "    def apply_smote(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Apply SMOTE to balance the dataset.\"\"\"\n",
        "        logging.info(\"Applying SMOTE for class balance...\")\n",
        "        smote = SMOTE(random_state=42)\n",
        "        return smote.fit_resample(X, y)\n",
        "\n",
        "    def split_data(self, X: np.ndarray, y: np.ndarray) -> Tuple:\n",
        "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
        "        logging.info(\"Splitting data into train, validation, and test sets...\")\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            X, y, test_size=0.4, random_state=42, stratify=y\n",
        "        )\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "        )\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def train_tabnet(self, X_train: np.ndarray, y_train: np.ndarray,\n",
        "                     X_val: np.ndarray, y_val: np.ndarray) -> None:\n",
        "        \"\"\"Train TabNet model.\"\"\"\n",
        "        logging.info(\"Training TabNet model...\")\n",
        "        self.tabnet_model = TabNetClassifier(\n",
        "            n_d=64, n_a=64, n_steps=5,\n",
        "            gamma=1.5, n_independent=2, n_shared=2,\n",
        "            lambda_sparse=1e-3, optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params=dict(lr=2e-2),\n",
        "            mask_type=\"entmax\",\n",
        "            scheduler_params=dict(mode=\"min\", patience=5, min_lr=1e-5, factor=0.9),\n",
        "            scheduler_fn=torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
        "            seed=42,\n",
        "            verbose=10\n",
        "        )\n",
        "\n",
        "        self.tabnet_model.fit(\n",
        "            X_train=X_train, y_train=y_train,\n",
        "            eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "            eval_name=['train', 'valid'],\n",
        "            eval_metric=['accuracy'],\n",
        "            max_epochs=100,\n",
        "            patience=10,\n",
        "            batch_size=1024,\n",
        "            virtual_batch_size=128,\n",
        "            num_workers=0,\n",
        "            drop_last=False\n",
        "        )\n",
        "\n",
        "    def train_svm(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
        "        \"\"\"Train SVM model.\"\"\"\n",
        "        logging.info(\"Training SVM model...\")\n",
        "        self.svm_model = SVC(\n",
        "            kernel='rbf',\n",
        "            C=1.0,\n",
        "            probability=True,\n",
        "            random_state=42,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "        self.svm_model.fit(X_train, y_train)\n",
        "\n",
        "    def evaluate_models(self, X_val: np.ndarray, y_val: np.ndarray) -> Dict:\n",
        "        \"\"\"Evaluate both models on validation set.\"\"\"\n",
        "        logging.info(\"Evaluating models...\")\n",
        "        results = {}\n",
        "\n",
        "        # Evaluate TabNet\n",
        "        y_pred_tabnet = self.tabnet_model.predict(X_val)\n",
        "        results['tabnet'] = {\n",
        "            'accuracy': accuracy_score(y_val, y_pred_tabnet),\n",
        "            'f1': f1_score(y_val, y_pred_tabnet, average='weighted'),\n",
        "            'report': classification_report(y_val, y_pred_tabnet)\n",
        "        }\n",
        "\n",
        "        # Evaluate SVM\n",
        "        y_pred_svm = self.svm_model.predict(X_val)\n",
        "        results['svm'] = {\n",
        "            'accuracy': accuracy_score(y_val, y_pred_svm),\n",
        "            'f1': f1_score(y_val, y_pred_svm, average='weighted'),\n",
        "            'report': classification_report(y_val, y_pred_svm)\n",
        "        }\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_models(self, save_dir: str) -> None:\n",
        "        \"\"\"Save trained models and preprocessor.\"\"\"\n",
        "        logging.info(\"Saving models...\")\n",
        "        self.tabnet_model.save_model(f\"{save_dir}/tabnet_model\")\n",
        "        joblib.dump(self.svm_model, f\"{save_dir}/svm_model.joblib\")\n",
        "        joblib.dump(self.preprocessor, f\"{save_dir}/preprocessor.joblib\")\n",
        "\n",
        "def main():\n",
        "    # Initialize classifier\n",
        "    classifier = InsuranceRiskClassifier()\n",
        "\n",
        "    # Load and process data\n",
        "    df = classifier.load_and_preprocess_data('/content/drive/My Drive/telematics_syn.csv')\n",
        "\n",
        "    # Prepare features\n",
        "    X, y = classifier.prepare_features(df)\n",
        "\n",
        "    # Apply SMOTE\n",
        "    X_balanced, y_balanced = classifier.apply_smote(X, y)\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_val, X_test, y_train, y_val, y_test = classifier.split_data(X_balanced, y_balanced)\n",
        "\n",
        "    # Train models\n",
        "    classifier.train_tabnet(X_train, y_train, X_val, y_val)\n",
        "    classifier.train_svm(X_train, y_train)\n",
        "\n",
        "    # Evaluate models\n",
        "    results = classifier.evaluate_models(X_val, y_val)\n",
        "\n",
        "    # Print results\n",
        "    for model_name, metrics in results.items():\n",
        "        print(f\"\\n{model_name.upper()} Results:\")\n",
        "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "        print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(metrics['report'])\n",
        "\n",
        "    # Save models\n",
        "    classifier.save_models('/content/drive/My Drive/models')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "hz_PldcfaFNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9511b620-6adf-46cf-ff96-1bf3884f8137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.10216 | train_accuracy: 0.99951 | valid_accuracy: 0.99965 |  0:00:13s\n",
            "epoch 10 | loss: 0.00182 | train_accuracy: 0.99963 | valid_accuracy: 0.99982 |  0:02:16s\n",
            "\n",
            "Early stopping occurred at epoch 17 with best_epoch = 7 and best_valid_accuracy = 0.9999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TABNET Results:\n",
            "Accuracy: 0.9999\n",
            "F1 Score: 0.9999\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   High Risk       1.00      1.00      1.00     19964\n",
            "    Low Risk       1.00      1.00      1.00     19965\n",
            "\n",
            "    accuracy                           1.00     39929\n",
            "   macro avg       1.00      1.00      1.00     39929\n",
            "weighted avg       1.00      1.00      1.00     39929\n",
            "\n",
            "\n",
            "SVM Results:\n",
            "Accuracy: 0.9999\n",
            "F1 Score: 0.9999\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   High Risk       1.00      1.00      1.00     19964\n",
            "    Low Risk       1.00      1.00      1.00     19965\n",
            "\n",
            "    accuracy                           1.00     39929\n",
            "   macro avg       1.00      1.00      1.00     39929\n",
            "weighted avg       1.00      1.00      1.00     39929\n",
            "\n",
            "Successfully saved model at /content/drive/My Drive/models/tabnet_model.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import torch\n",
        "import joblib\n",
        "from typing import Tuple, Dict\n",
        "import logging\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Define the InsuranceRiskClassifier class\n",
        "class InsuranceRiskClassifier:\n",
        "    # [Paste the entire class definition here from the previous response]\n",
        "    pass\n",
        "\n",
        "# Create classifier instance and run the analysis\n",
        "classifier = InsuranceRiskClassifier()\n",
        "\n",
        "# Load and process data\n",
        "df = classifier.load_and_preprocess_data('/content/drive/My Drive/telematics_syn.csv')\n",
        "\n",
        "# Prepare features\n",
        "X, y = classifier.prepare_features(df)\n",
        "\n",
        "# Apply SMOTE\n",
        "X_balanced, y_balanced = classifier.apply_smote(X, y)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = classifier.split_data(X_balanced, y_balanced)\n",
        "\n",
        "# Train models\n",
        "classifier.train_tabnet(X_train, y_train, X_val, y_val)\n",
        "classifier.train_svm(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "results = classifier.evaluate_models(X_val, y_val)\n",
        "\n",
        "# Print results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"\\n{model_name.upper()} Results:\")\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(metrics['report'])\n",
        "\n",
        "# Save models\n",
        "classifier.save_models('/content/drive/My Drive/models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "cQvUOJTX4r2Y",
        "outputId": "23517dec-d0c0-466d-944a-378276f75980"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'InsuranceRiskClassifier' object has no attribute 'save_models'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-3576847392ae>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'InsuranceRiskClassifier' object has no attribute 'save_models'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from insurance_risk_classifier import InsuranceRiskClassifier\n",
        "classifier = InsuranceRiskClassifier()\n",
        "# Follow the main() function steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "PPw_ma6e4Xog",
        "outputId": "90be6b93-db3f-4b41-a1ad-6a6a0b4af0e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'insurance_risk_classifier'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d7b02980677f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0minsurance_risk_classifier\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInsuranceRiskClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInsuranceRiskClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Follow the main() function steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'insurance_risk_classifier'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    }
  ]
}