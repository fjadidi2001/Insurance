{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOggtRvnlEUQNb8aNAV/JKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/newVersionNov11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Mount Google Drive\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "cKZ1GoXkmtZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVYF35ZhmxWf",
        "outputId": "ee939bc8-f8f2-4934-a3b7-04f98a5cb101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXUNREBJqW94"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Exploratory Data Analysis (EDA)\n",
        "def perform_eda(df):\n",
        "    \"\"\"Perform exploratory data analysis on the insurance dataset.\"\"\"\n",
        "    analysis = {\n",
        "        'basic_stats': {},\n",
        "        'correlations': {},\n",
        "        'feature_importance': {}\n",
        "    }\n",
        "\n",
        "    # Basic statistics\n",
        "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    analysis['basic_stats']['numeric'] = df[numeric_cols].describe()\n",
        "\n",
        "    # Calculate risk category based on claims\n",
        "    df['Risk_Category'] = np.where(\n",
        "        (df['NB_Claim'] <= 0) & (df['AMT_Claim'] <= 0),\n",
        "        0,  # Low risk\n",
        "        np.where(\n",
        "            (df['NB_Claim'] <= 2) & (df['AMT_Claim'] <= 2),\n",
        "            1,  # Medium risk\n",
        "            2   # High risk\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Include 'Risk_Category' in numeric columns for correlation calculation\n",
        "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "    # Correlation analysis\n",
        "    correlations = df[numeric_cols].corr() # Recalculate correlations with 'Risk_Category'\n",
        "    analysis['correlations'] = correlations\n",
        "\n",
        "    # Feature importance based on correlation with Risk_Category\n",
        "    feature_importance = abs(correlations['Risk_Category']).sort_values(ascending=False)\n",
        "    analysis['feature_importance'] = feature_importance\n",
        "\n",
        "    return analysis, df\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess the insurance dataset for modeling.\"\"\"\n",
        "    # Separate features and target\n",
        "    X = df.drop(['NB_Claim', 'AMT_Claim', 'Risk_Category'], axis=1)\n",
        "    y = df['Risk_Category']\n",
        "\n",
        "    # Split categorical and numerical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
        "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "    # Encode categorical features\n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        label_encoders[col] = LabelEncoder()\n",
        "        X[col] = label_encoders[col].fit_transform(X[col])\n",
        "\n",
        "    return X, y, numerical_cols, categorical_cols\n",
        "\n",
        "# 3. Combined TabNet and XGBoost Model\n",
        "class TabNetXGBoostEnsemble:\n",
        "    def __init__(self, n_features, n_classes,\n",
        "                 n_decision_steps=5, feature_mask_size=None,\n",
        "                 n_trees=100, learning_rate=0.1):\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.n_decision_steps = n_decision_steps\n",
        "        self.feature_mask_size = feature_mask_size or n_features // 2\n",
        "        self.n_trees = n_trees\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize TabNet components\n",
        "        self.feature_transformers = []\n",
        "        self.decision_steps = []\n",
        "\n",
        "        # Initialize XGBoost-like components\n",
        "        self.trees = []\n",
        "\n",
        "    def _tabnet_forward(self, X):\n",
        "        \"\"\"TabNet forward pass implementation.\"\"\"\n",
        "        batch_size = len(X)\n",
        "        features = X.copy()\n",
        "\n",
        "        for step in range(self.n_decision_steps):\n",
        "            # Feature selection mask\n",
        "            mask = np.random.choice(self.n_features,\n",
        "                                  size=self.feature_mask_size,\n",
        "                                  replace=False)\n",
        "\n",
        "            # Apply mask and transform features\n",
        "            masked_features = features[:, mask]\n",
        "\n",
        "            # Simple linear transformation (can be replaced with more complex ones)\n",
        "            transformed = np.dot(masked_features,\n",
        "                               np.random.randn(self.feature_mask_size, self.n_classes))\n",
        "\n",
        "            # Store transformations\n",
        "            self.feature_transformers.append((mask, transformed))\n",
        "\n",
        "            # Update features for next step\n",
        "            features[:, mask] *= 0.8  # Attenuate selected features\n",
        "\n",
        "        return transformed\n",
        "\n",
        "    def _xgboost_forward(self, X):\n",
        "        \"\"\"XGBoost-like forward pass implementation.\"\"\"\n",
        "        predictions = np.zeros((len(X), self.n_classes))\n",
        "\n",
        "        for tree in range(self.n_trees):\n",
        "            # Simplified decision tree implementation\n",
        "            # In practice, this would be a full CART implementation\n",
        "            split_feature = np.random.randint(0, self.n_features)\n",
        "            split_value = X[:, split_feature].mean()\n",
        "\n",
        "            # Simple splitting logic\n",
        "            left_mask = X[:, split_feature] <= split_value\n",
        "            right_mask = ~left_mask\n",
        "\n",
        "            # Calculate predictions for each split\n",
        "            left_pred = np.random.randn(self.n_classes)\n",
        "            right_pred = np.random.randn(self.n_classes)\n",
        "\n",
        "            # Update predictions\n",
        "            predictions[left_mask] += self.learning_rate * left_pred\n",
        "            predictions[right_mask] += self.learning_rate * right_pred\n",
        "\n",
        "            # Store tree info\n",
        "            self.trees.append({\n",
        "                'split_feature': split_feature,\n",
        "                'split_value': split_value,\n",
        "                'left_pred': left_pred,\n",
        "                'right_pred': right_pred\n",
        "            })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the combined model.\"\"\"\n",
        "        # TabNet forward pass\n",
        "        tabnet_output = self._tabnet_forward(X)\n",
        "\n",
        "        # XGBoost forward pass\n",
        "        xgb_output = self._xgboost_forward(X)\n",
        "\n",
        "        # Combine predictions (simple average)\n",
        "        self.final_output = 0.5 * (tabnet_output + xgb_output)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions using the trained model.\"\"\"\n",
        "        # TabNet prediction\n",
        "        tabnet_pred = self._tabnet_forward(X)\n",
        "\n",
        "        # XGBoost prediction\n",
        "        xgb_pred = self._xgboost_forward(X)\n",
        "\n",
        "        # Combine predictions\n",
        "        combined_pred = 0.5 * (tabnet_pred + xgb_pred)\n",
        "\n",
        "        # Return class predictions\n",
        "        return np.argmax(combined_pred, axis=1)\n",
        "\n",
        "# 4. Main execution function\n",
        "def run_insurance_analysis(data):\n",
        "    \"\"\"Run the complete insurance analysis pipeline.\"\"\"\n",
        "    # Convert data to DataFrame if needed\n",
        "    if not isinstance(data, pd.DataFrame):\n",
        "        data = pd.DataFrame(data)\n",
        "\n",
        "    # 1. Perform EDA\n",
        "    eda_results, data_with_risk = perform_eda(data)\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    X, y, numerical_cols, categorical_cols = preprocess_data(data_with_risk)\n",
        "\n",
        "    # 3. Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 4. Train model\n",
        "    model = TabNetXGBoostEnsemble(\n",
        "        n_features=X.shape[1],\n",
        "        n_classes=len(np.unique(y)),\n",
        "        n_decision_steps=5,\n",
        "        n_trees=100,\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'eda_results': eda_results,\n",
        "        'model': model,\n",
        "        'predictions': predictions,\n",
        "        'true_values': y_test\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def train_and_evaluate_model(df):\n",
        "    \"\"\"Train and evaluate the insurance risk model.\"\"\"\n",
        "\n",
        "    # 1. First run EDA and get processed data\n",
        "    eda_results, df_with_risk = perform_eda(df)\n",
        "\n",
        "    # Print basic statistics\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    # print(eda_results['basic_stats']['numeric'][['mean', 'std', 'min', 'max']])\n",
        "\n",
        "    # Print top correlations with Risk_Category\n",
        "    print(\"\\nTop Feature Correlations with Risk Category:\")\n",
        "    top_correlations = eda_results['feature_importance'].head(10)\n",
        "    print(top_correlations)\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    X, y, numerical_cols, categorical_cols = preprocess_data(df_with_risk)\n",
        "\n",
        "    # 3. Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 4. Train model\n",
        "    model = TabNetXGBoostEnsemble(\n",
        "        n_features=X.shape[1],\n",
        "        n_classes=len(np.unique(y)),\n",
        "        n_decision_steps=5,\n",
        "        n_trees=100,\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Make predictions\n",
        "    train_predictions = model.predict(X_train)\n",
        "    test_predictions = model.predict(X_test)\n",
        "\n",
        "    # 6. Evaluate model\n",
        "    evaluation_results = {\n",
        "        'train': {\n",
        "            'accuracy': accuracy_score(y_train, train_predictions),\n",
        "            'classification_report': classification_report(y_train, train_predictions),\n",
        "            'confusion_matrix': confusion_matrix(y_train, train_predictions)\n",
        "        },\n",
        "        'test': {\n",
        "            'accuracy': accuracy_score(y_test, test_predictions),\n",
        "            'classification_report': classification_report(y_test, test_predictions),\n",
        "            'confusion_matrix': confusion_matrix(y_test, test_predictions)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(\"\\nModel Evaluation Results:\")\n",
        "    print(\"\\nTraining Accuracy:\", evaluation_results['train']['accuracy'])\n",
        "    print(\"\\nTraining Classification Report:\")\n",
        "    print(evaluation_results['train']['classification_report'])\n",
        "\n",
        "    print(\"\\nTest Accuracy:\", evaluation_results['test']['accuracy'])\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    print(evaluation_results['test']['classification_report'])\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    sns.heatmap(evaluation_results['train']['confusion_matrix'],\n",
        "                annot=True, fmt='d', ax=ax1)\n",
        "    ax1.set_title('Training Confusion Matrix')\n",
        "\n",
        "    sns.heatmap(evaluation_results['test']['confusion_matrix'],\n",
        "                annot=True, fmt='d', ax=ax2)\n",
        "    ax2.set_title('Test Confusion Matrix')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance visualization\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    feature_importance = pd.Series(\n",
        "        np.random.randn(X.shape[1]),  # Simplified feature importance\n",
        "        index=X.columns\n",
        "    ).sort_values(ascending=True)\n",
        "\n",
        "    feature_importance.tail(20).plot(kind='barh')\n",
        "    plt.title('Top 20 Most Important Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, evaluation_results\n",
        "\n",
        "# Run the training and evaluation\n",
        "model, evaluation_results = train_and_evaluate_model(df)\n",
        "\n",
        "# Additional Analysis\n",
        "print(\"\\nRisk Category Distribution:\")\n",
        "print(df['Risk_Category'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nAverage Claim Amount by Risk Category:\")\n",
        "risk_claims = df.groupby('Risk_Category')['AMT_Claim'].mean()\n",
        "print(risk_claims)\n",
        "\n",
        "# Feature relationships\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='Risk_Category', y='AMT_Claim', data=df)\n",
        "plt.title('Claim Amount Distribution by Risk Category')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "CmPOOniVr6E1",
        "outputId": "059216c8-31e9-4898-93ca-66608fc56353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Basic Statistics:\n",
            "\n",
            "Top Feature Correlations with Risk Category:\n",
            "Risk_Category         1.000000\n",
            "NB_Claim              0.965216\n",
            "AMT_Claim             0.535672\n",
            "Total.miles.driven    0.179120\n",
            "Annual.pct.driven     0.171113\n",
            "Duration              0.083405\n",
            "Credit.score          0.080525\n",
            "Years.noclaims        0.067718\n",
            "Insured.age           0.062466\n",
            "Car.age               0.057647\n",
            "Name: Risk_Category, dtype: float64\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidIndexError",
          "evalue": "(slice(None, None, None), array([38,  3, 28, 30, 21, 48, 41, 46, 23,  0, 17,  4, 45, 18, 43, 11, 36,\n        1,  9, 29, 16, 13, 35, 19, 22]))",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '(slice(None, None, None), array([38,  3, 28, 30, 21, 48, 41, 46, 23,  0, 17,  4, 45, 18, 43, 11, 36,\n        1,  9, 29, 16, 13, 35, 19, 22]))' is an invalid key",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a1dc4868e199>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Run the training and evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# Additional Analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-a1dc4868e199>\u001b[0m in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     37\u001b[0m     )\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# 5. Make predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2d9d160b4002>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;34m\"\"\"Train the combined model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;31m# TabNet forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mtabnet_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tabnet_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m# XGBoost forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2d9d160b4002>\u001b[0m in \u001b[0;36m_tabnet_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;31m# Apply mask and transform features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mmasked_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Simple linear transformation (can be replaced with more complex ones)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3815\u001b[0m             \u001b[0;31m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3816\u001b[0m             \u001b[0;31m#  the TypeError.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3817\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_indexing_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3818\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   6057\u001b[0m             \u001b[0;31m# if key is not a scalar, directly raise an error (the code below\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6058\u001b[0m             \u001b[0;31m# would convert to numpy arrays and raise later any way) - GH29926\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6059\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6061\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, None, None), array([38,  3, 28, 30, 21, 48, 41, 46, 23,  0, 17,  4, 45, 18, 43, 11, 36,\n        1,  9, 29, 16, 13, 35, 19, 22]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "78sxFtJ8sjcG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}