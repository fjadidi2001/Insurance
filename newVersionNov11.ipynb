{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPccwu2/yHlLZUR6aeUB5BO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/newVersionNov11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Mount Google Drive\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "cKZ1GoXkmtZ1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVYF35ZhmxWf",
        "outputId": "ee939bc8-f8f2-4934-a3b7-04f98a5cb101"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zXUNREBJqW94"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Exploratory Data Analysis (EDA)\n",
        "def perform_eda(df):\n",
        "    \"\"\"Perform exploratory data analysis on the insurance dataset.\"\"\"\n",
        "    analysis = {\n",
        "        'basic_stats': {},\n",
        "        'correlations': {},\n",
        "        'feature_importance': {}\n",
        "    }\n",
        "\n",
        "    # Basic statistics\n",
        "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "    analysis['basic_stats']['numeric'] = df[numeric_cols].describe()\n",
        "\n",
        "    # Calculate risk category based on claims\n",
        "    df['Risk_Category'] = np.where(\n",
        "        (df['NB_Claim'] <= 0) & (df['AMT_Claim'] <= 0),\n",
        "        0,  # Low risk\n",
        "        np.where(\n",
        "            (df['NB_Claim'] <= 2) & (df['AMT_Claim'] <= 2),\n",
        "            1,  # Medium risk\n",
        "            2   # High risk\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Correlation analysis\n",
        "    correlations = df[numeric_cols].corr()\n",
        "    analysis['correlations'] = correlations\n",
        "\n",
        "    # Feature importance based on correlation with Risk_Category\n",
        "    feature_importance = abs(correlations['Risk_Category']).sort_values(ascending=False)\n",
        "    analysis['feature_importance'] = feature_importance\n",
        "\n",
        "    return analysis, df\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess the insurance dataset for modeling.\"\"\"\n",
        "    # Separate features and target\n",
        "    X = df.drop(['NB_Claim', 'AMT_Claim', 'Risk_Category'], axis=1)\n",
        "    y = df['Risk_Category']\n",
        "\n",
        "    # Split categorical and numerical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'bool']).columns\n",
        "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "    # Scale numerical features\n",
        "    scaler = StandardScaler()\n",
        "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "\n",
        "    # Encode categorical features\n",
        "    label_encoders = {}\n",
        "    for col in categorical_cols:\n",
        "        label_encoders[col] = LabelEncoder()\n",
        "        X[col] = label_encoders[col].fit_transform(X[col])\n",
        "\n",
        "    return X, y, numerical_cols, categorical_cols\n",
        "\n",
        "# 3. Combined TabNet and XGBoost Model\n",
        "class TabNetXGBoostEnsemble:\n",
        "    def __init__(self, n_features, n_classes,\n",
        "                 n_decision_steps=5, feature_mask_size=None,\n",
        "                 n_trees=100, learning_rate=0.1):\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.n_decision_steps = n_decision_steps\n",
        "        self.feature_mask_size = feature_mask_size or n_features // 2\n",
        "        self.n_trees = n_trees\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize TabNet components\n",
        "        self.feature_transformers = []\n",
        "        self.decision_steps = []\n",
        "\n",
        "        # Initialize XGBoost-like components\n",
        "        self.trees = []\n",
        "\n",
        "    def _tabnet_forward(self, X):\n",
        "        \"\"\"TabNet forward pass implementation.\"\"\"\n",
        "        batch_size = len(X)\n",
        "        features = X.copy()\n",
        "\n",
        "        for step in range(self.n_decision_steps):\n",
        "            # Feature selection mask\n",
        "            mask = np.random.choice(self.n_features,\n",
        "                                  size=self.feature_mask_size,\n",
        "                                  replace=False)\n",
        "\n",
        "            # Apply mask and transform features\n",
        "            masked_features = features[:, mask]\n",
        "\n",
        "            # Simple linear transformation (can be replaced with more complex ones)\n",
        "            transformed = np.dot(masked_features,\n",
        "                               np.random.randn(self.feature_mask_size, self.n_classes))\n",
        "\n",
        "            # Store transformations\n",
        "            self.feature_transformers.append((mask, transformed))\n",
        "\n",
        "            # Update features for next step\n",
        "            features[:, mask] *= 0.8  # Attenuate selected features\n",
        "\n",
        "        return transformed\n",
        "\n",
        "    def _xgboost_forward(self, X):\n",
        "        \"\"\"XGBoost-like forward pass implementation.\"\"\"\n",
        "        predictions = np.zeros((len(X), self.n_classes))\n",
        "\n",
        "        for tree in range(self.n_trees):\n",
        "            # Simplified decision tree implementation\n",
        "            # In practice, this would be a full CART implementation\n",
        "            split_feature = np.random.randint(0, self.n_features)\n",
        "            split_value = X[:, split_feature].mean()\n",
        "\n",
        "            # Simple splitting logic\n",
        "            left_mask = X[:, split_feature] <= split_value\n",
        "            right_mask = ~left_mask\n",
        "\n",
        "            # Calculate predictions for each split\n",
        "            left_pred = np.random.randn(self.n_classes)\n",
        "            right_pred = np.random.randn(self.n_classes)\n",
        "\n",
        "            # Update predictions\n",
        "            predictions[left_mask] += self.learning_rate * left_pred\n",
        "            predictions[right_mask] += self.learning_rate * right_pred\n",
        "\n",
        "            # Store tree info\n",
        "            self.trees.append({\n",
        "                'split_feature': split_feature,\n",
        "                'split_value': split_value,\n",
        "                'left_pred': left_pred,\n",
        "                'right_pred': right_pred\n",
        "            })\n",
        "\n",
        "        return predictions\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train the combined model.\"\"\"\n",
        "        # TabNet forward pass\n",
        "        tabnet_output = self._tabnet_forward(X)\n",
        "\n",
        "        # XGBoost forward pass\n",
        "        xgb_output = self._xgboost_forward(X)\n",
        "\n",
        "        # Combine predictions (simple average)\n",
        "        self.final_output = 0.5 * (tabnet_output + xgb_output)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions using the trained model.\"\"\"\n",
        "        # TabNet prediction\n",
        "        tabnet_pred = self._tabnet_forward(X)\n",
        "\n",
        "        # XGBoost prediction\n",
        "        xgb_pred = self._xgboost_forward(X)\n",
        "\n",
        "        # Combine predictions\n",
        "        combined_pred = 0.5 * (tabnet_pred + xgb_pred)\n",
        "\n",
        "        # Return class predictions\n",
        "        return np.argmax(combined_pred, axis=1)\n",
        "\n",
        "# 4. Main execution function\n",
        "def run_insurance_analysis(data):\n",
        "    \"\"\"Run the complete insurance analysis pipeline.\"\"\"\n",
        "    # Convert data to DataFrame if needed\n",
        "    if not isinstance(data, pd.DataFrame):\n",
        "        data = pd.DataFrame(data)\n",
        "\n",
        "    # 1. Perform EDA\n",
        "    eda_results, data_with_risk = perform_eda(data)\n",
        "\n",
        "    # 2. Preprocess data\n",
        "    X, y, numerical_cols, categorical_cols = preprocess_data(data_with_risk)\n",
        "\n",
        "    # 3. Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # 4. Train model\n",
        "    model = TabNetXGBoostEnsemble(\n",
        "        n_features=X.shape[1],\n",
        "        n_classes=len(np.unique(y)),\n",
        "        n_decision_steps=5,\n",
        "        n_trees=100,\n",
        "        learning_rate=0.1\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # 5. Make predictions\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    return {\n",
        "        'eda_results': eda_results,\n",
        "        'model': model,\n",
        "        'predictions': predictions,\n",
        "        'true_values': y_test\n",
        "    }"
      ]
    }
  ]
}