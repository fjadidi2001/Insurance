{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOii9BNnAj6/bXNKqOTajAX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/Insurance_ICCKe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insurance Claims Prediction - Complete ML Pipeline with xLSTM\n",
        "## Step-by-step implementation for telematics data analysis with advanced neural architecture"
      ],
      "metadata": {
        "id": "rmtKD_j4XrtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 1: IMPORT REQUIRED LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Machine Learning Libraries\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Evaluation Metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, matthews_corrcoef, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve, average_precision_score,\n",
        "    classification_report\n",
        ")\n",
        "\n",
        "# Handle Imbalanced Data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Model Interpretation\n",
        "try:\n",
        "    import shap\n",
        "    import lime\n",
        "    import lime.lime_tabular\n",
        "    INTERPRETATION_AVAILABLE = True\n",
        "except ImportError:\n",
        "    INTERPRETATION_AVAILABLE = False\n",
        "    print(\"SHAP and LIME not available. Install with: pip install shap lime\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iARp4QDIXkjO",
        "outputId": "91b0678f-8794-4189-8388-7c7722897bdd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAP and LIME not available. Install with: pip install shap lime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 1.5: xLSTM IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "class xLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Extended LSTM Cell with exponential gating and matrix memory\n",
        "    Based on xLSTM architecture for enhanced sequence modeling\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, memory_size=64):\n",
        "        super(xLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.memory_size = memory_size\n",
        "\n",
        "        # Standard LSTM gates\n",
        "        self.forget_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.input_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.output_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.candidate_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "\n",
        "        # xLSTM extensions\n",
        "        self.exp_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.matrix_memory = nn.Parameter(torch.randn(memory_size, hidden_size))\n",
        "        self.memory_gate = nn.Linear(input_size + hidden_size, memory_size)\n",
        "\n",
        "        # Weight computation layers\n",
        "        self.weight_transform = nn.Linear(hidden_size, hidden_size)\n",
        "        self.weight_attention = nn.MultiheadAttention(hidden_size, num_heads=4)\n",
        "\n",
        "    def forward(self, x, hidden_state, cell_state):\n",
        "        combined = torch.cat([x, hidden_state], dim=1)\n",
        "\n",
        "        # Standard LSTM computations\n",
        "        forget = torch.sigmoid(self.forget_gate(combined))\n",
        "        input_gate_val = torch.sigmoid(self.input_gate(combined))\n",
        "        output = torch.sigmoid(self.output_gate(combined))\n",
        "        candidate = torch.tanh(self.candidate_gate(combined))\n",
        "\n",
        "        # xLSTM extensions\n",
        "        exp_gate_val = torch.exp(self.exp_gate(combined))\n",
        "        memory_weights = torch.softmax(self.memory_gate(combined), dim=1)\n",
        "\n",
        "        # Matrix memory interaction\n",
        "        memory_contribution = torch.mm(memory_weights, self.matrix_memory)\n",
        "\n",
        "        # Enhanced cell state update\n",
        "        cell_state = forget * cell_state + input_gate_val * candidate + 0.1 * memory_contribution\n",
        "        cell_state = cell_state * exp_gate_val  # Exponential gating\n",
        "\n",
        "        # Enhanced hidden state\n",
        "        hidden_state = output * torch.tanh(cell_state)\n",
        "\n",
        "        # Weight computation using attention\n",
        "        weight_features = self.weight_transform(hidden_state).unsqueeze(0)\n",
        "        attended_weights, attention_weights = self.weight_attention(\n",
        "            weight_features, weight_features, weight_features\n",
        "        )\n",
        "\n",
        "        return hidden_state, cell_state, attended_weights.squeeze(0), attention_weights\n",
        "\n",
        "class xLSTMWeightPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    xLSTM-based model for predicting feature weights and insurance claims\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size=128, num_layers=2, memory_size=64):\n",
        "        super(xLSTMWeightPredictor, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Feature embedding\n",
        "        self.feature_embedding = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # xLSTM layers\n",
        "        self.xlstm_cells = nn.ModuleList([\n",
        "            xLSTMCell(hidden_size if i == 0 else hidden_size, hidden_size, memory_size)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Weight prediction layers\n",
        "        self.weight_predictor = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size // 2, input_size),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Classification layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_size // 2, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        # Initialize weights\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, x, return_weights=False):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Feature embedding\n",
        "        embedded = self.feature_embedding(x)\n",
        "\n",
        "        # Initialize hidden and cell states\n",
        "        hidden_states = [torch.zeros(batch_size, self.hidden_size) for _ in range(self.num_layers)]\n",
        "        cell_states = [torch.zeros(batch_size, self.hidden_size) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Pass through xLSTM layers\n",
        "        current_input = embedded\n",
        "        all_attention_weights = []\n",
        "\n",
        "        for i, xlstm_cell in enumerate(self.xlstm_cells):\n",
        "            hidden_states[i], cell_states[i], weighted_features, attention_weights = xlstm_cell(\n",
        "                current_input, hidden_states[i], cell_states[i]\n",
        "            )\n",
        "            current_input = hidden_states[i]\n",
        "            all_attention_weights.append(attention_weights)\n",
        "\n",
        "        # Final hidden state\n",
        "        final_hidden = hidden_states[-1]\n",
        "\n",
        "        # Predict feature weights\n",
        "        feature_weights = self.weight_predictor(final_hidden)\n",
        "\n",
        "        # Classification prediction\n",
        "        classification_output = self.classifier(final_hidden)\n",
        "\n",
        "        if return_weights:\n",
        "            return classification_output, feature_weights, all_attention_weights\n",
        "        return classification_output\n",
        "\n",
        "class xLSTMTrainer:\n",
        "    \"\"\"\n",
        "    Trainer class for xLSTM model\n",
        "    \"\"\"\n",
        "    def __init__(self, model, device='cpu'):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            self.optimizer, mode='min', factor=0.5, patience=10\n",
        "        )\n",
        "        self.criterion = nn.BCELoss()\n",
        "\n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch_x, batch_y in dataloader:\n",
        "            batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            predictions, weights, _ = self.model(batch_x, return_weights=True)\n",
        "\n",
        "            # Classification loss\n",
        "            class_loss = self.criterion(predictions.squeeze(), batch_y.float())\n",
        "\n",
        "            # Weight regularization (encourage sparse, meaningful weights)\n",
        "            weight_reg = torch.mean(torch.sum(weights * torch.log(weights + 1e-8), dim=1))\n",
        "\n",
        "            # Total loss\n",
        "            total_loss_batch = class_loss + 0.01 * weight_reg\n",
        "\n",
        "            total_loss_batch.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            total_loss += total_loss_batch.item()\n",
        "\n",
        "        return total_loss / len(dataloader)\n",
        "\n",
        "    def validate(self, dataloader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in dataloader:\n",
        "                batch_x, batch_y = batch_x.to(self.device), batch_y.to(self.device)\n",
        "\n",
        "                pred, weights, _ = self.model(batch_x, return_weights=True)\n",
        "                loss = self.criterion(pred.squeeze(), batch_y.float())\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                predictions.extend(pred.cpu().numpy())\n",
        "                targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "        return total_loss / len(dataloader), np.array(predictions), np.array(targets)\n",
        "\n",
        "    def train(self, train_loader, val_loader, epochs=100, early_stopping_patience=20):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        best_val_loss = float('inf')\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "\n",
        "            # Validation\n",
        "            val_loss, val_preds, val_targets = self.validate(val_loader)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "            # Learning rate scheduling\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                patience_counter = 0\n",
        "                # Save best model\n",
        "                torch.save(self.model.state_dict(), 'best_xlstm_model.pth')\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                val_auc = roc_auc_score(val_targets, val_preds)\n",
        "                print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}')\n",
        "\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(f'Early stopping at epoch {epoch}')\n",
        "                break\n",
        "\n",
        "        # Load best model\n",
        "        self.model.load_state_dict(torch.load('best_xlstm_model.pth'))\n",
        "\n",
        "        return train_losses, val_losses"
      ],
      "metadata": {
        "id": "0OMg0jjoX2kY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: DATA LOADING AND INITIAL EXPLORATION\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_explore_data(file_path):\n",
        "    \"\"\"\n",
        "    Load the telematics dataset and perform initial exploration\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 2: DATA LOADING AND EXPLORATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Display basic info\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nBasic statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    # Check for missing values\n",
        "    missing_values = df.isnull().sum()\n",
        "    print(f\"\\nMissing values per column:\")\n",
        "    print(missing_values[missing_values > 0])\n",
        "    print(f\"Total missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "    # Check for duplicate rows\n",
        "    duplicates = df.duplicated().sum()\n",
        "    print(f\"Duplicate rows: {duplicates}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: ENHANCED DATA PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Enhanced preprocessing with feature engineering\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 3: ENHANCED DATA PREPROCESSING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Create target variable: Claims with amount > 1000\n",
        "    if 'NB_Claim' in df.columns and 'AMT_Claim' in df.columns:\n",
        "        df['ClaimYN'] = ((df['NB_Claim'] >= 1) & (df['AMT_Claim'] > 1000)).astype(int)\n",
        "        df = df.drop(['NB_Claim', 'AMT_Claim'], axis=1)\n",
        "    else:\n",
        "        # If target already exists, use it\n",
        "        if 'ClaimYN' not in df.columns:\n",
        "            print(\"Warning: No target variable found. Creating dummy target.\")\n",
        "            df['ClaimYN'] = np.random.binomial(1, 0.3, len(df))\n",
        "\n",
        "    print(f\"Target variable distribution:\")\n",
        "    print(df['ClaimYN'].value_counts())\n",
        "    print(f\"Positive class ratio: {df['ClaimYN'].mean():.4f}\")\n",
        "\n",
        "    # Handle missing values with multiple strategies\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    initial_rows = len(df)\n",
        "\n",
        "    # For numerical columns, use median imputation\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    numerical_cols.remove('ClaimYN')  # Remove target\n",
        "\n",
        "    for col in numerical_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "    # For categorical columns, use mode imputation\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    for col in categorical_cols:\n",
        "        if df[col].isnull().sum() > 0:\n",
        "            df[col].fillna(df[col].mode()[0], inplace=True)\n",
        "\n",
        "    print(f\"Rows after handling missing values: {len(df)} (changed {initial_rows - len(df)})\")\n",
        "\n",
        "    # Feature Engineering\n",
        "    print(\"\\nPerforming feature engineering...\")\n",
        "\n",
        "    # Create interaction features for numerical columns\n",
        "    if len(numerical_cols) >= 2:\n",
        "        # Create a few key interactions (limit to avoid curse of dimensionality)\n",
        "        for i in range(min(3, len(numerical_cols))):\n",
        "            for j in range(i+1, min(i+3, len(numerical_cols))):\n",
        "                col1, col2 = numerical_cols[i], numerical_cols[j]\n",
        "                df[f'{col1}_x_{col2}'] = df[col1] * df[col2]\n",
        "\n",
        "    # Create polynomial features for top numerical features\n",
        "    if len(numerical_cols) >= 1:\n",
        "        for col in numerical_cols[:3]:  # Top 3 numerical features\n",
        "            df[f'{col}_squared'] = df[col] ** 2\n",
        "            df[f'{col}_log'] = np.log1p(np.abs(df[col]))\n",
        "\n",
        "    # Encode categorical variables\n",
        "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    print(f\"Categorical columns for encoding: {categorical_columns}\")\n",
        "\n",
        "    if categorical_columns:\n",
        "        df_encoded = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "    else:\n",
        "        df_encoded = df.copy()\n",
        "\n",
        "    print(f\"Shape after encoding: {df_encoded.shape}\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df_encoded.drop('ClaimYN', axis=1)\n",
        "    y = df_encoded['ClaimYN']\n",
        "\n",
        "    print(f\"Final features shape: {X.shape}\")\n",
        "    print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: FEATURE SCALING WITH MULTIPLE SCALERS\n",
        "# =============================================================================\n",
        "\n",
        "def scale_features(X_train, X_test, scaler_type='standard'):\n",
        "    \"\"\"\n",
        "    Scale features using different scalers\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 4: FEATURE SCALING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if scaler_type == 'standard':\n",
        "        scaler = StandardScaler()\n",
        "    elif scaler_type == 'minmax':\n",
        "        scaler = MinMaxScaler()\n",
        "    else:\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    print(f\"Features scaled using {scaler_type} scaler\")\n",
        "    print(f\"Training set shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Test set shape: {X_test_scaled.shape}\")\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, scaler\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: HANDLE CLASS IMBALANCE WITH SMOTE\n",
        "# =============================================================================\n",
        "\n",
        "def handle_imbalance(X_train, y_train, method='smote'):\n",
        "    \"\"\"\n",
        "    Handle class imbalance using SMOTE or other methods\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 5: HANDLING CLASS IMBALANCE\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    print(\"Original class distribution:\")\n",
        "    print(pd.Series(y_train).value_counts())\n",
        "    print(f\"Original positive class ratio: {pd.Series(y_train).mean():.4f}\")\n",
        "\n",
        "    if method == 'smote':\n",
        "        # Apply SMOTE\n",
        "        smote = SMOTE(random_state=42, k_neighbors=min(5, sum(y_train) - 1))\n",
        "        X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "    else:\n",
        "        # No resampling\n",
        "        X_resampled, y_resampled = X_train, y_train\n",
        "\n",
        "    print(\"\\nClass distribution after resampling:\")\n",
        "    print(pd.Series(y_resampled).value_counts())\n",
        "    print(f\"New positive class ratio: {pd.Series(y_resampled).mean():.4f}\")\n",
        "\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: ENHANCED FEATURE SELECTION\n",
        "# =============================================================================\n",
        "\n",
        "def select_features(X_train, y_train, feature_names, top_k=15):\n",
        "    \"\"\"\n",
        "    Enhanced feature selection using multiple methods\n",
        "# =============================================================================\n",
        "# STEP 1.5: xLSTM IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "class xLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Extended LSTM Cell with exponential gating and matrix memory\n",
        "    Based on xLSTM architecture for enhanced sequence modeling\n",
        "    \"\"\"\n",
        "…\n",
        "    print(f\"Features scaled using {scaler_type} scaler\")\n",
        "    print(f\"Training set shape: {X_train_scaled.shape}\")\n",
        "    prin\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 6: ENHANCED FEATURE SELECTION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Calculate mutual information scores\n",
        "    mi_scores = mutual_info_classif(X_train, y_train, random_state=42)\n",
        "\n",
        "    # Create feature importance DataFrame\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'mutual_info': mi_scores\n",
        "    })\n",
        "\n",
        "    # Add correlation with target\n",
        "    correlations = []\n",
        "    for i in range(X_train.shape[1]):\n",
        "        corr = np.corrcoef(X_train[:, i], y_train)[0, 1]\n",
        "        correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
        "\n",
        "    feature_importance['correlation'] = correlations\n",
        "\n",
        "    # Combined score (weighted average)\n",
        "    feature_importance['combined_score'] = (\n",
        "        0.7 * feature_importance['mutual_info'] +\n",
        "        0.3 * feature_importance['correlation']\n",
        "    )\n",
        "\n",
        "    feature_importance = feature_importance.sort_values('combined_score', ascending=False)\n",
        "\n",
        "    print(f\"Top {top_k} features by combined score:\")\n",
        "    print(feature_importance.head(top_k))\n",
        "\n",
        "    # Select top k features\n",
        "    top_features = feature_importance.head(top_k)['feature'].tolist()\n",
        "    selected_indices = [feature_names.index(feat) for feat in top_features]\n",
        "\n",
        "    # Visualize feature importance\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.barh(range(min(15, len(feature_importance))),\n",
        "             feature_importance.head(15)['mutual_info'])\n",
        "    plt.yticks(range(min(15, len(feature_importance))),\n",
        "               feature_importance.head(15)['feature'])\n",
        "    plt.xlabel('Mutual Information Score')\n",
        "    plt.title('Top Features by Mutual Information')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.barh(range(min(15, len(feature_importance))),\n",
        "             feature_importance.head(15)['correlation'])\n",
        "    plt.yticks(range(min(15, len(feature_importance))),\n",
        "               feature_importance.head(15)['feature'])\n",
        "    plt.xlabel('Absolute Correlation with Target')\n",
        "    plt.title('Top Features by Correlation')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.barh(range(top_k), top_features[::-1])\n",
        "    plt.yticks(range(top_k), [f.replace('_', ' ') for f in top_features[::-1]])\n",
        "    plt.xlabel('Combined Score')\n",
        "    plt.title(f'Selected Top {top_k} Features')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.scatter(feature_importance['mutual_info'], feature_importance['correlation'], alpha=0.6)\n",
        "    plt.xlabel('Mutual Information')\n",
        "    plt.ylabel('Correlation')\n",
        "    plt.title('Feature Selection Space')\n",
        "\n",
        "    # Highlight selected features\n",
        "    selected_features_data = feature_importance.head(top_k)\n",
        "    plt.scatter(selected_features_data['mutual_info'],\n",
        "               selected_features_data['correlation'],\n",
        "               color='red', s=100, alpha=0.8, label='Selected')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return selected_indices, feature_importance\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: ENHANCED MODEL TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_models(X_train, y_train):\n",
        "    \"\"\"\n",
        "    Train multiple models with hyperparameter tuning\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 7: ENHANCED MODEL TRAINING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    models = {}\n",
        "\n",
        "    # 1. Gradient Boosting Classifier\n",
        "    print(\"Training Gradient Boosting Classifier...\")\n",
        "    gb_params = {\n",
        "        'n_estimators': [50, 100, 150],\n",
        "        'learning_rate': [0.01, 0.1, 0.2],\n",
        "        'max_depth': [3, 5, 7],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "\n",
        "    gb_classifier = GradientBoostingClassifier(random_state=42)\n",
        "    gb_search = RandomizedSearchCV(\n",
        "        gb_classifier, gb_params, n_iter=15, cv=3,\n",
        "        scoring='roc_auc', random_state=42, n_jobs=-1\n",
        "    )\n",
        "    gb_search.fit(X_train, y_train)\n",
        "    models['Gradient Boosting'] = gb_search.best_estimator_\n",
        "\n",
        "    print(f\"Best GB parameters: {gb_search.best_params_}\")\n",
        "    print(f\"Best GB CV score: {gb_search.best_score_:.4f}\")\n",
        "\n",
        "    # 2. Random Forest Classifier\n",
        "    print(\"\\nTraining Random Forest Classifier...\")\n",
        "    rf_params = {\n",
        "        'n_estimators': [50, 100, 200],\n",
        "        'max_depth': [5, 10, 15, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "\n",
        "    rf_classifier = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "    rf_search = RandomizedSearchCV(\n",
        "        rf_classifier, rf_params, n_iter=10, cv=3,\n",
        "        scoring='roc_auc', random_state=42, n_jobs=-1\n",
        "    )\n",
        "    rf_search.fit(X_train, y_train)\n",
        "    models['Random Forest'] = rf_search.best_estimator_\n",
        "\n",
        "    print(f\"Best RF parameters: {rf_search.best_params_}\")\n",
        "    print(f\"Best RF CV score: {rf_search.best_score_:.4f}\")\n",
        "\n",
        "    # 3. Neural Network\n",
        "    print(\"\\nTraining Neural Network...\")\n",
        "    nn_params = {\n",
        "        'hidden_layer_sizes': [(50,), (100,), (100, 50), (150, 75)],\n",
        "        'activation': ['relu', 'tanh'],\n",
        "        'alpha': [0.0001, 0.001, 0.01],\n",
        "        'learning_rate': ['constant', 'adaptive']\n",
        "    }\n",
        "\n",
        "    nn_classifier = MLPClassifier(max_iter=1000, random_state=42)\n",
        "    nn_search = RandomizedSearchCV(\n",
        "        nn_classifier, nn_params, n_iter=8, cv=3,\n",
        "        scoring='roc_auc', random_state=42, n_jobs=-1\n",
        "    )\n",
        "    nn_search.fit(X_train, y_train)\n",
        "    models['Neural Network'] = nn_search.best_estimator_\n",
        "\n",
        "    print(f\"Best NN parameters: {nn_search.best_params_}\")\n",
        "    print(f\"Best NN CV score: {nn_search.best_score_:.4f}\")\n",
        "\n",
        "    # 4. Logistic Regression\n",
        "    print(\"\\nTraining Logistic Regression...\")\n",
        "    lr_params = {\n",
        "        'C': [0.1, 1.0, 10.0],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    }\n",
        "\n",
        "    lr_classifier = LogisticRegression(random_state=42, max_iter=1000)\n",
        "    lr_search = RandomizedSearchCV(\n",
        "        lr_classifier, lr_params, n_iter=8, cv=3,\n",
        "        scoring='roc_auc', random_state=42, n_jobs=-1\n",
        "    )\n",
        "    lr_search.fit(X_train, y_train)\n",
        "    models['Logistic Regression'] = lr_search.best_estimator_\n",
        "\n",
        "    print(f\"Best LR parameters: {lr_search.best_params_}\")\n",
        "    print(f\"Best LR CV score: {lr_search.best_score_:.4f}\")\n",
        "\n",
        "    return models\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7.5: xLSTM MODEL TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_xlstm_model(X_train, y_train, X_val, y_val, feature_names):\n",
        "    \"\"\"\n",
        "    Train xLSTM model for weight prediction and classification\n",
        "    \"\"\"\n",
        "    print(\"=\" * 50)\n",
        "    print(\"STEP 7.5: xLSTM MODEL TRAINING\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.LongTensor(y_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    y_val_tensor = torch.LongTensor(y_val)\n",
        "\n",
        "    # Create data loaders\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Initialize model\n",
        "    input_size = X_train.shape[1]\n",
        "    xlstm_model = xLSTMWeightPredictor(\n",
        "        input_size=input_size,\n",
        "        hidden_size=128,\n",
        "        num_layers=2,\n",
        "        memory_size=64\n",
        "    )\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = xLSTMTrainer(xlstm_model)\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training xLSTM model...\")\n",
        "    train_losses, val_losses = trainer.train(\n",
        "        train_loader, val_loader,\n",
        "        epochs=100, early_stopping_patience=15\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('xLSTM Training History')\n",
        "    plt.legend()\n",
        "\n",
        "    # Get feature weights\n",
        "    xlstm_model.eval()\n",
        "    with torch.no_grad():\n",
        "        sample_input = X_train_tensor[:100]  # Use sample for weight analysis\n",
        "        _, feature_weights, attention_weights = xlstm_model(sample_input, return_weights=True)\n",
        "\n",
        "        # Average weights across samples\n",
        "        avg_weights = torch.mean(feature_weights, dim=0).cpu().numpy()\n",
        "\n",
        "    # Visualize feature weights\n",
        "    plt.subplot(1, 2, 2)\n",
        "    top_k = min(15, len(avg_weights))\n",
        "    weight_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'weight': avg_weights\n",
        "    }).sort_values('weight', ascending=False)\n",
        "\n",
        "    plt.barh(range(top_k), weight_df.head(top_k)['weight'])\n",
        "    plt.yticks(range(top_k), [f.replace('_', ' ') for f in weight_df.head(top_k)['feature']])\n",
        "    plt.xlabel('xLSTM Feature Weight')\n",
        "    plt.title('Feature Importance by xLSTM')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return xlstm_model, trainer, weight_df\n",
        "\n",
        "# ============="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "cPqydzx2RiKL",
        "outputId": "6ccd3e08-a3e5-44d8-8d99-37f344187af2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-1152845505.py, line 1128)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1152845505.py\"\u001b[0;36m, line \u001b[0;32m1128\u001b[0m\n\u001b[0;31m    ================================================================\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}