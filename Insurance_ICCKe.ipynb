{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNAmdxVAFpyF9HhTCb5VV8P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/Insurance_ICCKe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Insurance Claim Prediction - Complete ML Pipeline\n",
        "# Comprehensive implementation with xLSTM, Neural Networks, and Gradient Boosting\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ML Libraries\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                           roc_auc_score, confusion_matrix, roc_curve, precision_recall_curve,\n",
        "                           average_precision_score, matthews_corrcoef)\n",
        "from sklearn.calibration import calibration_curve\n",
        "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "tMjxh804JBPQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# 1. DATA LOADING AND INITIAL EXPLORATION\n",
        "# =============================================================================\n",
        "\n",
        "def load_and_explore_data(file_path):\n",
        "    \"\"\"Load dataset and perform initial exploration\"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"1. DATA LOADING AND INITIAL EXPLORATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "    # Display basic info\n",
        "    print(\"\\nDataset Info:\")\n",
        "    print(df.info())\n",
        "\n",
        "    print(\"\\nFirst 5 rows:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "IRRZSR5bJLHf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mTDIEHO1GHUZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 2. EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# =============================================================================\n",
        "\n",
        "def perform_eda(df):\n",
        "    \"\"\"Comprehensive Exploratory Data Analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"2. EXPLORATORY DATA ANALYSIS (EDA)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create target variable\n",
        "    df['ClaimYN'] = ((df['NB_Claim'] >= 1) & (df['AMT_Claim'] > 1000)).astype(int)\n",
        "\n",
        "    # Class distribution\n",
        "    print(\"\\nOriginal Class Distribution:\")\n",
        "    class_dist = df['ClaimYN'].value_counts()\n",
        "    print(f\"Not Risky (0): {class_dist[0]} instances ({class_dist[0]/len(df)*100:.2f}%)\")\n",
        "    print(f\"Risky (1): {class_dist[1]} instances ({class_dist[1]/len(df)*100:.2f}%)\")\n",
        "\n",
        "    # Visualization 1: Class Distribution\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    df['ClaimYN'].value_counts().plot(kind='bar', color=['skyblue', 'salmon'])\n",
        "    plt.title('Original Class Distribution')\n",
        "    plt.xlabel('ClaimYN')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks([0, 1], ['Not Risky', 'Risky'], rotation=0)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.pie(class_dist.values, labels=['Not Risky', 'Risky'], autopct='%1.1f%%',\n",
        "            colors=['skyblue', 'salmon'])\n",
        "    plt.title('Class Distribution Percentage')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Visualization 2: Correlation Matrix\n",
        "    plt.figure(figsize=(15, 12))\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    correlation_matrix = df[numeric_cols].corr()\n",
        "    sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Visualization 3: Feature distributions by class\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    important_features = ['Duration', 'Insured.age', 'Car.age', 'Credit.score',\n",
        "                         'Annual.miles.drive', 'Years.noclaims']\n",
        "\n",
        "    for i, feature in enumerate(important_features):\n",
        "        row = i // 3\n",
        "        col = i % 3\n",
        "\n",
        "        df.boxplot(column=feature, by='ClaimYN', ax=axes[row, col])\n",
        "        axes[row, col].set_title(f'{feature} by ClaimYN')\n",
        "        axes[row, col].set_xlabel('ClaimYN')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "# =============================================================================\n",
        "# 3. DATA PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Comprehensive data preprocessing pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"3. DATA PREPROCESSING\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Drop original claim columns\n",
        "    df = df.drop(['NB_Claim', 'AMT_Claim'], axis=1)\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"Handling missing values...\")\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Handle anomalies in Car.age (negative values)\n",
        "    print(\"Handling Car.age anomalies...\")\n",
        "    df['Car.age'] = np.where(df['Car.age'] < 0, np.nan, df['Car.age'])\n",
        "    df['Car.age'] = df['Car.age'].fillna(df['Car.age'].median())\n",
        "\n",
        "    # Log transformation for skewed features\n",
        "    print(\"Applying log transformation to intensity features...\")\n",
        "    intensity_cols = [col for col in df.columns if 'intensity' in col or 'Accel' in col or 'Brake' in col]\n",
        "    for col in intensity_cols:\n",
        "        df[col] = np.log1p(df[col])  # log(1+x) to handle zeros\n",
        "\n",
        "    # Encode categorical variables\n",
        "    print(\"Encoding categorical variables...\")\n",
        "    df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop('ClaimYN', axis=1)\n",
        "    y = df['ClaimYN']\n",
        "\n",
        "    # Feature scaling\n",
        "    print(\"Scaling features...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Handle class imbalance with SMOTE\n",
        "    print(\"Applying SMOTE for class balance...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_scaled, y)\n",
        "\n",
        "    print(f\"After SMOTE - Class distribution:\")\n",
        "    unique, counts = np.unique(y_resampled, return_counts=True)\n",
        "    for u, c in zip(unique, counts):\n",
        "        print(f\"Class {u}: {c} instances\")\n",
        "\n",
        "    # Visualization: Before and After SMOTE\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Before SMOTE\n",
        "    plt.subplot(1, 2, 1)\n",
        "    original_counts = y.value_counts().values\n",
        "    plt.bar(['Not Risky', 'Risky'], original_counts, color=['skyblue', 'salmon'])\n",
        "    plt.title('Before SMOTE')\n",
        "    plt.ylabel('Count')\n",
        "    for i, v in enumerate(original_counts):\n",
        "        plt.text(i, v + 1000, str(v), ha='center', va='bottom')\n",
        "\n",
        "    # After SMOTE\n",
        "    plt.subplot(1, 2, 2)\n",
        "    resampled_counts = [counts[0], counts[1]]\n",
        "    plt.bar(['Not Risky', 'Risky'], resampled_counts, color=['lightblue', 'coral'])\n",
        "    plt.title('After SMOTE')\n",
        "    plt.ylabel('Count')\n",
        "    for i, v in enumerate(resampled_counts):\n",
        "        plt.text(i, v + 1000, str(v), ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return X_resampled, y_resampled, X.columns, scaler\n",
        "\n",
        "# =============================================================================\n",
        "# 4. MODEL ARCHITECTURES WITH INNOVATIONS\n",
        "# =============================================================================\n",
        "\n",
        "# 4.1 xLSTM Implementation (Novel Architecture)\n",
        "class xLSTMCell(nn.Module):\n",
        "    \"\"\"Extended LSTM Cell with noise filtering and global gating\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, noise_factor=0.1):\n",
        "        super(xLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.noise_factor = noise_factor\n",
        "\n",
        "        # Extended gates: input, forget, cell, output, global\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, 5 * hidden_size)\n",
        "        self.h2h = nn.Linear(hidden_size, 5 * hidden_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        h, c = hidden\n",
        "\n",
        "        # Add noise for regularization\n",
        "        noise = torch.randn_like(x) * self.noise_factor\n",
        "        x_noisy = x + noise\n",
        "\n",
        "        # Combine input and hidden state\n",
        "        combined = torch.cat((x_noisy, h), dim=1)\n",
        "        gates = self.i2h(combined) + self.h2h(h)\n",
        "\n",
        "        # Split into 5 gates\n",
        "        i, f, g, o, g_global = gates.chunk(5, 1)\n",
        "\n",
        "        # Apply activations\n",
        "        i = torch.sigmoid(i)  # Input gate\n",
        "        f = torch.sigmoid(f)  # Forget gate\n",
        "        o = torch.sigmoid(o)  # Output gate\n",
        "        g_global = torch.sigmoid(g_global)  # Global gate (innovation)\n",
        "\n",
        "        # Update cell state with global filtering\n",
        "        c_new = f * c + i * torch.tanh(g)\n",
        "        c_filtered = g_global * torch.tanh(c_new) + (1 - g_global) * c\n",
        "\n",
        "        # Update hidden state\n",
        "        h_new = o * torch.tanh(c_filtered)\n",
        "\n",
        "        return h_new, c_filtered\n",
        "\n",
        "class xLSTM(nn.Module):\n",
        "    \"\"\"Extended LSTM with multiple layers and advanced features\"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
        "        super(xLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Stack of xLSTM cells\n",
        "        self.cells = nn.ModuleList([\n",
        "            xLSTMCell(input_size if i == 0 else hidden_size, hidden_size)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Dropout and final layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Initialize hidden states\n",
        "        h = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
        "        c = [torch.zeros(batch_size, self.hidden_size).to(x.device) for _ in range(self.num_layers)]\n",
        "\n",
        "        # Process through time steps\n",
        "        for t in range(x.size(1)):\n",
        "            for l in range(self.num_layers):\n",
        "                if l == 0:\n",
        "                    h[l], c[l] = self.cells[l](x[:, t, :], (h[l], c[l]))\n",
        "                else:\n",
        "                    h[l], c[l] = self.cells[l](self.dropout(h[l-1]), (h[l], c[l]))\n",
        "\n",
        "        # Final prediction\n",
        "        return self.fc(self.dropout(h[-1]))\n",
        "\n",
        "# 4.2 Advanced Neural Network\n",
        "class AdvancedNN(nn.Module):\n",
        "    \"\"\"Advanced Neural Network with residual connections and batch normalization\"\"\"\n",
        "    def __init__(self, input_size, hidden_sizes=[128, 64, 32], output_size=2, dropout=0.3):\n",
        "        super(AdvancedNN, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "\n",
        "        for i, hidden_size in enumerate(hidden_sizes):\n",
        "            layers.extend([\n",
        "                nn.Linear(prev_size, hidden_size),\n",
        "                nn.BatchNorm1d(hidden_size),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(dropout)\n",
        "            ])\n",
        "            prev_size = hidden_size\n",
        "\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "# =============================================================================\n",
        "# 5. MODEL TRAINING AND EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "class ModelTrainer:\n",
        "    \"\"\"Comprehensive model training and evaluation class\"\"\"\n",
        "\n",
        "    def __init__(self, X, y, feature_names):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.feature_names = feature_names\n",
        "        self.models = {}\n",
        "        self.results = {}\n",
        "\n",
        "    def split_data(self, test_size=0.2, val_size=0.1):\n",
        "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
        "        # First split: separate test set\n",
        "        X_temp, self.X_test, y_temp, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=test_size, random_state=42, stratify=self.y\n",
        "        )\n",
        "\n",
        "        # Second split: separate train and validation\n",
        "        val_size_adjusted = val_size / (1 - test_size)\n",
        "        self.X_train, self.X_val, self.y_train, self.y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        print(f\"Train set: {self.X_train.shape[0]} samples\")\n",
        "        print(f\"Validation set: {self.X_val.shape[0]} samples\")\n",
        "        print(f\"Test set: {self.X_test.shape[0]} samples\")\n",
        "\n",
        "    def train_traditional_models(self):\n",
        "        \"\"\"Train traditional ML models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"5. TRAINING TRADITIONAL MODELS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Gradient Boosting with hyperparameter tuning\n",
        "        print(\"Training Gradient Boosting...\")\n",
        "        gb_param_grid = {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "\n",
        "        gb_model = GradientBoostingClassifier(random_state=42)\n",
        "        gb_search = RandomizedSearchCV(\n",
        "            gb_model, gb_param_grid, n_iter=20, cv=3,\n",
        "            scoring='roc_auc', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        gb_search.fit(self.X_train, self.y_train)\n",
        "        self.models['Gradient Boosting'] = gb_search.best_estimator_\n",
        "\n",
        "        print(f\"Best GB parameters: {gb_search.best_params_}\")\n",
        "\n",
        "        # Neural Network (sklearn)\n",
        "        print(\"Training Neural Network...\")\n",
        "        nn_param_grid = {\n",
        "            'hidden_layer_sizes': [(100, 50), (128, 64), (100, 100)],\n",
        "            'activation': ['relu', 'tanh'],\n",
        "            'alpha': [0.0001, 0.001, 0.01],\n",
        "            'learning_rate': ['constant', 'adaptive']\n",
        "        }\n",
        "\n",
        "        nn_model = MLPClassifier(max_iter=1000, random_state=42)\n",
        "        nn_search = RandomizedSearchCV(\n",
        "            nn_model, nn_param_grid, n_iter=15, cv=3,\n",
        "            scoring='roc_auc', random_state=42, n_jobs=-1\n",
        "        )\n",
        "        nn_search.fit(self.X_train, self.y_train)\n",
        "        self.models['Neural Network'] = nn_search.best_estimator_\n",
        "\n",
        "        print(f\"Best NN parameters: {nn_search.best_params_}\")\n",
        "\n",
        "        # Random Forest for comparison\n",
        "        print(\"Training Random Forest...\")\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "        rf_model.fit(self.X_train, self.y_train)\n",
        "        self.models['Random Forest'] = rf_model\n",
        "\n",
        "    def train_deep_models(self, epochs=50):\n",
        "        \"\"\"Train deep learning models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"6. TRAINING DEEP LEARNING MODELS\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        # Prepare data for PyTorch\n",
        "        X_train_tensor = torch.FloatTensor(self.X_train).unsqueeze(1)  # Add sequence dimension\n",
        "        y_train_tensor = torch.LongTensor(self.y_train)\n",
        "        X_val_tensor = torch.FloatTensor(self.X_val).unsqueeze(1)\n",
        "        y_val_tensor = torch.LongTensor(self.y_val)\n",
        "\n",
        "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=64)\n",
        "\n",
        "        # Train xLSTM\n",
        "        print(\"Training xLSTM...\")\n",
        "        xlstm_model = xLSTM(\n",
        "            input_size=self.X_train.shape[1],\n",
        "            hidden_size=64,\n",
        "            num_layers=2,\n",
        "            output_size=2\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        optimizer = optim.Adam(xlstm_model.parameters(), lr=0.001)\n",
        "\n",
        "        xlstm_history = self._train_pytorch_model(\n",
        "            xlstm_model, train_loader, val_loader, criterion, optimizer, epochs, \"xLSTM\"\n",
        "        )\n",
        "\n",
        "        self.models['xLSTM'] = xlstm_model\n",
        "\n",
        "        # Train Advanced NN\n",
        "        print(\"Training Advanced NN...\")\n",
        "        # For regular NN, we don't need sequence dimension\n",
        "        X_train_flat = torch.FloatTensor(self.X_train)\n",
        "        X_val_flat = torch.FloatTensor(self.X_val)\n",
        "\n",
        "        train_dataset_flat = TensorDataset(X_train_flat, y_train_tensor)\n",
        "        val_dataset_flat = TensorDataset(X_val_flat, y_val_tensor)\n",
        "\n",
        "        train_loader_flat = DataLoader(train_dataset_flat, batch_size=64, shuffle=True)\n",
        "        val_loader_flat = DataLoader(val_dataset_flat, batch_size=64)\n",
        "\n",
        "        advanced_nn = AdvancedNN(input_size=self.X_train.shape[1])\n",
        "        optimizer_nn = optim.Adam(advanced_nn.parameters(), lr=0.001)\n",
        "\n",
        "        nn_history = self._train_pytorch_model(\n",
        "            advanced_nn, train_loader_flat, val_loader_flat, criterion, optimizer_nn, epochs, \"Advanced NN\"\n",
        "        )\n",
        "\n",
        "        self.models['Advanced NN'] = advanced_nn\n",
        "\n",
        "        return xlstm_history, nn_history\n",
        "\n",
        "    def _train_pytorch_model(self, model, train_loader, val_loader, criterion, optimizer, epochs, model_name):\n",
        "        \"\"\"Helper function to train PyTorch models\"\"\"\n",
        "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Training phase\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            train_correct = 0\n",
        "            train_total = 0\n",
        "\n",
        "            for batch_x, batch_y in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(batch_x)\n",
        "                loss = criterion(outputs, batch_y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                train_total += batch_y.size(0)\n",
        "                train_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            val_loss = 0.0\n",
        "            val_correct = 0\n",
        "            val_total = 0\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch_x, batch_y in val_loader:\n",
        "                    outputs = model(batch_x)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "\n",
        "                    val_loss += loss.item()\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    val_total += batch_y.size(0)\n",
        "                    val_correct += (predicted == batch_y).sum().item()\n",
        "\n",
        "            # Record metrics\n",
        "            train_acc = 100 * train_correct / train_total\n",
        "            val_acc = 100 * val_correct / val_total\n",
        "\n",
        "            history['train_loss'].append(train_loss / len(train_loader))\n",
        "            history['val_loss'].append(val_loss / len(val_loader))\n",
        "            history['train_acc'].append(train_acc)\n",
        "            history['val_acc'].append(val_acc)\n",
        "\n",
        "            if (epoch + 1) % 10 == 0:\n",
        "                print(f'{model_name} Epoch [{epoch+1}/{epochs}] - '\n",
        "                      f'Train Acc: {train_acc:.2f}% - Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        return history\n",
        "\n",
        "    def evaluate_all_models(self):\n",
        "        \"\"\"Comprehensive evaluation of all models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"7. MODEL EVALUATION\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "            # Get predictions\n",
        "            if isinstance(model, nn.Module):\n",
        "                # PyTorch model\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    if 'xLSTM' in model_name:\n",
        "                        test_tensor = torch.FloatTensor(self.X_test).unsqueeze(1)\n",
        "                    else:\n",
        "                        test_tensor = torch.FloatTensor(self.X_test)\n",
        "\n",
        "                    outputs = model(test_tensor)\n",
        "                    probabilities = torch.softmax(outputs, dim=1)[:, 1].numpy()\n",
        "                    predictions = (probabilities > 0.5).astype(int)\n",
        "            else:\n",
        "                # Sklearn model\n",
        "                predictions = model.predict(self.X_test)\n",
        "                probabilities = model.predict_proba(self.X_test)[:, 1]\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(self.y_test, predictions)\n",
        "            precision = precision_score(self.y_test, predictions)\n",
        "            recall = recall_score(self.y_test, predictions)\n",
        "            f1 = f1_score(self.y_test, predictions)\n",
        "            auc = roc_auc_score(self.y_test, probabilities)\n",
        "            mcc = matthews_corrcoef(self.y_test, predictions)\n",
        "\n",
        "            self.results[model_name] = {\n",
        "                'accuracy': accuracy,\n",
        "                'precision': precision,\n",
        "                'recall': recall,\n",
        "                'f1': f1,\n",
        "                'auc': auc,\n",
        "                'mcc': mcc,\n",
        "                'predictions': predictions,\n",
        "                'probabilities': probabilities\n",
        "            }\n",
        "\n",
        "            print(f\"Accuracy: {accuracy:.4f}\")\n",
        "            print(f\"Precision: {precision:.4f}\")\n",
        "            print(f\"Recall: {recall:.4f}\")\n",
        "            print(f\"F1 Score: {f1:.4f}\")\n",
        "            print(f\"AUC-ROC: {auc:.4f}\")\n",
        "            print(f\"MCC: {mcc:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# 8. POST-PROCESSING AND VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "def create_comprehensive_visualizations(trainer):\n",
        "    \"\"\"Create comprehensive visualization suite\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"8. COMPREHENSIVE VISUALIZATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Model Performance Comparison\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc', 'mcc']\n",
        "    model_names = list(trainer.results.keys())\n",
        "\n",
        "    for i, metric in enumerate(metrics):\n",
        "        plt.subplot(2, 3, i+1)\n",
        "        values = [trainer.results[model][metric] for model in model_names]\n",
        "        bars = plt.bar(model_names, values, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'mediumpurple'])\n",
        "        plt.title(f'{metric.upper()} Comparison')\n",
        "        plt.ylabel(metric.upper())\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add value labels on bars\n",
        "        for bar, value in zip(bars, values):\n",
        "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                    f'{value:.3f}', ha='center', va='bottom')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 2. ROC Curves Comparison\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
        "\n",
        "    for i, (model_name, results) in enumerate(trainer.results.items()):\n",
        "        fpr, tpr, _ = roc_curve(trainer.y_test, results['probabilities'])\n",
        "        plt.plot(fpr, tpr, color=colors[i], label=f'{model_name} (AUC = {results[\"auc\"]:.3f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.6)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curves Comparison')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # 3. Confusion Matrices\n",
        "    n_models = len(trainer.results)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(4*n_models, 4))\n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, (model_name, results) in enumerate(trainer.results.items()):\n",
        "        cm = confusion_matrix(trainer.y_test, results['predictions'])\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
        "        axes[i].set_title(f'{model_name}')\n",
        "        axes[i].set_xlabel('Predicted')\n",
        "        axes[i].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4. Feature Importance (for tree-based models)\n",
        "    if 'Gradient Boosting' in trainer.models:\n",
        "        gb_model = trainer.models['Gradient Boosting']\n",
        "        feature_importance = gb_model.feature_importances_\n",
        "\n",
        "        # Get top 15 features\n",
        "        indices = np.argsort(feature_importance)[::-1][:15]\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.title('Top 15 Feature Importances (Gradient Boosting)')\n",
        "        plt.bar(range(15), feature_importance[indices])\n",
        "        plt.xticks(range(15), [trainer.feature_names[i] for i in indices], rotation=45, ha='right')\n",
        "        plt.ylabel('Importance')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "def create_ensemble_model(trainer):\n",
        "    \"\"\"Create and evaluate ensemble model\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"9. ENSEMBLE MODEL CREATION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Get probabilities from all models\n",
        "    probabilities = []\n",
        "    weights = []\n",
        "\n",
        "    for model_name, results in trainer.results.items():\n",
        "        probabilities.append(results['probabilities'])\n",
        "        weights.append(results['auc'])  # Weight by AUC score\n",
        "\n",
        "    probabilities = np.array(probabilities)\n",
        "    weights = np.array(weights)\n",
        "    weights = weights / weights.sum()  # Normalize weights\n",
        "\n",
        "    # Create weighted ensemble\n",
        "    ensemble_probs = np.average(probabilities, axis=0, weights=weights)\n",
        "    ensemble_preds = (ensemble_probs > 0.5).astype(int)\n",
        "\n",
        "    # Evaluate ensemble\n",
        "    ensemble_metrics = {\n",
        "        'accuracy': accuracy_score(trainer.y_test, ensemble_preds),\n",
        "        'precision': precision_score(trainer.y_test, ensemble_preds),\n",
        "        'recall': recall_score(trainer.y_test, ensemble_preds),\n",
        "        'f1': f1_score(trainer.y_test, ensemble_preds),\n",
        "        'auc': roc_auc_score(trainer.y_test, ensemble_probs),\n",
        "        'mcc': matthews_corrcoef(trainer.y_test, ensemble_preds)\n",
        "    }\n",
        "\n",
        "    print(\"Ensemble Model Performance:\")\n",
        "    for metric, value in ensemble_metrics.items():\n",
        "        print(f\"{metric.upper()}: {value:.4f}\")\n",
        "\n",
        "    # Add ensemble to results\n",
        "    trainer.results['Ensemble'] = {\n",
        "        **ensemble_metrics,\n",
        "        'predictions': ensemble_preds,\n",
        "        'probabilities': ensemble_probs\n",
        "    }\n",
        "\n",
        "    return ensemble_metrics\n",
        "\n",
        "# =============================================================================\n",
        "# 10. MAIN EXECUTION PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "# def main_pipeline(file_path):\n",
        "#     \"\"\"Execute the complete ML pipeline\"\"\"\n",
        "#     print(\"INSURANCE CLAIM PREDICTION - COMPLETE ML PIPELINE\")\n",
        "#     print(\"=\"*70)\n",
        "\n",
        "#     # Step 1: Load and explore data\n",
        "#     df = load_and_explore_data(file_path)\n",
        "\n",
        "#     # Step 2: EDA\n",
        "#     df = perform_eda(df)\n",
        "\n",
        "#     # Step 3: Preprocessing\n",
        "#     X_resampled, y_resampled, feature_names, scaler = preprocess_data(df)\n",
        "\n",
        "#     # Step 4: Initialize trainer\n",
        "#     trainer = ModelTrainer(X_resampled, y_resampled, feature_names)\n",
        "#     trainer.split_data()\n",
        "\n",
        "#     # Step 5: Train traditional models\n",
        "#     trainer.train_traditional_models()\n",
        "\n",
        "#     # Step 6: Train deep learning models\n",
        "#     xlstm_history, nn_history ="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main_pipeline(file_path):\n",
        "    \"\"\"Execute the complete ML pipeline\"\"\"\n",
        "    print(\"INSURANCE CLAIM PREDICTION - COMPLETE ML PIPELINE\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Load and explore data\n",
        "        df = load_and_explore_data(file_path)\n",
        "\n",
        "        # Step 2: EDA\n",
        "        df = perform_eda(df)\n",
        "\n",
        "        # Step 3: Preprocessing\n",
        "        X_resampled, y_resampled, feature_names, scaler = preprocess_data(df)\n",
        "\n",
        "        # Step 4: Initialize trainer\n",
        "        trainer = ModelTrainer(X_resampled, y_resampled, feature_names)\n",
        "        trainer.split_data()\n",
        "\n",
        "        # Step 5: Train traditional models\n",
        "        trainer.train_traditional_models()\n",
        "\n",
        "        # Step 6: Train deep learning models\n",
        "        xlstm_history, nn_history = trainer.train_deep_models(epochs=50)\n",
        "\n",
        "        # Step 7: Evaluate all models\n",
        "        trainer.evaluate_all_models()\n",
        "\n",
        "        # Step 8: Create comprehensive visualizations\n",
        "        create_comprehensive_visualizations(trainer)\n",
        "\n",
        "        # Step 9: Create ensemble model\n",
        "        ensemble_metrics = create_ensemble_model(trainer)\n",
        "\n",
        "        # Step 10: Final summary and results\n",
        "        print_final_summary(trainer, ensemble_metrics)\n",
        "\n",
        "        # Step 11: Save results and models\n",
        "        save_results_and_models(trainer, scaler, file_path)\n",
        "\n",
        "        return trainer, ensemble_metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in pipeline execution: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n",
        "\n",
        "def print_final_summary(trainer, ensemble_metrics):\n",
        "    \"\"\"Print comprehensive final summary\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"10. FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create results DataFrame\n",
        "    results_df = pd.DataFrame(trainer.results).T\n",
        "    results_df = results_df[['accuracy', 'precision', 'recall', 'f1', 'auc', 'mcc']]\n",
        "\n",
        "    print(\"\\nComplete Performance Summary:\")\n",
        "    print(results_df.round(4))\n",
        "\n",
        "    # Find best model for each metric\n",
        "    print(\"\\nBest Model by Metric:\")\n",
        "    for metric in ['accuracy', 'precision', 'recall', 'f1', 'auc', 'mcc']:\n",
        "        best_model = results_df[metric].idxmax()\n",
        "        best_score = results_df[metric].max()\n",
        "        print(f\"{metric.upper()}: {best_model} ({best_score:.4f})\")\n",
        "\n",
        "    # Overall best model (weighted by AUC and F1)\n",
        "    results_df['combined_score'] = 0.6 * results_df['auc'] + 0.4 * results_df['f1']\n",
        "    best_overall = results_df['combined_score'].idxmax()\n",
        "    print(f\"\\nBest Overall Model: {best_overall} (Combined Score: {results_df.loc[best_overall, 'combined_score']:.4f})\")\n",
        "\n",
        "    # Business insights\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n1. Model Performance Analysis:\")\n",
        "    if ensemble_metrics['auc'] > 0.85:\n",
        "        print(\"   ✓ Excellent predictive performance achieved (AUC > 0.85)\")\n",
        "    elif ensemble_metrics['auc'] > 0.75:\n",
        "        print(\"   ✓ Good predictive performance achieved (AUC > 0.75)\")\n",
        "    else:\n",
        "        print(\"   ⚠ Moderate predictive performance (AUC < 0.75) - consider feature engineering\")\n",
        "\n",
        "    print(f\"\\n2. Risk Assessment Capability:\")\n",
        "    print(f\"   • Ensemble model can correctly identify {ensemble_metrics['recall']:.1%} of high-risk claims\")\n",
        "    print(f\"   • {ensemble_metrics['precision']:.1%} of flagged claims are actually high-risk\")\n",
        "    print(f\"   • Overall accuracy: {ensemble_metrics['accuracy']:.1%}\")\n",
        "\n",
        "    print(f\"\\n3. Business Impact:\")\n",
        "    print(f\"   • Potential reduction in claim processing costs\")\n",
        "    print(f\"   • Improved risk-based pricing strategies\")\n",
        "    print(f\"   • Enhanced customer segmentation capabilities\")\n",
        "\n",
        "def save_results_and_models(trainer, scaler, original_file_path):\n",
        "    \"\"\"Save models, results, and create deployment artifacts\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"11. SAVING MODELS AND RESULTS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    import pickle\n",
        "    import json\n",
        "    from datetime import datetime\n",
        "    import os\n",
        "\n",
        "    # Create results directory\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    results_dir = f\"insurance_claim_results_{timestamp}\"\n",
        "    os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "    # Save traditional models\n",
        "    for model_name, model in trainer.models.items():\n",
        "        if not isinstance(model, nn.Module):  # Skip PyTorch models for pickle\n",
        "            model_file = os.path.join(results_dir, f\"{model_name.replace(' ', '_').lower()}_model.pkl\")\n",
        "            with open(model_file, 'wb') as f:\n",
        "                pickle.dump(model, f)\n",
        "            print(f\"Saved {model_name} to {model_file}\")\n",
        "\n",
        "    # Save PyTorch models\n",
        "    for model_name, model in trainer.models.items():\n",
        "        if isinstance(model, nn.Module):\n",
        "            model_file = os.path.join(results_dir, f\"{model_name.replace(' ', '_').lower()}_model.pth\")\n",
        "            torch.save(model.state_dict(), model_file)\n",
        "            print(f\"Saved {model_name} to {model_file}\")\n",
        "\n",
        "    # Save scaler\n",
        "    scaler_file = os.path.join(results_dir, \"scaler.pkl\")\n",
        "    with open(scaler_file, 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n",
        "    print(f\"Saved scaler to {scaler_file}\")\n",
        "\n",
        "    # Save results\n",
        "    results_file = os.path.join(results_dir, \"model_results.json\")\n",
        "    # Convert numpy arrays to lists for JSON serialization\n",
        "    json_results = {}\n",
        "    for model_name, results in trainer.results.items():\n",
        "        json_results[model_name] = {\n",
        "            k: v.tolist() if isinstance(v, np.ndarray) else float(v) if isinstance(v, np.floating) else v\n",
        "            for k, v in results.items() if k not in ['predictions', 'probabilities']\n",
        "        }\n",
        "\n",
        "    with open(results_file, 'w') as f:\n",
        "        json.dump(json_results, f, indent=2)\n",
        "    print(f\"Saved results to {results_file}\")\n",
        "\n",
        "    # Save feature names\n",
        "    features_file = os.path.join(results_dir, \"feature_names.json\")\n",
        "    with open(features_file, 'w') as f:\n",
        "        json.dump(trainer.feature_names.tolist(), f, indent=2)\n",
        "    print(f\"Saved feature names to {features_file}\")\n",
        "\n",
        "    # Create deployment guide\n",
        "    deployment_guide = f\"\"\"\n",
        "# Insurance Claim Prediction Model - Deployment Guide\n",
        "\n",
        "## Generated on: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
        "\n",
        "## Model Performance Summary\n",
        "- Best Overall Model: {max(trainer.results.keys(), key=lambda x: trainer.results[x]['auc'])}\n",
        "- Best AUC Score: {max(trainer.results[model]['auc'] for model in trainer.results):.4f}\n",
        "- Ensemble AUC Score: {trainer.results.get('Ensemble', {}).get('auc', 'N/A')}\n",
        "\n",
        "## Files in this directory:\n",
        "- *.pkl files: Trained scikit-learn models\n",
        "- *.pth files: Trained PyTorch models\n",
        "- scaler.pkl: Feature scaler for preprocessing\n",
        "- model_results.json: Detailed performance metrics\n",
        "- feature_names.json: List of feature names\n",
        "- deployment_guide.md: This file\n",
        "\n",
        "## Quick Start for Predictions:\n",
        "```python\n",
        "import pickle\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Load best traditional model (example with Gradient Boosting)\n",
        "with open('gradient_boosting_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)\n",
        "\n",
        "# Load scaler\n",
        "with open('scaler.pkl', 'rb') as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "# Make prediction on new data\n",
        "def predict_claim_risk(new_data):\n",
        "    # new_data should be a pandas DataFrame with same features as training\n",
        "    scaled_data = scaler.transform(new_data)\n",
        "    prediction = model.predict(scaled_data)\n",
        "    probability = model.predict_proba(scaled_data)[:, 1]\n",
        "    return prediction, probability\n",
        "```\n",
        "\n",
        "## Model Interpretation:\n",
        "- Prediction = 1: High risk of significant insurance claim\n",
        "- Prediction = 0: Low risk of significant insurance claim\n",
        "- Probability threshold: 0.5 (adjustable based on business needs)\n",
        "\n",
        "## Recommended Usage:\n",
        "1. Use ensemble model for highest accuracy\n",
        "2. Use Gradient Boosting for interpretability\n",
        "3. Use xLSTM for complex pattern detection\n",
        "4. Monitor model performance regularly and retrain as needed\n",
        "\"\"\"\n",
        "\n",
        "    guide_file = os.path.join(results_dir, \"deployment_guide.md\")\n",
        "    with open(guide_file, 'w') as f:\n",
        "        f.write(deployment_guide)\n",
        "    print(f\"Saved deployment guide to {guide_file}\")\n",
        "\n",
        "    print(f\"\\nAll results saved to directory: {results_dir}\")\n",
        "    return results_dir\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN EXECUTION\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    DATA_FILE_PATH = \"insurance_data.csv\"  # Update this path to your dataset\n",
        "\n",
        "    print(\"Starting Insurance Claim Prediction Pipeline...\")\n",
        "    print(\"Please ensure your dataset is available at:\", DATA_FILE_PATH)\n",
        "\n",
        "    # Check if file exists\n",
        "    import os\n",
        "    if not os.path.exists(DATA_FILE_PATH):\n",
        "        print(f\"Error: Dataset file '{DATA_FILE_PATH}' not found!\")\n",
        "        print(\"Please update the DATA_FILE_PATH variable with the correct path to your dataset.\")\n",
        "        print(\"\\nAlternatively, you can run the pipeline with a custom path:\")\n",
        "        print(\"trainer, ensemble_metrics = main_pipeline('path/to/your/dataset.csv')\")\n",
        "    else:\n",
        "        # Execute the complete pipeline\n",
        "        trainer, ensemble_metrics = main_pipeline(DATA_FILE_PATH)\n",
        "\n",
        "        if trainer is not None:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"Total models trained: {len(trainer.models)}\")\n",
        "            print(f\"Best ensemble AUC: {ensemble_metrics['auc']:.4f}\")\n",
        "            print(\"Check the results directory for saved models and detailed reports.\")\n",
        "        else:\n",
        "            print(\"Pipeline execution failed. Please check the error messages above.\")\n",
        "\n",
        "# Alternative execution for Jupyter notebooks or custom usage:\n",
        "\"\"\"\n",
        "# For Jupyter notebook or custom usage:\n",
        "DATA_PATH = \"your_insurance_dataset.csv\"\n",
        "trainer, ensemble_results = main_pipeline(DATA_PATH)\n",
        "\n",
        "# Access specific model results:\n",
        "print(\"Gradient Boosting AUC:\", trainer.results['Gradient Boosting']['auc'])\n",
        "print(\"xLSTM AUC:\", trainer.results['xLSTM']['auc'])\n",
        "print(\"Ensemble AUC:\", ensemble_results['auc'])\n",
        "\n",
        "# Make predictions on new data:\n",
        "# new_predictions = trainer.models['Gradient Boosting'].predict(new_scaled_data)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "zz_Ul6TnI2Kw",
        "outputId": "e9b9c3b7-8e06-47fc-cd9b-d89ae7f0c55f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Insurance Claim Prediction Pipeline...\n",
            "Please ensure your dataset is available at: insurance_data.csv\n",
            "Error: Dataset file 'insurance_data.csv' not found!\n",
            "Please update the DATA_FILE_PATH variable with the correct path to your dataset.\n",
            "\n",
            "Alternatively, you can run the pipeline with a custom path:\n",
            "trainer, ensemble_metrics = main_pipeline('path/to/your/dataset.csv')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# For Jupyter notebook or custom usage:\\nDATA_PATH = \"your_insurance_dataset.csv\"\\ntrainer, ensemble_results = main_pipeline(DATA_PATH)\\n\\n# Access specific model results:\\nprint(\"Gradient Boosting AUC:\", trainer.results[\\'Gradient Boosting\\'][\\'auc\\'])\\nprint(\"xLSTM AUC:\", trainer.results[\\'xLSTM\\'][\\'auc\\'])\\nprint(\"Ensemble AUC:\", ensemble_results[\\'auc\\'])\\n\\n# Make predictions on new data:\\n# new_predictions = trainer.models[\\'Gradient Boosting\\'].predict(new_scaled_data)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}