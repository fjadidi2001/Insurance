{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6FuTogC9AnVcDDev6Q0gp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/Insurance_ICCKe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Load the dataset from Google Drive\n",
        "file_path = '/content/drive/My Drive/Insurance/telematics_syn.csv'\n",
        "df = pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cVLFKKghSnQ",
        "outputId": "d5e4d472-0c66-4868-8b7a-6b319642af96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "XcAkEjP-hiKL",
        "outputId": "c7155bdd-8c90-4647-f990-7e6a8f62e091"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
              "0       366           45        Male       -1  Married  Commute         609.0   \n",
              "1       182           44      Female        3  Married  Commute         575.0   \n",
              "2       184           48      Female        6  Married  Commute         847.0   \n",
              "3       183           71        Male        6  Married  Private         842.0   \n",
              "4       183           84        Male       10  Married  Private         856.0   \n",
              "\n",
              "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
              "0  Urban             6213.71              25  ...                    1.0   \n",
              "1  Urban            12427.42              20  ...                   58.0   \n",
              "2  Urban            12427.42              14  ...                    0.0   \n",
              "3  Urban             6213.71              43  ...                    0.0   \n",
              "4  Urban             6213.71              65  ...                    2.0   \n",
              "\n",
              "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
              "0                    0.0                    0.0                     3.0   \n",
              "1                   24.0                   11.0                  1099.0   \n",
              "2                    0.0                    0.0                     0.0   \n",
              "3                    0.0                    0.0                     0.0   \n",
              "4                    0.0                    0.0                   325.0   \n",
              "\n",
              "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
              "0                     1.0                     0.0                     0.0   \n",
              "1                   615.0                   219.0                   101.0   \n",
              "2                     0.0                     0.0                     0.0   \n",
              "3                     0.0                     0.0                     0.0   \n",
              "4                   111.0                    18.0                     4.0   \n",
              "\n",
              "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
              "0                     0.0         1  5100.171753  \n",
              "1                    40.0         1   883.554840  \n",
              "2                     0.0         0     0.000000  \n",
              "3                     0.0         0     0.000000  \n",
              "4                     2.0         0     0.000000  \n",
              "\n",
              "[5 rows x 52 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0dedd023-6f3c-46d3-b589-e1638dc78a46\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Duration</th>\n",
              "      <th>Insured.age</th>\n",
              "      <th>Insured.sex</th>\n",
              "      <th>Car.age</th>\n",
              "      <th>Marital</th>\n",
              "      <th>Car.use</th>\n",
              "      <th>Credit.score</th>\n",
              "      <th>Region</th>\n",
              "      <th>Annual.miles.drive</th>\n",
              "      <th>Years.noclaims</th>\n",
              "      <th>...</th>\n",
              "      <th>Left.turn.intensity10</th>\n",
              "      <th>Left.turn.intensity11</th>\n",
              "      <th>Left.turn.intensity12</th>\n",
              "      <th>Right.turn.intensity08</th>\n",
              "      <th>Right.turn.intensity09</th>\n",
              "      <th>Right.turn.intensity10</th>\n",
              "      <th>Right.turn.intensity11</th>\n",
              "      <th>Right.turn.intensity12</th>\n",
              "      <th>NB_Claim</th>\n",
              "      <th>AMT_Claim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>366</td>\n",
              "      <td>45</td>\n",
              "      <td>Male</td>\n",
              "      <td>-1</td>\n",
              "      <td>Married</td>\n",
              "      <td>Commute</td>\n",
              "      <td>609.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>6213.71</td>\n",
              "      <td>25</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>5100.171753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>182</td>\n",
              "      <td>44</td>\n",
              "      <td>Female</td>\n",
              "      <td>3</td>\n",
              "      <td>Married</td>\n",
              "      <td>Commute</td>\n",
              "      <td>575.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>12427.42</td>\n",
              "      <td>20</td>\n",
              "      <td>...</td>\n",
              "      <td>58.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>1099.0</td>\n",
              "      <td>615.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>1</td>\n",
              "      <td>883.554840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>184</td>\n",
              "      <td>48</td>\n",
              "      <td>Female</td>\n",
              "      <td>6</td>\n",
              "      <td>Married</td>\n",
              "      <td>Commute</td>\n",
              "      <td>847.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>12427.42</td>\n",
              "      <td>14</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>183</td>\n",
              "      <td>71</td>\n",
              "      <td>Male</td>\n",
              "      <td>6</td>\n",
              "      <td>Married</td>\n",
              "      <td>Private</td>\n",
              "      <td>842.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>6213.71</td>\n",
              "      <td>43</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>183</td>\n",
              "      <td>84</td>\n",
              "      <td>Male</td>\n",
              "      <td>10</td>\n",
              "      <td>Married</td>\n",
              "      <td>Private</td>\n",
              "      <td>856.0</td>\n",
              "      <td>Urban</td>\n",
              "      <td>6213.71</td>\n",
              "      <td>65</td>\n",
              "      <td>...</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>325.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 52 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0dedd023-6f3c-46d3-b589-e1638dc78a46')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0dedd023-6f3c-46d3-b589-e1638dc78a46 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0dedd023-6f3c-46d3-b589-e1638dc78a46');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6b450a6d-eccb-4c21-9ecd-4eeb2fd37e7f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6b450a6d-eccb-4c21-9ecd-4eeb2fd37e7f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6b450a6d-eccb-4c21-9ecd-4eeb2fd37e7f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "2KoCyTpSiP32",
        "outputId": "17a1ad5b-28a6-4f2d-b66b-2597e33e2964"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Duration    Insured.age        Car.age   Credit.score  \\\n",
              "count  100000.000000  100000.000000  100000.000000  100000.000000   \n",
              "mean      314.204060      51.378950       5.639720     800.888870   \n",
              "std        79.746222      15.467075       4.062135      83.382316   \n",
              "min        27.000000      16.000000      -2.000000     422.000000   \n",
              "25%       200.000000      39.000000       2.000000     766.000000   \n",
              "50%       365.000000      51.000000       5.000000     825.000000   \n",
              "75%       366.000000      63.000000       8.000000     856.000000   \n",
              "max       366.000000     103.000000      20.000000     900.000000   \n",
              "\n",
              "       Annual.miles.drive  Years.noclaims      Territory  Annual.pct.driven  \\\n",
              "count       100000.000000   100000.000000  100000.000000      100000.000000   \n",
              "mean          9124.122908       28.839960      56.531390           0.502294   \n",
              "std           3826.144730       16.123717      24.036518           0.299189   \n",
              "min              0.000000        0.000000      11.000000           0.002740   \n",
              "25%           6213.710000       15.000000      35.000000           0.249315   \n",
              "50%           7456.452000       29.000000      62.000000           0.490411   \n",
              "75%          12427.420000       41.000000      78.000000           0.753425   \n",
              "max          56731.172300       79.000000      91.000000           1.000000   \n",
              "\n",
              "       Total.miles.driven  Pct.drive.mon  ...  Left.turn.intensity10  \\\n",
              "count       100000.000000  100000.000000  ...          100000.000000   \n",
              "mean          4833.575303       0.139365  ...             551.574010   \n",
              "std           4545.943016       0.042807  ...           14687.929802   \n",
              "min              0.095298       0.000000  ...               0.000000   \n",
              "25%           1529.897500       0.120894  ...               0.000000   \n",
              "50%           3468.287765       0.137909  ...               3.000000   \n",
              "75%           6779.876842       0.155203  ...              30.000000   \n",
              "max          47282.603936       0.998172  ...          794380.000000   \n",
              "\n",
              "       Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
              "count          100000.000000          100000.000000           100000.000000   \n",
              "mean              487.340690             447.758420              843.461830   \n",
              "std             14198.331308           13719.790281            11630.185503   \n",
              "min                 0.000000               0.000000                0.000000   \n",
              "25%                 0.000000               0.000000               11.000000   \n",
              "50%                 1.000000               0.000000              122.000000   \n",
              "75%                 9.000000               2.000000              680.000000   \n",
              "max            793926.000000          793170.000000           841210.000000   \n",
              "\n",
              "       Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
              "count           100000.000000           100000.000000           100000.000000   \n",
              "mean               565.056100              326.654840              246.713120   \n",
              "std              10657.402935             9460.244357             8977.569994   \n",
              "min                  0.000000                0.000000                0.000000   \n",
              "25%                  3.000000                0.000000                0.000000   \n",
              "50%                 43.000000                7.000000                2.000000   \n",
              "75%                321.000000               81.000000               27.000000   \n",
              "max             841207.000000           841200.000000           841176.000000   \n",
              "\n",
              "       Right.turn.intensity12      NB_Claim      AMT_Claim  \n",
              "count           100000.000000  100000.00000  100000.000000  \n",
              "mean               198.753690       0.04494     137.602253  \n",
              "std               8585.177049       0.21813    1264.320056  \n",
              "min                  0.000000       0.00000       0.000000  \n",
              "25%                  0.000000       0.00000       0.000000  \n",
              "50%                  0.000000       0.00000       0.000000  \n",
              "75%                  9.000000       0.00000       0.000000  \n",
              "max             841144.000000       3.00000  104074.886700  \n",
              "\n",
              "[8 rows x 48 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29b730b3-2f8d-414f-9983-7ac692bd8292\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Duration</th>\n",
              "      <th>Insured.age</th>\n",
              "      <th>Car.age</th>\n",
              "      <th>Credit.score</th>\n",
              "      <th>Annual.miles.drive</th>\n",
              "      <th>Years.noclaims</th>\n",
              "      <th>Territory</th>\n",
              "      <th>Annual.pct.driven</th>\n",
              "      <th>Total.miles.driven</th>\n",
              "      <th>Pct.drive.mon</th>\n",
              "      <th>...</th>\n",
              "      <th>Left.turn.intensity10</th>\n",
              "      <th>Left.turn.intensity11</th>\n",
              "      <th>Left.turn.intensity12</th>\n",
              "      <th>Right.turn.intensity08</th>\n",
              "      <th>Right.turn.intensity09</th>\n",
              "      <th>Right.turn.intensity10</th>\n",
              "      <th>Right.turn.intensity11</th>\n",
              "      <th>Right.turn.intensity12</th>\n",
              "      <th>NB_Claim</th>\n",
              "      <th>AMT_Claim</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.000000</td>\n",
              "      <td>100000.00000</td>\n",
              "      <td>100000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>314.204060</td>\n",
              "      <td>51.378950</td>\n",
              "      <td>5.639720</td>\n",
              "      <td>800.888870</td>\n",
              "      <td>9124.122908</td>\n",
              "      <td>28.839960</td>\n",
              "      <td>56.531390</td>\n",
              "      <td>0.502294</td>\n",
              "      <td>4833.575303</td>\n",
              "      <td>0.139365</td>\n",
              "      <td>...</td>\n",
              "      <td>551.574010</td>\n",
              "      <td>487.340690</td>\n",
              "      <td>447.758420</td>\n",
              "      <td>843.461830</td>\n",
              "      <td>565.056100</td>\n",
              "      <td>326.654840</td>\n",
              "      <td>246.713120</td>\n",
              "      <td>198.753690</td>\n",
              "      <td>0.04494</td>\n",
              "      <td>137.602253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>79.746222</td>\n",
              "      <td>15.467075</td>\n",
              "      <td>4.062135</td>\n",
              "      <td>83.382316</td>\n",
              "      <td>3826.144730</td>\n",
              "      <td>16.123717</td>\n",
              "      <td>24.036518</td>\n",
              "      <td>0.299189</td>\n",
              "      <td>4545.943016</td>\n",
              "      <td>0.042807</td>\n",
              "      <td>...</td>\n",
              "      <td>14687.929802</td>\n",
              "      <td>14198.331308</td>\n",
              "      <td>13719.790281</td>\n",
              "      <td>11630.185503</td>\n",
              "      <td>10657.402935</td>\n",
              "      <td>9460.244357</td>\n",
              "      <td>8977.569994</td>\n",
              "      <td>8585.177049</td>\n",
              "      <td>0.21813</td>\n",
              "      <td>1264.320056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>27.000000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>-2.000000</td>\n",
              "      <td>422.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>0.002740</td>\n",
              "      <td>0.095298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>200.000000</td>\n",
              "      <td>39.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>766.000000</td>\n",
              "      <td>6213.710000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.249315</td>\n",
              "      <td>1529.897500</td>\n",
              "      <td>0.120894</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>365.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>825.000000</td>\n",
              "      <td>7456.452000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.490411</td>\n",
              "      <td>3468.287765</td>\n",
              "      <td>0.137909</td>\n",
              "      <td>...</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>366.000000</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>856.000000</td>\n",
              "      <td>12427.420000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>0.753425</td>\n",
              "      <td>6779.876842</td>\n",
              "      <td>0.155203</td>\n",
              "      <td>...</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>680.000000</td>\n",
              "      <td>321.000000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>366.000000</td>\n",
              "      <td>103.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>900.000000</td>\n",
              "      <td>56731.172300</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>47282.603936</td>\n",
              "      <td>0.998172</td>\n",
              "      <td>...</td>\n",
              "      <td>794380.000000</td>\n",
              "      <td>793926.000000</td>\n",
              "      <td>793170.000000</td>\n",
              "      <td>841210.000000</td>\n",
              "      <td>841207.000000</td>\n",
              "      <td>841200.000000</td>\n",
              "      <td>841176.000000</td>\n",
              "      <td>841144.000000</td>\n",
              "      <td>3.00000</td>\n",
              "      <td>104074.886700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 48 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29b730b3-2f8d-414f-9983-7ac692bd8292')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29b730b3-2f8d-414f-9983-7ac692bd8292 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29b730b3-2f8d-414f-9983-7ac692bd8292');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-df833b25-7b52-460d-ba16-5f962d835ed6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df833b25-7b52-460d-ba16-5f962d835ed6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-df833b25-7b52-460d-ba16-5f962d835ed6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYk3fcETiesu",
        "outputId": "0717aeef-6ff5-42e3-c604-d4c890fea0ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 52 columns):\n",
            " #   Column                  Non-Null Count   Dtype  \n",
            "---  ------                  --------------   -----  \n",
            " 0   Duration                100000 non-null  int64  \n",
            " 1   Insured.age             100000 non-null  int64  \n",
            " 2   Insured.sex             100000 non-null  object \n",
            " 3   Car.age                 100000 non-null  int64  \n",
            " 4   Marital                 100000 non-null  object \n",
            " 5   Car.use                 100000 non-null  object \n",
            " 6   Credit.score            100000 non-null  float64\n",
            " 7   Region                  100000 non-null  object \n",
            " 8   Annual.miles.drive      100000 non-null  float64\n",
            " 9   Years.noclaims          100000 non-null  int64  \n",
            " 10  Territory               100000 non-null  int64  \n",
            " 11  Annual.pct.driven       100000 non-null  float64\n",
            " 12  Total.miles.driven      100000 non-null  float64\n",
            " 13  Pct.drive.mon           100000 non-null  float64\n",
            " 14  Pct.drive.tue           100000 non-null  float64\n",
            " 15  Pct.drive.wed           100000 non-null  float64\n",
            " 16  Pct.drive.thr           100000 non-null  float64\n",
            " 17  Pct.drive.fri           100000 non-null  float64\n",
            " 18  Pct.drive.sat           100000 non-null  float64\n",
            " 19  Pct.drive.sun           100000 non-null  float64\n",
            " 20  Pct.drive.2hrs          100000 non-null  float64\n",
            " 21  Pct.drive.3hrs          100000 non-null  float64\n",
            " 22  Pct.drive.4hrs          100000 non-null  float64\n",
            " 23  Pct.drive.wkday         100000 non-null  float64\n",
            " 24  Pct.drive.wkend         100000 non-null  float64\n",
            " 25  Pct.drive.rush am       100000 non-null  float64\n",
            " 26  Pct.drive.rush pm       100000 non-null  float64\n",
            " 27  Avgdays.week            100000 non-null  float64\n",
            " 28  Accel.06miles           100000 non-null  float64\n",
            " 29  Accel.08miles           100000 non-null  float64\n",
            " 30  Accel.09miles           100000 non-null  float64\n",
            " 31  Accel.11miles           100000 non-null  float64\n",
            " 32  Accel.12miles           100000 non-null  float64\n",
            " 33  Accel.14miles           100000 non-null  float64\n",
            " 34  Brake.06miles           100000 non-null  float64\n",
            " 35  Brake.08miles           100000 non-null  float64\n",
            " 36  Brake.09miles           100000 non-null  float64\n",
            " 37  Brake.11miles           100000 non-null  float64\n",
            " 38  Brake.12miles           100000 non-null  float64\n",
            " 39  Brake.14miles           100000 non-null  float64\n",
            " 40  Left.turn.intensity08   100000 non-null  float64\n",
            " 41  Left.turn.intensity09   100000 non-null  float64\n",
            " 42  Left.turn.intensity10   100000 non-null  float64\n",
            " 43  Left.turn.intensity11   100000 non-null  float64\n",
            " 44  Left.turn.intensity12   100000 non-null  float64\n",
            " 45  Right.turn.intensity08  100000 non-null  float64\n",
            " 46  Right.turn.intensity09  100000 non-null  float64\n",
            " 47  Right.turn.intensity10  100000 non-null  float64\n",
            " 48  Right.turn.intensity11  100000 non-null  float64\n",
            " 49  Right.turn.intensity12  100000 non-null  float64\n",
            " 50  NB_Claim                100000 non-null  int64  \n",
            " 51  AMT_Claim               100000 non-null  float64\n",
            "dtypes: float64(42), int64(6), object(4)\n",
            "memory usage: 39.7+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Preprocessing\n",
        "\n",
        "# Derive the target variable\n",
        "df['ClaimYN'] = ((df['NB_Claim'] >= 1) & (df['AMT_Claim'] > 1000)).astype(int)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(['NB_Claim', 'AMT_Claim'], axis=1)\n",
        "\n",
        "# Handle missing values (if any)\n",
        "df = df.dropna()\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X = df.drop('ClaimYN', axis=1)\n",
        "y = df['ClaimYN']\n",
        "\n",
        "# Normalize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "TD3smLImjBrH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "BF7Vx3xdlbJm",
        "outputId": "e7fa9c50-5661-485c-e95e-c93ec2c52290"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Duration  Insured.age  Car.age  Credit.score  Annual.miles.drive  \\\n",
              "0       366           45       -1         609.0             6213.71   \n",
              "1       182           44        3         575.0            12427.42   \n",
              "2       184           48        6         847.0            12427.42   \n",
              "3       183           71        6         842.0             6213.71   \n",
              "4       183           84       10         856.0             6213.71   \n",
              "\n",
              "   Years.noclaims  Territory  Annual.pct.driven  Total.miles.driven  \\\n",
              "0              25         70           0.849315         8864.376247   \n",
              "1              20         26           0.465753         8092.308208   \n",
              "2              14         84           0.520548         3225.832512   \n",
              "3              43         30           0.065753          253.024528   \n",
              "4              65         70           0.441096         4374.379634   \n",
              "\n",
              "   Pct.drive.mon  ...  Right.turn.intensity10  Right.turn.intensity11  \\\n",
              "0       0.148070  ...                     0.0                     0.0   \n",
              "1       0.147686  ...                   219.0                   101.0   \n",
              "2       0.153735  ...                     0.0                     0.0   \n",
              "3       0.106702  ...                     0.0                     0.0   \n",
              "4       0.123807  ...                    18.0                     4.0   \n",
              "\n",
              "   Right.turn.intensity12  ClaimYN  Insured.sex_Male  Marital_Single  \\\n",
              "0                     0.0        1              True           False   \n",
              "1                    40.0        0             False           False   \n",
              "2                     0.0        0             False           False   \n",
              "3                     0.0        0              True           False   \n",
              "4                     2.0        0              True           False   \n",
              "\n",
              "   Car.use_Commute  Car.use_Farmer  Car.use_Private  Region_Urban  \n",
              "0             True           False            False          True  \n",
              "1             True           False            False          True  \n",
              "2             True           False            False          True  \n",
              "3            False           False             True          True  \n",
              "4            False           False             True          True  \n",
              "\n",
              "[5 rows x 53 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88471b27-2dc6-46a4-9620-fa0cc29c566f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Duration</th>\n",
              "      <th>Insured.age</th>\n",
              "      <th>Car.age</th>\n",
              "      <th>Credit.score</th>\n",
              "      <th>Annual.miles.drive</th>\n",
              "      <th>Years.noclaims</th>\n",
              "      <th>Territory</th>\n",
              "      <th>Annual.pct.driven</th>\n",
              "      <th>Total.miles.driven</th>\n",
              "      <th>Pct.drive.mon</th>\n",
              "      <th>...</th>\n",
              "      <th>Right.turn.intensity10</th>\n",
              "      <th>Right.turn.intensity11</th>\n",
              "      <th>Right.turn.intensity12</th>\n",
              "      <th>ClaimYN</th>\n",
              "      <th>Insured.sex_Male</th>\n",
              "      <th>Marital_Single</th>\n",
              "      <th>Car.use_Commute</th>\n",
              "      <th>Car.use_Farmer</th>\n",
              "      <th>Car.use_Private</th>\n",
              "      <th>Region_Urban</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>366</td>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>609.0</td>\n",
              "      <td>6213.71</td>\n",
              "      <td>25</td>\n",
              "      <td>70</td>\n",
              "      <td>0.849315</td>\n",
              "      <td>8864.376247</td>\n",
              "      <td>0.148070</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>182</td>\n",
              "      <td>44</td>\n",
              "      <td>3</td>\n",
              "      <td>575.0</td>\n",
              "      <td>12427.42</td>\n",
              "      <td>20</td>\n",
              "      <td>26</td>\n",
              "      <td>0.465753</td>\n",
              "      <td>8092.308208</td>\n",
              "      <td>0.147686</td>\n",
              "      <td>...</td>\n",
              "      <td>219.0</td>\n",
              "      <td>101.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>184</td>\n",
              "      <td>48</td>\n",
              "      <td>6</td>\n",
              "      <td>847.0</td>\n",
              "      <td>12427.42</td>\n",
              "      <td>14</td>\n",
              "      <td>84</td>\n",
              "      <td>0.520548</td>\n",
              "      <td>3225.832512</td>\n",
              "      <td>0.153735</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>183</td>\n",
              "      <td>71</td>\n",
              "      <td>6</td>\n",
              "      <td>842.0</td>\n",
              "      <td>6213.71</td>\n",
              "      <td>43</td>\n",
              "      <td>30</td>\n",
              "      <td>0.065753</td>\n",
              "      <td>253.024528</td>\n",
              "      <td>0.106702</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>183</td>\n",
              "      <td>84</td>\n",
              "      <td>10</td>\n",
              "      <td>856.0</td>\n",
              "      <td>6213.71</td>\n",
              "      <td>65</td>\n",
              "      <td>70</td>\n",
              "      <td>0.441096</td>\n",
              "      <td>4374.379634</td>\n",
              "      <td>0.123807</td>\n",
              "      <td>...</td>\n",
              "      <td>18.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 53 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88471b27-2dc6-46a4-9620-fa0cc29c566f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-88471b27-2dc6-46a4-9620-fa0cc29c566f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-88471b27-2dc6-46a4-9620-fa0cc29c566f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-553a3704-27fc-4045-9311-52536a68538f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-553a3704-27fc-4045-9311-52536a68538f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-553a3704-27fc-4045-9311-52536a68538f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'ClaimYN' column from the DataFrame 'df'\n",
        "df['ClaimYN'].describe()  # Use .describe() for distribution statistics\n",
        "\n",
        "# Or, if you want the ClaimYN Series:\n",
        "ClaimYN_series = df['ClaimYN']\n",
        "ClaimYN_series.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "eZq_BMdVlbEN",
        "outputId": "a07c1c91-ab73-475d-ddc1-96b845efd022"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    100000.000000\n",
              "mean          0.026980\n",
              "std           0.162026\n",
              "min           0.000000\n",
              "25%           0.000000\n",
              "50%           0.000000\n",
              "75%           0.000000\n",
              "max           1.000000\n",
              "Name: ClaimYN, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ClaimYN</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.026980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.162026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Dual-Path Machine Learning Framework for Driver Risk Assessment\n",
        "# Complete Step-by-Step Implementation with Innovations\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: IMPORT REQUIRED LIBRARIES\n",
        "# =============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, RandomizedSearchCV, cross_val_score,\n",
        "    StratifiedKFold, learning_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import (\n",
        "    mutual_info_classif, SelectKBest, RFE, VarianceThreshold\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, matthews_corrcoef, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve, average_precision_score,\n",
        "    classification_report, log_loss\n",
        ")\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
        "\n",
        "# Class Imbalance Handling\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, LSTM,\n",
        "    GRU, Attention, MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "# Interpretability\n",
        "try:\n",
        "    import shap\n",
        "    import lime\n",
        "    import lime.lime_tabular\n",
        "    INTERPRETABILITY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    INTERPRETABILITY_AVAILABLE = False\n",
        "    print(\"SHAP/LIME not available. Install for interpretability features.\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: ENHANCED xLSTM IMPLEMENTATION\n",
        "# =============================================================================\n",
        "\n",
        "class xLSTMCell(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Enhanced xLSTM Cell with exponential gating and matrix memory\n",
        "    Innovation: Implements advanced memory mechanisms for temporal patterns\n",
        "    \"\"\"\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.state_size = [units, units, units]  # h, c, m (matrix memory)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Input gates with exponential gating\n",
        "        self.W_i = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  name='W_i', initializer='glorot_uniform')\n",
        "        self.U_i = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='U_i', initializer='orthogonal')\n",
        "        self.b_i = self.add_weight(shape=(self.units,), name='b_i',\n",
        "                                  initializer='zeros')\n",
        "\n",
        "        # Forget gates with enhanced memory control\n",
        "        self.W_f = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  name='W_f', initializer='glorot_uniform')\n",
        "        self.U_f = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='U_f', initializer='orthogonal')\n",
        "        self.b_f = self.add_weight(shape=(self.units,), name='b_f',\n",
        "                                  initializer='ones')\n",
        "\n",
        "        # Candidate values\n",
        "        self.W_c = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  name='W_c', initializer='glorot_uniform')\n",
        "        self.U_c = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='U_c', initializer='orthogonal')\n",
        "        self.b_c = self.add_weight(shape=(self.units,), name='b_c',\n",
        "                                  initializer='zeros')\n",
        "\n",
        "        # Output gates\n",
        "        self.W_o = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  name='W_o', initializer='glorot_uniform')\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='U_o', initializer='orthogonal')\n",
        "        self.b_o = self.add_weight(shape=(self.units,), name='b_o',\n",
        "                                  initializer='zeros')\n",
        "\n",
        "        # Matrix memory weights (Innovation)\n",
        "        self.W_m = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                                  name='W_m', initializer='glorot_uniform')\n",
        "        self.U_m = self.add_weight(shape=(self.units, self.units),\n",
        "                                  name='U_m', initializer='orthogonal')\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        h_prev, c_prev, m_prev = states\n",
        "\n",
        "        # Exponential input gate (Innovation)\n",
        "        i = tf.nn.sigmoid(tf.matmul(inputs, self.W_i) +\n",
        "                         tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        i = tf.exp(i) / (tf.exp(i) + 1)  # Exponential gating\n",
        "\n",
        "        # Enhanced forget gate with matrix memory influence\n",
        "        f = tf.nn.sigmoid(tf.matmul(inputs, self.W_f) +\n",
        "                         tf.matmul(h_prev, self.U_f) +\n",
        "                         tf.reduce_mean(m_prev, axis=-1, keepdims=True) + self.b_f)\n",
        "\n",
        "        # Candidate values\n",
        "        c_candidate = tf.nn.tanh(tf.matmul(inputs, self.W_c) +\n",
        "                               tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "\n",
        "        # Matrix memory update (Innovation)\n",
        "        m_candidate = tf.nn.tanh(tf.matmul(inputs, self.W_m) +\n",
        "                               tf.matmul(h_prev, self.U_m))\n",
        "        m_new = f * m_prev + i * tf.expand_dims(m_candidate, axis=-1)\n",
        "\n",
        "        # Cell state update with matrix memory influence\n",
        "        c_new = f * c_prev + i * c_candidate * tf.reduce_mean(m_new, axis=-1)\n",
        "\n",
        "        # Output gate\n",
        "        o = tf.nn.sigmoid(tf.matmul(inputs, self.W_o) +\n",
        "                         tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "\n",
        "        # Hidden state with matrix memory\n",
        "        h_new = o * tf.nn.tanh(c_new + tf.reduce_mean(m_new, axis=-1))\n",
        "\n",
        "        return h_new, [h_new, c_new, m_new]\n",
        "\n",
        "class xLSTMLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"xLSTM Layer wrapper for sequential processing\"\"\"\n",
        "    def __init__(self, units, return_sequences=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.return_sequences = return_sequences\n",
        "        self.cell = xLSTMCell(units)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        seq_len = tf.shape(inputs)[1]\n",
        "\n",
        "        # Initialize states\n",
        "        h = tf.zeros((batch_size, self.units))\n",
        "        c = tf.zeros((batch_size, self.units))\n",
        "        m = tf.zeros((batch_size, self.units, self.units))\n",
        "        states = [h, c, m]\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            output, states = self.cell(inputs[:, t, :], states)\n",
        "            outputs.append(output)\n",
        "\n",
        "        if self.return_sequences:\n",
        "            return tf.stack(outputs, axis=1)\n",
        "        else:\n",
        "            return outputs[-1]\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: COMPREHENSIVE DATA PREPROCESSING CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Comprehensive data preprocessing with multiple scaling strategies\n",
        "    and advanced feature engineering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scaling_method='standard', handle_outliers=True):\n",
        "        self.scaling_method = scaling_method\n",
        "        self.handle_outliers = handle_outliers\n",
        "        self.scalers = {}\n",
        "        self.feature_names = None\n",
        "        self.categorical_features = []\n",
        "        self.numerical_features = []\n",
        "\n",
        "    def detect_feature_types(self, df):\n",
        "        \"\"\"Automatically detect categorical and numerical features\"\"\"\n",
        "        categorical = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "        numerical = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "        return categorical, numerical\n",
        "\n",
        "    def handle_missing_values(self, df):\n",
        "        \"\"\"Advanced missing value handling\"\"\"\n",
        "        # Handle negative Car.age values as missing\n",
        "        if 'Car.age' in df.columns:\n",
        "            df.loc[df['Car.age'] < 0, 'Car.age'] = np.nan\n",
        "            df['Car.age'].fillna(df['Car.age'].median(), inplace=True)\n",
        "\n",
        "        # Fill other missing values\n",
        "        for col in df.columns:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                if df[col].dtype in ['object', 'category']:\n",
        "                    df[col].fillna(df[col].mode().iloc[0], inplace=True)\n",
        "                else:\n",
        "                    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_outliers(self, df, method='iqr', threshold=3):\n",
        "        \"\"\"Remove outliers using IQR or Z-score method\"\"\"\n",
        "        if not self.handle_outliers:\n",
        "            return df\n",
        "\n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if method == 'iqr':\n",
        "            for col in numerical_cols:\n",
        "                Q1 = df[col].quantile(0.25)\n",
        "                Q3 = df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
        "\n",
        "        elif method == 'zscore':\n",
        "            for col in numerical_cols:\n",
        "                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
        "                df = df[z_scores < threshold]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def log_transform_skewed_features(self, df, skewness_threshold=0.75):\n",
        "        \"\"\"Apply log transformation to highly skewed features\"\"\"\n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            if df[col].min() >= 0:  # Can only log-transform non-negative values\n",
        "                skewness = df[col].skew()\n",
        "                if abs(skewness) > skewness_threshold:\n",
        "                    # Add small constant to handle zeros\n",
        "                    df[f'{col}_log'] = np.log1p(df[col])\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_interaction_features(self, df):\n",
        "        \"\"\"Create interaction features for behavioral metrics\"\"\"\n",
        "        # Acceleration-Brake intensity combinations\n",
        "        accel_cols = [col for col in df.columns if 'Accel' in col]\n",
        "        brake_cols = [col for col in df.columns if 'Brake' in col]\n",
        "\n",
        "        for accel_col in accel_cols:\n",
        "            for brake_col in brake_cols:\n",
        "                if accel_col.split('.')[-1] == brake_col.split('.')[-1]:  # Same mile threshold\n",
        "                    df[f'{accel_col}_x_{brake_col}'] = df[accel_col] * df[brake_col]\n",
        "\n",
        "        # Turn intensity ratios\n",
        "        left_turn_cols = [col for col in df.columns if 'Left.turn.intensity' in col]\n",
        "        right_turn_cols = [col for col in df.columns if 'Right.turn.intensity' in col]\n",
        "\n",
        "        for left_col in left_turn_cols:\n",
        "            for right_col in right_turn_cols:\n",
        "                if left_col.split('intensity')[-1] == right_col.split('intensity')[-1]:\n",
        "                    # Avoid division by zero\n",
        "                    df[f'{left_col}_ratio_{right_col}'] = (\n",
        "                        df[left_col] / (df[right_col] + 1e-8)\n",
        "                    )\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit preprocessor and transform data\"\"\"\n",
        "        df = X.copy()\n",
        "\n",
        "        # Store original feature names\n",
        "        self.feature_names = df.columns.tolist()\n",
        "\n",
        "        # Detect feature types\n",
        "        self.categorical_features, self.numerical_features = self.detect_feature_types(df)\n",
        "\n",
        "        # Handle missing values\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Remove outliers\n",
        "        if y is not None:\n",
        "            original_indices = df.index\n",
        "            df = self.remove_outliers(df)\n",
        "            # Filter y to match filtered df\n",
        "            y = y.loc[df.index] if hasattr(y, 'loc') else y[df.index]\n",
        "\n",
        "        # Log transform skewed features\n",
        "        df = self.log_transform_skewed_features(df)\n",
        "\n",
        "        # Create interaction features\n",
        "        df = self.create_interaction_features(df)\n",
        "\n",
        "        # One-hot encode categorical features\n",
        "        if self.categorical_features:\n",
        "            df = pd.get_dummies(df, columns=self.categorical_features, drop_first=True)\n",
        "\n",
        "        # Update numerical features after transformations\n",
        "        self.numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        # Apply scaling\n",
        "        if self.scaling_method == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif self.scaling_method == 'robust':\n",
        "            scaler = RobustScaler()\n",
        "        elif self.scaling_method == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "\n",
        "        df[self.numerical_features] = scaler.fit_transform(df[self.numerical_features])\n",
        "        self.scalers['numerical'] = scaler\n",
        "\n",
        "        return df, y\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform new data using fitted preprocessor\"\"\"\n",
        "        df = X.copy()\n",
        "\n",
        "        # Handle missing values\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Log transform skewed features\n",
        "        df = self.log_transform_skewed_features(df)\n",
        "\n",
        "        # Create interaction features\n",
        "        df = self.create_interaction_features(df)\n",
        "\n",
        "        # One-hot encode categorical features\n",
        "        if self.categorical_features:\n",
        "            df = pd.get_dummies(df, columns=self.categorical_features, drop_first=True)\n",
        "\n",
        "        # Ensure all columns are present\n",
        "        for col in self.numerical_features:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0\n",
        "\n",
        "        # Apply scaling\n",
        "        if 'numerical' in self.scalers:\n",
        "            df[self.numerical_features] = self.scalers['numerical'].transform(\n",
        "                df[self.numerical_features]\n",
        "            )\n",
        "\n",
        "        return df\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: ADVANCED CLASS IMBALANCE HANDLER\n",
        "# =============================================================================\n",
        "\n",
        "class AdvancedImbalanceHandler:\n",
        "    \"\"\"\n",
        "    Advanced class imbalance handling with multiple techniques\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, method='smote_tomek', random_state=42):\n",
        "        self.method = method\n",
        "        self.random_state = random_state\n",
        "        self.sampler = None\n",
        "\n",
        "    def get_sampler(self):\n",
        "        \"\"\"Get appropriate sampler based on method\"\"\"\n",
        "        if self.method == 'smote':\n",
        "            return SMOTE(random_state=self.random_state)\n",
        "        elif self.method == 'smote_tomek':\n",
        "            return SMOTETomek(random_state=self.random_state)\n",
        "        elif self.method == 'adasyn':\n",
        "            return ADASYN(random_state=self.random_state)\n",
        "        elif self.method == 'borderline_smote':\n",
        "            return BorderlineSMOTE(random_state=self.random_state)\n",
        "        else:\n",
        "            return SMOTE(random_state=self.random_state)\n",
        "\n",
        "    def fit_resample(self, X, y):\n",
        "        \"\"\"Apply resampling to training data\"\"\"\n",
        "        self.sampler = self.get_sampler()\n",
        "        X_resampled, y_resampled = self.sampler.fit_resample(X, y)\n",
        "\n",
        "        print(f\"Original class distribution: {np.bincount(y)}\")\n",
        "        print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
        "\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 5: ENHANCED FEATURE SELECTOR\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedFeatureSelector:\n",
        "    \"\"\"\n",
        "    Multi-method feature selection with ensemble approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features=20, methods=['mutual_info', 'rfe', 'variance']):\n",
        "        self.n_features = n_features\n",
        "        self.methods = methods\n",
        "        self.selected_features = None\n",
        "        self.feature_scores = {}\n",
        "\n",
        "    def mutual_info_selection(self, X, y):\n",
        "        \"\"\"Feature selection using mutual information\"\"\"\n",
        "        mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "        feature_scores = dict(zip(X.columns, mi_scores))\n",
        "        top_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [feat[0] for feat in top_features[:self.n_features]], feature_scores\n",
        "\n",
        "    def rfe_selection(self, X, y):\n",
        "        \"\"\"Recursive Feature Elimination\"\"\"\n",
        "        estimator = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
        "        selector = RFE(estimator, n_features_to_select=self.n_features)\n",
        "        selector.fit(X, y)\n",
        "\n",
        "        feature_scores = dict(zip(X.columns, selector.ranking_))\n",
        "        selected_features = X.columns[selector.support_].tolist()\n",
        "        return selected_features, feature_scores\n",
        "\n",
        "    def variance_selection(self, X, threshold=0.01):\n",
        "        \"\"\"Remove low-variance features\"\"\"\n",
        "        selector = VarianceThreshold(threshold=threshold)\n",
        "        selector.fit(X)\n",
        "        selected_features = X.columns[selector.get_support()].tolist()\n",
        "        feature_scores = dict(zip(X.columns, selector.variances_))\n",
        "        return selected_features, feature_scores\n",
        "\n",
        "    def permutation_importance_selection(self, X, y):\n",
        "        \"\"\"Feature selection using permutation importance\"\"\"\n",
        "        estimator = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
        "        estimator.fit(X, y)\n",
        "\n",
        "        perm_importance = permutation_importance(estimator, X, y, n_repeats=5, random_state=42)\n",
        "        feature_scores = dict(zip(X.columns, perm_importance.importances_mean))\n",
        "        top_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "        return [feat[0] for feat in top_features[:self.n_features]], feature_scores\n",
        "\n",
        "    def ensemble_selection(self, X, y):\n",
        "        \"\"\"Ensemble feature selection combining multiple methods\"\"\"\n",
        "        all_selected_features = []\n",
        "        method_scores = {}\n",
        "\n",
        "        if 'mutual_info' in self.methods:\n",
        "            features, scores = self.mutual_info_selection(X, y)\n",
        "            all_selected_features.extend(features)\n",
        "            method_scores['mutual_info'] = scores\n",
        "\n",
        "        if 'rfe' in self.methods:\n",
        "            features, scores = self.rfe_selection(X, y)\n",
        "            all_selected_features.extend(features)\n",
        "            method_scores['rfe'] = scores\n",
        "\n",
        "        if 'variance' in self.methods:\n",
        "            features, scores = self.variance_selection(X)\n",
        "            all_selected_features.extend(features)\n",
        "            method_scores['variance'] = scores\n",
        "\n",
        "        if 'permutation' in self.methods:\n",
        "            features, scores = self.permutation_importance_selection(X, y)\n",
        "            all_selected_features.extend(features)\n",
        "            method_scores['permutation'] = scores\n",
        "\n",
        "        # Count feature frequency across methods\n",
        "        feature_counts = defaultdict(int)\n",
        "        for feature in all_selected_features:\n",
        "            feature_counts[feature] += 1\n",
        "\n",
        "        # Select features that appear in multiple methods\n",
        "        ensemble_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "        self.selected_features = [feat[0] for feat in ensemble_features[:self.n_features]]\n",
        "        self.feature_scores = method_scores\n",
        "\n",
        "        return self.selected_features, method_scores\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 6: ENHANCED GRADIENT BOOSTING MODEL\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedGradientBoosting:\n",
        "    \"\"\"Enhanced Gradient Boosting with advanced hyperparameter optimization\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.best_params = None\n",
        "        self.cv_results = None\n",
        "\n",
        "    def get_param_grid(self):\n",
        "        \"\"\"Enhanced parameter grid for optimization\"\"\"\n",
        "        return {\n",
        "            'n_estimators': [100, 200, 300],\n",
        "            'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7, 9],\n",
        "            'min_samples_split': [2, 5, 10],\n",
        "            'min_samples_leaf': [1, 2, 4],\n",
        "            'subsample': [0.8, 0.9, 1.0],\n",
        "            'max_features': ['sqrt', 'log2', None]\n",
        "        }\n",
        "\n",
        "    def optimize_hyperparameters(self, X, y, cv_folds=5, n_iter=50):\n",
        "        \"\"\"Optimize hyperparameters using RandomizedSearchCV\"\"\"\n",
        "        gb_classifier = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            estimator=gb_classifier,\n",
        "            param_distributions=self.get_param_grid(),\n",
        "            n_iter=n_iter,\n",
        "            cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),\n",
        "            scoring='roc_auc',\n",
        "            n_jobs=-1,\n",
        "            random_state=42,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        search.fit(X, y)\n",
        "        self.best_params = search.best_params_\n",
        "        self.cv_results = search.cv_results_\n",
        "        self.model = search.best_estimator_\n",
        "\n",
        "        print(\"Best Gradient Boosting Parameters:\")\n",
        "        for param, value in self.best_params.items():\n",
        "            print(f\"  {param}: {value}\")\n",
        "        print(f\"Best CV Score: {search.best_score_:.4f}\")\n",
        "\n",
        "        return self.model\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 7: ENHANCED NEURAL NETWORK WITH xLSTM\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedNeuralNetwork:\n",
        "    \"\"\"Enhanced Neural Network with xLSTM integration\"\"\"\n",
        "\n",
        "    def __init__(self, include_xlstm=True):\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.include_xlstm = include_xlstm\n",
        "\n",
        "    def create_mlp_model(self, input_dim, hidden_layers=[128, 64, 32]):\n",
        "        \"\"\"Create Multi-Layer Perceptron model\"\"\"\n",
        "        inputs = Input(shape=(input_dim,))\n",
        "        x = inputs\n",
        "\n",
        "        for i, units in enumerate(hidden_layers):\n",
        "            x = Dense(units, activation='relu',\n",
        "                     kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Dropout(0.3)(x)\n",
        "\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def create_xlstm_enhanced_model(self, input_dim, sequence_features=None):\n",
        "        \"\"\"Create xLSTM-enhanced model for sequential features\"\"\"\n",
        "        if sequence_features is None:\n",
        "            sequence_features = ['Daily.Driving.Percentage', 'Night.Driving.Percentage',\n",
        "                               'Avg.Speed.Per.Trip']\n",
        "\n",
        "        # Main input\n",
        "        main_input = Input(shape=(input_dim,), name='main_input')\n",
        "\n",
        "        # Sequential features processing with xLSTM\n",
        "        if self.include_xlstm:\n",
        "            # Reshape sequential features for xLSTM processing\n",
        "            seq_input = Input(shape=(len(sequence_features), 1), name='seq_input')\n",
        "            xlstm_output = xLSTMLayer(64, return_sequences=False)(seq_input)\n",
        "            xlstm_dense = Dense(32, activation='relu')(xlstm_output)\n",
        "\n",
        "            # Combine main features with xLSTM output\n",
        "            combined = keras.layers.concatenate([main_input, xlstm_dense])\n",
        "        else:\n",
        "            combined = main_input\n",
        "\n",
        "        # MLP layers\n",
        "        x = Dense(128, activation='relu',\n",
        "                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(combined)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Dense(64, activation='relu',\n",
        "                 kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        if self.include_xlstm:\n",
        "            model = Model(inputs=[main_input, seq_input], outputs=outputs)\n",
        "        else:\n",
        "            model = Model(inputs=main_input, outputs=outputs)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def compile_and_train(self, model, X_train, y_train, X_val, y_val,\n",
        "                         epochs=100, batch_size=32):\n",
        "        \"\"\"Compile and train the neural network\"\"\"\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', 'precision', 'recall']\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7)\n",
        "        ]\n",
        "\n",
        "        if isinstance(X_train, list):  # xLSTM model with multiple inputs\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "        else:  # Standard MLP model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                validation_data=(X_val, y_val),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "        self.model = model\n",
        "        self.history = history\n",
        "        return model, history\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 8: WEIGHTED ENSEMBLE INTEGRATOR\n",
        "# =============================================================================\n",
        "\n",
        "class WeightedEnsemble:\n",
        "    \"\"\"Advanced weighted ensemble with dynamic weight calculation\"\"\"\n",
        "\n",
        "    def __init__(self, models, weight_method='accuracy'):\n",
        "        self.models = models\n",
        "        self.weight_method = weight_method\n",
        "        self.weights = None\n",
        "\n",
        "    def calculate_weights(self, X_val, y_val):\n",
        "        \"\"\"Calculate dynamic weights based on validation performance\"\"\"\n",
        "        accuracies = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                y_pred = model.predict_proba(X_val)[:, 1]\n",
        "            else:\n",
        "                y_pred = model.predict(X_val)\n",
        "\n",
        "            if self.weight_method == 'accuracy':\n",
        "                y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "                accuracies[name] = accuracy_score(y_val, y_pred_binary)\n",
        "            elif self.weight_method == 'auc':\n",
        "                accuracies[name] = roc_auc_score(y_val, y_pred)\n",
        "            elif self.weight_method == 'f1':\n",
        "                y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "                accuracies[name] = f1_score(y_val, y_pred_binary)\n",
        "\n",
        "        # Normalize weights\n",
        "        total_accuracy = sum(accuracies.values())\n",
        "        self.weights = {name: acc/total_accuracy for name, acc in accuracies.items()}\n",
        "\n",
        "        print(\"Ensemble Weights:\")\n",
        "        for name, weight in self.weights.items():\n",
        "            print(f\"  {name}: {weight:.4f}\")\n",
        "\n",
        "        return self.weights\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Generate ensemble predictions\"\"\"\n",
        "        predictions = {}\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                predictions[name] = model.predict_proba(X)[:, 1]\n",
        "            else:\n",
        "                predictions[name] = model.predict(X)\n",
        "\n",
        "        # Weighted average\n",
        "        ensemble_pred = np.zeros(len(X))\n",
        "        for name, pred in predictions.items():\n",
        "            ensemble_pred += self.weights[name] * pred\n",
        "\n",
        "        return ensemble_pred\n",
        "\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        \"\"\"Generate binary predictions\"\"\"\n",
        "        proba = self.predict_proba(X)\n",
        "        return (proba > threshold).astype(int)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 9: THRESHOLD OPTIMIZER\n",
        "# =============================================================================\n",
        "\n",
        "class ThresholdOptimizer:\n",
        "    \"\"\"Optimize classification threshold for specific metrics\"\"\"\n",
        "\n",
        "    def __init__(self, metric='f1'):\n",
        "        self.metric = metric\n",
        "        self.optimal_threshold = 0.5\n",
        "\n",
        "    def optimize(self, y_true, y_pred_proba):\n",
        "        \"\"\"Find optimal threshold\"\"\"\n",
        "        thresholds = np.linspace(0.01, 0.99, 100)\n",
        "        best_score = 0\n",
        "        best_threshold = 0.5\n",
        "\n",
        "        for threshold in thresholds:\n",
        "            y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "            if self.metric == 'f1':\n",
        "                score = f1_score(y_true, y_pred, zero_division=0)\n",
        "            elif self.metric == 'precision':\n",
        "                score = precision_score(y_true, y_pred, zero_division=0)\n",
        "            elif self.metric == 'recall':\n",
        "                score = recall_score(y_true, y_pred, zero_division=0)\n",
        "            elif self.metric == 'balanced':\n",
        "                precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "                score = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_threshold = threshold\n",
        "\n",
        "        self.optimal_threshold = best_threshold\n",
        "        print(f\"Optimal threshold for {self.metric}: {best_threshold:.4f}\")\n",
        "        print(f\"Best {self.metric} score: {best_score:.4f}\")\n",
        "\n",
        "        return best_threshold\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 10: COMPREHENSIVE MODEL EVALUATOR\n",
        "# =============================================================================\n",
        "\n",
        "class ComprehensiveEvaluator:\n",
        "    \"\"\"Comprehensive model evaluation with advanced metrics and visualizations\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.results = {}\n",
        "\n",
        "    def calculate_all_metrics(self, y_true, y_pred, y_pred_proba, model_name):\n",
        "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "            'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "            'f1': f1_score(y_true, y_pred, zero_division=0),\n",
        "            'auc_roc': roc_auc_score(y_true, y_pred_proba),\n",
        "            'mcc': matthews_corrcoef(y_true, y_pred),\n",
        "            'log_loss': log_loss(y_true, y_pred_proba),\n",
        "            'avg_precision': average_precision_score(y_true, y_pred_proba)\n",
        "        }\n",
        "\n",
        "        self.results[model_name] = metrics\n",
        "        return metrics\n",
        "\n",
        "    def plot_comprehensive_evaluation(self, y_true, y_pred_proba_dict):\n",
        "        \"\"\"Create comprehensive evaluation plots\"\"\"\n",
        "        n_models = len(y_pred_proba_dict)\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "        # ROC Curves\n",
        "        ax = axes[0, 0]\n",
        "        for model_name, y_pred_proba in y_pred_proba_dict.items():\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
        "            auc_score = roc_auc_score(y_true, y_pred_proba)\n",
        "            ax.plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
        "\n",
        "        ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
        "        ax.set_xlabel('False Positive Rate')\n",
        "        ax.set_ylabel('True Positive Rate')\n",
        "        ax.set_title('ROC Curves')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Precision-Recall Curves\n",
        "        ax = axes[0, 1]\n",
        "        for model_name, y_pred_proba in y_pred_proba_dict.items():\n",
        "            precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
        "            avg_precision = average_precision_score(y_true, y_pred_proba)\n",
        "            ax.plot(recall, precision, label=f'{model_name} (AP = {avg_precision:.3f})')\n",
        "\n",
        "        ax.set_xlabel('Recall')\n",
        "        ax.set_ylabel('Precision')\n",
        "        ax.set_title('Precision-Recall Curves')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Model Comparison Bar Chart\n",
        "        ax = axes[0, 2]\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc']\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.8 / len(y_pred_proba_dict)\n",
        "\n",
        "        for i, (model_name, _) in enumerate(y_pred_proba_dict.items()):\n",
        "            if model_name in self.results:\n",
        "                values = [self.results[model_name][metric] for metric in metrics]\n",
        "                ax.bar(x + i * width, values, width, label=model_name, alpha=0.8)\n",
        "\n",
        "        ax.set_xlabel('Metrics')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Model Comparison')\n",
        "        ax.set_xticks(x + width/2)\n",
        "        ax.set_xticklabels(metrics, rotation=45)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Confusion Matrices\n",
        "        if len(y_pred_proba_dict) <= 3:\n",
        "            for i, (model_name, y_pred_proba) in enumerate(y_pred_proba_dict.items()):\n",
        "                ax = axes[1, i]\n",
        "                y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "                cm = confusion_matrix(y_true, y_pred)\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "                ax.set_title(f'{model_name} Confusion Matrix')\n",
        "                ax.set_xlabel('Predicted')\n",
        "                ax.set_ylabel('Actual')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def print_results_table(self):\n",
        "        \"\"\"Print formatted results table\"\"\"\n",
        "        if not self.results:\n",
        "            print(\"No results to display\")\n",
        "            return\n",
        "\n",
        "        # Create DataFrame for better formatting\n",
        "        df_results = pd.DataFrame(self.results).T\n",
        "        df_results = df_results.round(4)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE MODEL EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "        print(df_results.to_string())\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 11: MODEL INTERPRETABILITY SUITE\n",
        "# =============================================================================\n",
        "\n",
        "class InterpretabilitySuite:\n",
        "    \"\"\"Comprehensive model interpretability using SHAP and LIME\"\"\"\n",
        "\n",
        "    def __init__(self, models, feature_names):\n",
        "        self.models = models\n",
        "        self.feature_names = feature_names\n",
        "        self.explainers = {}\n",
        "\n",
        "    def setup_shap_explainers(self, X_background, sample_size=100):\n",
        "        \"\"\"Setup SHAP explainers for all models\"\"\"\n",
        "        if not INTERPRETABILITY_AVAILABLE:\n",
        "            print(\"SHAP not available. Install shap package for interpretability.\")\n",
        "            return\n",
        "\n",
        "        # Sample background data\n",
        "        if len(X_background) > sample_size:\n",
        "            background_sample = shap.sample(X_background, sample_size)\n",
        "        else:\n",
        "            background_sample = X_background\n",
        "\n",
        "        for model_name, model in self.models.items():\n",
        "            try:\n",
        "                if hasattr(model, 'predict_proba'):\n",
        "                    self.explainers[model_name] = shap.KernelExplainer(\n",
        "                        lambda x: model.predict_proba(x)[:, 1],\n",
        "                        background_sample\n",
        "                    )\n",
        "                else:\n",
        "                    self.explainers[model_name] = shap.KernelExplainer(\n",
        "                        model.predict, background_sample\n",
        "                    )\n",
        "                print(f\"SHAP explainer ready for {model_name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to create SHAP explainer for {model_name}: {e}\")\n",
        "\n",
        "    def explain_predictions(self, X_explain, model_name, max_display=10):\n",
        "        \"\"\"Generate SHAP explanations for predictions\"\"\"\n",
        "        if not INTERPRETABILITY_AVAILABLE or model_name not in self.explainers:\n",
        "            print(f\"SHAP explainer not available for {model_name}\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            shap_values = self.explainers[model_name].shap_values(X_explain)\n",
        "\n",
        "            # Summary plot\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            shap.summary_plot(shap_values, X_explain,\n",
        "                            feature_names=self.feature_names[:X_explain.shape[1]],\n",
        "                            max_display=max_display, show=False)\n",
        "            plt.title(f'SHAP Summary Plot - {model_name}')\n",
        "            plt.show()\n",
        "\n",
        "            return shap_values\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to generate SHAP explanations for {model_name}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def setup_lime_explainer(self, X_train):\n",
        "        \"\"\"Setup LIME explainer\"\"\"\n",
        "        if not INTERPRETABILITY_AVAILABLE:\n",
        "            print(\"LIME not available. Install lime package for interpretability.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "                X_train,\n",
        "                feature_names=self.feature_names[:X_train.shape[1]],\n",
        "                class_names=['No Claim', 'Claim'],\n",
        "                mode='classification'\n",
        "            )\n",
        "            return explainer\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to create LIME explainer: {e}\")\n",
        "            return None\n",
        "\n",
        "    def explain_instance_lime(self, lime_explainer, instance, model, model_name):\n",
        "        \"\"\"Explain single instance using LIME\"\"\"\n",
        "        if lime_explainer is None:\n",
        "            print(\"LIME explainer not available\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                explanation = lime_explainer.explain_instance(\n",
        "                    instance, model.predict_proba, num_features=10\n",
        "                )\n",
        "            else:\n",
        "                explanation = lime_explainer.explain_instance(\n",
        "                    instance, lambda x: np.column_stack([1-model.predict(x), model.predict(x)]),\n",
        "                    num_features=10\n",
        "                )\n",
        "\n",
        "            explanation.show_in_notebook(show_table=True)\n",
        "            return explanation\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to generate LIME explanation: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6mm6UeafMBX",
        "outputId": "4bdcf537-7041-40a9-a872-cba091fb4e6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SHAP/LIME not available. Install for interpretability features.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SHAP lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSkwCDwvgwDc",
        "outputId": "50f2e444-90f3-4fb0-a8f6-2411cb56fb2a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: SHAP in /usr/local/lib/python3.11/dist-packages (0.48.0)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/275.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from SHAP) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from SHAP) (1.16.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from SHAP) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from SHAP) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from SHAP) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from SHAP) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from SHAP) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from SHAP) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from SHAP) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from SHAP) (4.14.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->SHAP) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->SHAP) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->SHAP) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->SHAP) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->SHAP) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=934b326f781dc166b1891bf4d567752c429dd256cd77d770b288f2527297c431\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/fa/a3/9c2d44c9f3cd77cf4e533b58900b2bf4487f2a17e8ec212a3d\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# STEP 12: MAIN DUAL-PATH FRAMEWORK CLASS\n",
        "# =============================================================================\n",
        "\n",
        "class DualPathFramework:\n",
        "    \"\"\"\n",
        "    Main class implementing the complete dual-path machine learning framework\n",
        "    with all enhancements and innovations\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or self._get_default_config()\n",
        "        self.preprocessor = None\n",
        "        self.imbalance_handler = None\n",
        "        self.feature_selector = None\n",
        "        self.gb_model = None\n",
        "        self.nn_model = None\n",
        "        self.ensemble = None\n",
        "        self.threshold_optimizer = None\n",
        "        self.evaluator = None\n",
        "        self.interpretability_suite = None\n",
        "        self.results = {}\n",
        "\n",
        "    def _get_default_config(self):\n",
        "        \"\"\"Default configuration for the framework\"\"\"\n",
        "        return {\n",
        "            'preprocessing': {\n",
        "                'scaling_method': 'standard',\n",
        "                'handle_outliers': True,\n",
        "                'create_interactions': True\n",
        "            },\n",
        "            'imbalance_handling': {\n",
        "                'method': 'smote_tomek',\n",
        "                'random_state': 42\n",
        "            },\n",
        "            'feature_selection': {\n",
        "                'n_features': 20,\n",
        "                'methods': ['mutual_info', 'rfe', 'permutation']\n",
        "            },\n",
        "            'models': {\n",
        "                'gb_optimization': {\n",
        "                    'cv_folds': 5,\n",
        "                    'n_iter': 30\n",
        "                },\n",
        "                'nn_config': {\n",
        "                    'include_xlstm': True,\n",
        "                    'epochs': 100,\n",
        "                    'batch_size': 32\n",
        "                }\n",
        "            },\n",
        "            'ensemble': {\n",
        "                'weight_method': 'auc'\n",
        "            },\n",
        "            'threshold_optimization': {\n",
        "                'metric': 'f1'\n",
        "            },\n",
        "            'interpretability': {\n",
        "                'enable_shap': True,\n",
        "                'enable_lime': True,\n",
        "                'background_sample_size': 100\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path):\n",
        "        \"\"\"Load and preprocess the telematics dataset\"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded dataset with shape: {df.shape}\")\n",
        "\n",
        "        # Create target variable\n",
        "        df['ClaimYN'] = ((df['NB_Claim'] >= 1) & (df['AMT_Claim'] > 1000)).astype(int)\n",
        "\n",
        "        # Remove original claim columns\n",
        "        df = df.drop(['NB_Claim', 'AMT_Claim'], axis=1)\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df.drop('ClaimYN', axis=1)\n",
        "        y = df['ClaimYN']\n",
        "\n",
        "        print(f\"Class distribution: {y.value_counts().to_dict()}\")\n",
        "        print(f\"Class imbalance ratio: {y.value_counts()[0] / y.value_counts()[1]:.2f}\")\n",
        "\n",
        "        # Initialize and apply preprocessing\n",
        "        self.preprocessor = EnhancedDataPreprocessor(\n",
        "            scaling_method=self.config['preprocessing']['scaling_method'],\n",
        "            handle_outliers=self.config['preprocessing']['handle_outliers']\n",
        "        )\n",
        "\n",
        "        X_processed, y_processed = self.preprocessor.fit_transform(X, y)\n",
        "\n",
        "        print(f\"Processed dataset shape: {X_processed.shape}\")\n",
        "        print(f\"Added {X_processed.shape[1] - X.shape[1]} new features\")\n",
        "\n",
        "        return X_processed, y_processed\n",
        "\n",
        "    def split_data(self, X, y, test_size=0.15, val_size=0.15):\n",
        "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
        "        # First split: separate test set\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Second split: separate train and validation from remaining data\n",
        "        val_size_adjusted = val_size / (1 - test_size)  # Adjust val_size for remaining data\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        print(f\"Data split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def handle_class_imbalance(self, X_train, y_train):\n",
        "        \"\"\"Apply class imbalance handling techniques\"\"\"\n",
        "        print(\"Handling class imbalance...\")\n",
        "\n",
        "        self.imbalance_handler = AdvancedImbalanceHandler(\n",
        "            method=self.config['imbalance_handling']['method'],\n",
        "            random_state=self.config['imbalance_handling']['random_state']\n",
        "        )\n",
        "\n",
        "        X_resampled, y_resampled = self.imbalance_handler.fit_resample(X_train, y_train)\n",
        "\n",
        "        return X_resampled, y_resampled\n",
        "\n",
        "    def select_features(self, X_train, y_train):\n",
        "        \"\"\"Perform advanced feature selection\"\"\"\n",
        "        print(\"Performing feature selection...\")\n",
        "\n",
        "        self.feature_selector = EnhancedFeatureSelector(\n",
        "            n_features=self.config['feature_selection']['n_features'],\n",
        "            methods=self.config['feature_selection']['methods']\n",
        "        )\n",
        "\n",
        "        selected_features, feature_scores = self.feature_selector.ensemble_selection(X_train, y_train)\n",
        "\n",
        "        print(f\"Selected {len(selected_features)} features:\")\n",
        "        for i, feature in enumerate(selected_features[:10]):  # Show top 10\n",
        "            print(f\"  {i+1}. {feature}\")\n",
        "\n",
        "        return selected_features, feature_scores\n",
        "\n",
        "    def train_gradient_boosting(self, X_train, y_train):\n",
        "        \"\"\"Train enhanced gradient boosting model\"\"\"\n",
        "        print(\"Training Gradient Boosting model...\")\n",
        "\n",
        "        self.gb_model = EnhancedGradientBoosting()\n",
        "        gb_trained = self.gb_model.optimize_hyperparameters(\n",
        "            X_train, y_train,\n",
        "            cv_folds=self.config['models']['gb_optimization']['cv_folds'],\n",
        "            n_iter=self.config['models']['gb_optimization']['n_iter']\n",
        "        )\n",
        "\n",
        "        return gb_trained\n",
        "\n",
        "    def train_neural_network(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train enhanced neural network with xLSTM\"\"\"\n",
        "        print(\"Training Neural Network with xLSTM...\")\n",
        "\n",
        "        self.nn_model = EnhancedNeuralNetwork(\n",
        "            include_xlstm=self.config['models']['nn_config']['include_xlstm']\n",
        "        )\n",
        "\n",
        "        # Create model\n",
        "        if self.config['models']['nn_config']['include_xlstm']:\n",
        "            try:\n",
        "                # Prepare sequential features for xLSTM\n",
        "                sequential_features = ['Annual.pct.driven', 'Total.miles.driven', 'Avgdays.week']\n",
        "                seq_indices = [i for i, col in enumerate(X_train.columns) if any(seq_feat in col for seq_feat in sequential_features)]\n",
        "\n",
        "                if len(seq_indices) >= 3:\n",
        "                    X_train_seq = X_train.iloc[:, seq_indices[:3]].values.reshape(-1, 3, 1)\n",
        "                    X_val_seq = X_val.iloc[:, seq_indices[:3]].values.reshape(-1, 3, 1)\n",
        "\n",
        "                    model = self.nn_model.create_xlstm_enhanced_model(X_train.shape[1])\n",
        "\n",
        "                    trained_model, history = self.nn_model.compile_and_train(\n",
        "                        model, [X_train.values, X_train_seq], y_train,\n",
        "                        [X_val.values, X_val_seq], y_val,\n",
        "                        epochs=self.config['models']['nn_config']['epochs'],\n",
        "                        batch_size=self.config['models']['nn_config']['batch_size']\n",
        "                    )\n",
        "\n",
        "                    # Store sequential indices for prediction\n",
        "                    self.nn_model.seq_indices = seq_indices[:3]\n",
        "                else:\n",
        "                    print(\"Insufficient sequential features for xLSTM. Using standard MLP.\")\n",
        "                    model = self.nn_model.create_mlp_model(X_train.shape[1])\n",
        "                    trained_model, history = self.nn_model.compile_and_train(\n",
        "                        model, X_train.values, y_train, X_val.values, y_val,\n",
        "                        epochs=self.config['models']['nn_config']['epochs'],\n",
        "                        batch_size=self.config['models']['nn_config']['batch_size']\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                print(f\"xLSTM failed with error: {e}\")\n",
        "                print(\"Falling back to standard MLP...\")\n",
        "                model = self.nn_model.create_mlp_model(X_train.shape[1])\n",
        "                trained_model, history = self.nn_model.compile_and_train(\n",
        "                    model, X_train.values, y_train, X_val.values, y_val,\n",
        "                    epochs=self.config['models']['nn_config']['epochs'],\n",
        "                    batch_size=self.config['models']['nn_config']['batch_size']\n",
        "                )\n",
        "        else:\n",
        "            model = self.nn_model.create_mlp_model(X_train.shape[1])\n",
        "            trained_model, history = self.nn_model.compile_and_train(\n",
        "                model, X_train.values, y_train, X_val.values, y_val,\n",
        "                epochs=self.config['models']['nn_config']['epochs'],\n",
        "                batch_size=self.config['models']['nn_config']['batch_size']\n",
        "            )\n",
        "\n",
        "        return trained_model, history\n",
        "\n",
        "    def create_ensemble(self, X_val, y_val):\n",
        "        \"\"\"Create weighted ensemble of trained models\"\"\"\n",
        "        print(\"Creating weighted ensemble...\")\n",
        "\n",
        "        # Neural Network wrapper to handle different input formats\n",
        "        class NNWrapper:\n",
        "            def __init__(self, nn_model, has_seq=False, seq_indices=None):\n",
        "                self.model = nn_model\n",
        "                self.has_seq = has_seq\n",
        "                self.seq_indices = seq_indices\n",
        "\n",
        "            def predict_proba(self, X):\n",
        "                if self.has_seq and self.seq_indices is not None:\n",
        "                    X_seq = X.iloc[:, self.seq_indices].values.reshape(-1, len(self.seq_indices), 1)\n",
        "                    pred = self.model.predict([X.values, X_seq])\n",
        "                    return np.column_stack([1-pred.flatten(), pred.flatten()])\n",
        "                else:\n",
        "                    pred = self.model.predict(X.values)\n",
        "                    return np.column_stack([1-pred.flatten(), pred.flatten()])\n",
        "\n",
        "            def predict(self, X):\n",
        "                proba = self.predict_proba(X)\n",
        "                return (proba[:, 1] > 0.5).astype(int)\n",
        "\n",
        "        # Check if neural network has sequential features\n",
        "        has_seq = hasattr(self.nn_model, 'seq_indices') and self.nn_model.seq_indices is not None\n",
        "        seq_indices = getattr(self.nn_model, 'seq_indices', None)\n",
        "\n",
        "        models_dict = {\n",
        "            'Gradient Boosting': self.gb_model.model,\n",
        "            'Neural Network': NNWrapper(self.nn_model.model, has_seq, seq_indices)\n",
        "        }\n",
        "\n",
        "        self.ensemble = WeightedEnsemble(\n",
        "            models_dict,\n",
        "            weight_method=self.config['ensemble']['weight_method']\n",
        "        )\n",
        "\n",
        "        # Calculate ensemble weights\n",
        "        weights = self.ensemble.calculate_weights(X_val, y_val)\n",
        "\n",
        "        return self.ensemble\n",
        "\n",
        "    def optimize_threshold(self, X_val, y_val):\n",
        "        \"\"\"Optimize classification threshold\"\"\"\n",
        "        print(\"Optimizing classification threshold...\")\n",
        "\n",
        "        self.threshold_optimizer = ThresholdOptimizer(\n",
        "            metric=self.config['threshold_optimization']['metric']\n",
        "        )\n",
        "\n",
        "        # Get ensemble predictions\n",
        "        y_pred_proba = self.ensemble.predict_proba(X_val)\n",
        "\n",
        "        optimal_threshold = self.threshold_optimizer.optimize(y_val, y_pred_proba)\n",
        "\n",
        "        return optimal_threshold\n",
        "\n",
        "    def comprehensive_evaluation(self, X_test, y_test):\n",
        "        \"\"\"Perform comprehensive model evaluation\"\"\"\n",
        "        print(\"Performing comprehensive evaluation...\")\n",
        "\n",
        "        self.evaluator = ComprehensiveEvaluator()\n",
        "\n",
        "        # Evaluate individual models and ensemble\n",
        "        models_to_evaluate = {\n",
        "            'Gradient Boosting': self.gb_model.model,\n",
        "            'Neural Network': self.ensemble.models['Neural Network'],  # Use wrapper\n",
        "            'Ensemble': self.ensemble\n",
        "        }\n",
        "\n",
        "        y_pred_proba_dict = {}\n",
        "\n",
        "        for model_name, model in models_to_evaluate.items():\n",
        "            if hasattr(model, 'predict_proba'):\n",
        "                y_pred_proba = model.predict_proba(X_test)\n",
        "                if y_pred_proba.ndim > 1:\n",
        "                    y_pred_proba = y_pred_proba[:, 1] if y_pred_proba.shape[1] > 1 else y_pred_proba.flatten()\n",
        "            else:\n",
        "                y_pred_proba = model.predict(X_test)\n",
        "\n",
        "            y_pred_proba_dict[model_name] = y_pred_proba\n",
        "\n",
        "            # Get binary predictions using optimal threshold\n",
        "            if hasattr(self.threshold_optimizer, 'optimal_threshold'):\n",
        "                y_pred = (y_pred_proba >= self.threshold_optimizer.optimal_threshold).astype(int)\n",
        "            else:\n",
        "                y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "\n",
        "            # Calculate metrics\n",
        "            metrics = self.evaluator.calculate_all_metrics(y_test, y_pred, y_pred_proba, model_name)\n",
        "            self.results[model_name] = metrics\n",
        "\n",
        "        # Create comprehensive plots\n",
        "        self.evaluator.plot_comprehensive_evaluation(y_test, y_pred_proba_dict)\n",
        "\n",
        "        # Print results table\n",
        "        self.evaluator.print_results_table()\n",
        "\n",
        "        return self.results\n",
        "\n",
        "    def setup_interpretability(self, X_train, X_test):\n",
        "        \"\"\"Setup model interpretability suite\"\"\"\n",
        "        if not self.config['interpretability']['enable_shap'] and not self.config['interpretability']['enable_lime']:\n",
        "            print(\"Interpretability disabled in config\")\n",
        "            return\n",
        "\n",
        "        print(\"Setting up interpretability suite...\")\n",
        "\n",
        "        models_for_interpretation = {\n",
        "            'Gradient Boosting': self.gb_model.model,\n",
        "            'Ensemble': self.ensemble\n",
        "        }\n",
        "\n",
        "        self.interpretability_suite = InterpretabilitySuite(\n",
        "            models_for_interpretation,\n",
        "            X_train.columns.tolist()\n",
        "        )\n",
        "\n",
        "        if self.config['interpretability']['enable_shap']:\n",
        "            self.interpretability_suite.setup_shap_explainers(\n",
        "                X_train,\n",
        "                sample_size=self.config['interpretability']['background_sample_size']\n",
        "            )\n",
        "\n",
        "        # Generate explanations for test set sample\n",
        "        if len(X_test) > 10:\n",
        "            X_explain = X_test.iloc[:10]  # Explain first 10 test samples\n",
        "        else:\n",
        "            X_explain = X_test\n",
        "\n",
        "        for model_name in models_for_interpretation.keys():\n",
        "            self.interpretability_suite.explain_predictions(X_explain, model_name)\n",
        "\n",
        "    def save_models(self, save_path='models/'):\n",
        "        \"\"\"Save trained models and preprocessors\"\"\"\n",
        "        import os\n",
        "\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "\n",
        "        # Save preprocessor\n",
        "        joblib.dump(self.preprocessor, os.path.join(save_path, 'preprocessor.pkl'))\n",
        "\n",
        "        # Save feature selector\n",
        "        joblib.dump(self.feature_selector, os.path.join(save_path, 'feature_selector.pkl'))\n",
        "\n",
        "        # Save gradient boosting model\n",
        "        if self.gb_model and self.gb_model.model:\n",
        "            joblib.dump(self.gb_model.model, os.path.join(save_path, 'gradient_boosting_model.pkl'))\n",
        "\n",
        "        # Save neural network model\n",
        "        if self.nn_model and self.nn_model.model:\n",
        "            self.nn_model.model.save(os.path.join(save_path, 'neural_network_model.h5'))\n",
        "\n",
        "        # Save ensemble\n",
        "        joblib.dump(self.ensemble, os.path.join(save_path, 'ensemble.pkl'))\n",
        "\n",
        "        # Save threshold optimizer\n",
        "        joblib.dump(self.threshold_optimizer, os.path.join(save_path, 'threshold_optimizer.pkl'))\n",
        "\n",
        "        # Save results\n",
        "        joblib.dump(self.results, os.path.join(save_path, 'results.pkl'))\n",
        "\n",
        "        print(f\"Models saved to {save_path}\")\n",
        "\n",
        "    def load_models(self, load_path='models/'):\n",
        "        \"\"\"Load previously trained models\"\"\"\n",
        "        import os\n",
        "\n",
        "        # Load preprocessor\n",
        "        preprocessor_path = os.path.join(load_path, 'preprocessor.pkl')\n",
        "        if os.path.exists(preprocessor_path):\n",
        "            self.preprocessor = joblib.load(preprocessor_path)\n",
        "\n",
        "        # Load feature selector\n",
        "        feature_selector_path = os.path.join(load_path, 'feature_selector.pkl')\n",
        "        if os.path.exists(feature_selector_path):\n",
        "            self.feature_selector = joblib.load(feature_selector_path)\n",
        "\n",
        "        # Load gradient boosting model\n",
        "        gb_model_path = os.path.join(load_path, 'gradient_boosting_model.pkl')\n",
        "        if os.path.exists(gb_model_path):\n",
        "            self.gb_model = EnhancedGradientBoosting()\n",
        "            self.gb_model.model = joblib.load(gb_model_path)\n",
        "\n",
        "        # Load neural network model\n",
        "        nn_model_path = os.path.join(load_path, 'neural_network_model.h5')\n",
        "        if os.path.exists(nn_model_path):\n",
        "            self.nn_model = EnhancedNeuralNetwork()\n",
        "            self.nn_model.model = keras.models.load_model(nn_model_path, custom_objects={\n",
        "                'xLSTMCell': xLSTMCell,\n",
        "                'xLSTMLayer': xLSTMLayer\n",
        "            })\n",
        "\n",
        "        # Load ensemble\n",
        "        ensemble_path = os.path.join(load_path, 'ensemble.pkl')\n",
        "        if os.path.exists(ensemble_path):\n",
        "            self.ensemble = joblib.load(ensemble_path)\n",
        "\n",
        "        # Load threshold optimizer\n",
        "        threshold_path = os.path.join(load_path, 'threshold_optimizer.pkl')\n",
        "        if os.path.exists(threshold_path):\n",
        "            self.threshold_optimizer = joblib.load(threshold_path)\n",
        "\n",
        "        # Load results\n",
        "        results_path = os.path.join(load_path, 'results.pkl')\n",
        "        if os.path.exists(results_path):\n",
        "            self.results = joblib.load(results_path)\n",
        "\n",
        "        print(f\"Models loaded from {load_path}\")\n",
        "\n",
        "    def run_complete_pipeline(self, file_path, save_models=True):\n",
        "        \"\"\"\n",
        "        Run the complete dual-path machine learning pipeline\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the telematics dataset\n",
        "            save_models (bool): Whether to save trained models\n",
        "\n",
        "        Returns:\n",
        "            dict: Complete results from the pipeline\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        print(\"=\"*80)\n",
        "        print(\"STARTING DUAL-PATH MACHINE LEARNING FRAMEWORK\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load and preprocess data\n",
        "            X, y = self.load_and_preprocess_data(file_path)\n",
        "\n",
        "            # Step 2: Split data\n",
        "            X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)\n",
        "\n",
        "            # Step 3: Handle class imbalance\n",
        "            X_train_balanced, y_train_balanced = self.handle_class_imbalance(X_train, y_train)\n",
        "\n",
        "            # Step 4: Feature selection\n",
        "            selected_features, feature_scores = self.select_features(X_train_balanced, y_train_balanced)\n",
        "\n",
        "            # Apply feature selection to all datasets\n",
        "            X_train_selected = X_train_balanced[selected_features]\n",
        "            X_val_selected = X_val[selected_features]\n",
        "            X_test_selected = X_test[selected_features]\n",
        "\n",
        "            # Step 5: Train Gradient Boosting model\n",
        "            gb_model = self.train_gradient_boosting(X_train_selected, y_train_balanced)\n",
        "\n",
        "            # Step 6: Train Neural Network model\n",
        "            nn_model, nn_history = self.train_neural_network(\n",
        "                X_train_selected, y_train_balanced, X_val_selected, y_val\n",
        "            )\n",
        "\n",
        "            # Step 7: Create ensemble\n",
        "            ensemble = self.create_ensemble(X_val_selected, y_val)\n",
        "\n",
        "            # Step 8: Optimize threshold\n",
        "            optimal_threshold = self.optimize_threshold(X_val_selected, y_val)\n",
        "\n",
        "            # Step 9: Comprehensive evaluation\n",
        "            evaluation_results = self.comprehensive_evaluation(X_test_selected, y_test)\n",
        "\n",
        "            # Step 10: Setup interpretability\n",
        "            self.setup_interpretability(X_train_selected, X_test_selected)\n",
        "\n",
        "            # Step 11: Save models if requested\n",
        "            if save_models:\n",
        "                self.save_models()\n",
        "\n",
        "            end_time = time.time()\n",
        "            total_time = end_time - start_time\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"DUAL-PATH FRAMEWORK COMPLETED SUCCESSFULLY\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "            print(f\"Optimal threshold: {optimal_threshold:.4f}\")\n",
        "            print(\"\\nBest performing model based on test set:\")\n",
        "\n",
        "            # Find best model based on F1 score\n",
        "            best_model = max(evaluation_results.keys(), key=lambda x: evaluation_results[x]['f1'])\n",
        "            best_f1 = evaluation_results[best_model]['f1']\n",
        "            best_auc = evaluation_results[best_model]['auc_roc']\n",
        "\n",
        "            print(f\"  {best_model}: F1={best_f1:.4f}, AUC={best_auc:.4f}\")\n",
        "\n",
        "            # Summary of results\n",
        "            pipeline_summary = {\n",
        "                'execution_time': total_time,\n",
        "                'optimal_threshold': optimal_threshold,\n",
        "                'best_model': best_model,\n",
        "                'best_f1_score': best_f1,\n",
        "                'best_auc_score': best_auc,\n",
        "                'selected_features': selected_features,\n",
        "                'feature_scores': feature_scores,\n",
        "                'model_results': evaluation_results,\n",
        "                'data_stats': {\n",
        "                    'original_shape': X.shape,\n",
        "                    'train_size': len(X_train_selected),\n",
        "                    'val_size': len(X_val_selected),\n",
        "                    'test_size': len(X_test_selected),\n",
        "                    'class_distribution': y.value_counts().to_dict()\n",
        "                }\n",
        "            }\n",
        "\n",
        "            return pipeline_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Pipeline failed with error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def predict_new_data(self, new_data_path):\n",
        "        \"\"\"\n",
        "        Make predictions on new data using trained models\n",
        "\n",
        "        Args:\n",
        "            new_data_path (str): Path to new data file\n",
        "\n",
        "        Returns:\n",
        "            dict: Predictions from all models\n",
        "        \"\"\"\n",
        "        if self.ensemble is None:\n",
        "            raise ValueError(\"Models not trained yet. Run the pipeline first.\")\n",
        "\n",
        "        print(\"Making predictions on new data...\")\n",
        "\n",
        "        # Load and preprocess new data\n",
        "        new_df = pd.read_csv(new_data_path)\n",
        "        new_X_processed = self.preprocessor.transform(new_df)\n",
        "\n",
        "        # Apply feature selection\n",
        "        if self.feature_selector and self.feature_selector.selected_features:\n",
        "            new_X_selected = new_X_processed[self.feature_selector.selected_features]\n",
        "        else:\n",
        "            new_X_selected = new_X_processed\n",
        "\n",
        "        # Get predictions from all models\n",
        "        predictions = {}\n",
        "\n",
        "        # Gradient Boosting predictions\n",
        "        gb_proba = self.gb_model.model.predict_proba(new_X_selected)[:, 1]\n",
        "        gb_pred = (gb_proba >= self.threshold_optimizer.optimal_threshold).astype(int)\n",
        "        predictions['gradient_boosting'] = {\n",
        "            'probabilities': gb_proba,\n",
        "            'predictions': gb_pred\n",
        "        }\n",
        "\n",
        "        # Neural Network predictions\n",
        "        nn_wrapper = self.ensemble.models['Neural Network']\n",
        "        nn_proba = nn_wrapper.predict_proba(new_X_selected)[:, 1]\n",
        "        nn_pred = (nn_proba >= self.threshold_optimizer.optimal_threshold).astype(int)\n",
        "        predictions['neural_network'] = {\n",
        "            'probabilities': nn_proba,\n",
        "            'predictions': nn_pred\n",
        "        }\n",
        "\n",
        "        # Ensemble predictions\n",
        "        ensemble_proba = self.ensemble.predict_proba(new_X_selected)\n",
        "        ensemble_pred = (ensemble_proba >= self.threshold_optimizer.optimal_threshold).astype(int)\n",
        "        predictions['ensemble'] = {\n",
        "            'probabilities': ensemble_proba,\n",
        "            'predictions': ensemble_pred\n",
        "        }\n",
        "\n",
        "        print(f\"Generated predictions for {len(new_X_selected)} samples\")\n",
        "\n",
        "        return predictions\n",
        "\n",
        "# Example usage and pipeline execution function\n",
        "def run_driver_risk_assessment_pipeline(data_path, config=None):\n",
        "    \"\"\"\n",
        "    Convenience function to run the complete driver risk assessment pipeline\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the telematics dataset\n",
        "        config (dict): Optional configuration dictionary\n",
        "\n",
        "    Returns:\n",
        "        tuple: (framework_instance, results_summary)\n",
        "    \"\"\"\n",
        "    # Initialize framework\n",
        "    framework = DualPathFramework(config)\n",
        "\n",
        "    # Run complete pipeline\n",
        "    results = framework.run_complete_pipeline(data_path)\n",
        "\n",
        "    return framework, results\n",
        "\n",
        "# Example usage with your dataset\n",
        "if __name__ == \"__main__\":\n",
        "    # Your dataset path\n",
        "    DATA_PATH = '/content/drive/MyDrive/Insurance/telematics_syn.csv'\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    framework, results = run_driver_risk_assessment_pipeline(DATA_PATH)\n",
        "\n",
        "    if results is not None:\n",
        "        print(\"Pipeline completed successfully!\")\n",
        "        print(f\"Best model: {results['best_model']}\")\n",
        "        print(f\"Best F1 Score: {results['best_f1_score']:.4f}\")\n",
        "        print(f\"Best AUC Score: {results['best_auc_score']:.4f}\")\n",
        "    else:\n",
        "        print(\"Pipeline failed. Check the error messages above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7Dm9tkhhl45",
        "outputId": "40e7eef3-7dd5-478d-b8e2-0064adfa1cec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STARTING DUAL-PATH MACHINE LEARNING FRAMEWORK\n",
            "================================================================================\n",
            "Loading and preprocessing data...\n",
            "Loaded dataset with shape: (100000, 52)\n",
            "Class distribution: {0: 97302, 1: 2698}\n",
            "Class imbalance ratio: 36.06\n",
            "Processed dataset shape: (9311, 87)\n",
            "Added 37 new features\n",
            "Data split - Train: 6517, Val: 1397, Test: 1397\n",
            "Handling class imbalance...\n",
            "Original class distribution: [6458   59]\n",
            "Resampled class distribution: [6458 6458]\n",
            "Performing feature selection...\n",
            "Selected 20 features:\n",
            "  1. Territory\n",
            "  2. Years.noclaims\n",
            "  3. Annual.pct.driven\n",
            "  4. Left.turn.intensity08\n",
            "  5. Right.turn.intensity09\n",
            "  6. Duration\n",
            "  7. Total.miles.driven\n",
            "  8. Pct.drive.sat\n",
            "  9. Brake.09miles\n",
            "  10. Brake.11miles\n",
            "Training Gradient Boosting model...\n",
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "Best Gradient Boosting Parameters:\n",
            "  subsample: 0.8\n",
            "  n_estimators: 300\n",
            "  min_samples_split: 10\n",
            "  min_samples_leaf: 1\n",
            "  max_features: log2\n",
            "  max_depth: 7\n",
            "  learning_rate: 0.2\n",
            "Best CV Score: 1.0000\n",
            "Training Neural Network with xLSTM...\n",
            "xLSTM failed with error: Exception encountered when calling xLSTMLayer.call().\n",
            "\n",
            "\u001b[1mCould not automatically infer the output shape / dtype of 'x_lstm_layer_1' (of type xLSTMLayer). Either the `xLSTMLayer.call()` method is incorrect, or you need to implement the `xLSTMLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n",
            "\n",
            "'SymbolicTensor' object cannot be interpreted as an integer\u001b[0m\n",
            "\n",
            "Arguments received by xLSTMLayer.call():\n",
            "  • args=('<KerasTensor shape=(None, 3, 1), dtype=float32, sparse=False, ragged=False, name=seq_input>',)\n",
            "  • kwargs=<class 'inspect._empty'>\n",
            "Falling back to standard MLP...\n",
            "Pipeline failed with error: Invalid dtype: object\n",
            "Pipeline failed. Check the error messages above.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-4133842393.py\", line 465, in run_complete_pipeline\n",
            "    nn_model, nn_history = self.train_neural_network(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-4133842393.py\", line 200, in train_neural_network\n",
            "    trained_model, history = self.nn_model.compile_and_train(\n",
            "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-1937268350.py\", line 638, in compile_and_train\n",
            "    history = model.fit(\n",
            "              ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optree/ops.py\", line 766, in tree_map\n",
            "    return treespec.unflatten(map(func, *flat_args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ValueError: Invalid dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Dual-Path Machine Learning Framework for Driver Risk Assessment\n",
        "# Fixed Implementation with Error Handling and Improvements\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, RandomizedSearchCV, cross_val_score,\n",
        "    StratifiedKFold, learning_curve\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_selection import (\n",
        "    mutual_info_classif, SelectKBest, RFE, VarianceThreshold\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, matthews_corrcoef, confusion_matrix,\n",
        "    roc_curve, precision_recall_curve, average_precision_score,\n",
        "    classification_report, log_loss\n",
        ")\n",
        "from sklearn.inspection import permutation_importance, partial_dependence\n",
        "\n",
        "# Class Imbalance Handling\n",
        "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Dense, Dropout, BatchNormalization, LSTM,\n",
        "    GRU, Attention, MultiHeadAttention, LayerNormalization\n",
        ")\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('seaborn-v0_8')\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED: Enhanced LSTM Implementation (Simplified)\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedLSTMModel:\n",
        "    \"\"\"\n",
        "    Simplified LSTM model for sequential feature processing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "\n",
        "    def create_lstm_enhanced_model(self, input_dim, sequence_length=5):\n",
        "        \"\"\"Create LSTM-enhanced model\"\"\"\n",
        "        # Main features input\n",
        "        main_input = Input(shape=(input_dim,), name='main_input')\n",
        "\n",
        "        # Sequential features input (reshape for LSTM)\n",
        "        seq_input = Input(shape=(sequence_length, 1), name='seq_input')\n",
        "\n",
        "        # LSTM processing\n",
        "        lstm_out = LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)(seq_input)\n",
        "        lstm_dense = Dense(32, activation='relu')(lstm_out)\n",
        "        lstm_dense = Dropout(0.3)(lstm_dense)\n",
        "\n",
        "        # Combine features\n",
        "        combined = keras.layers.concatenate([main_input, lstm_dense])\n",
        "\n",
        "        # Dense layers\n",
        "        x = Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(combined)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.4)(x)\n",
        "\n",
        "        x = Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(0.3)(x)\n",
        "\n",
        "        x = Dense(32, activation='relu')(x)\n",
        "        x = Dropout(0.2)(x)\n",
        "\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=[main_input, seq_input], outputs=outputs)\n",
        "        return model\n",
        "\n",
        "# =============================================================================\n",
        "# FIXED: Comprehensive Data Preprocessor\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedDataPreprocessor:\n",
        "    \"\"\"\n",
        "    Fixed data preprocessing with proper data type handling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scaling_method='standard', handle_outliers=True):\n",
        "        self.scaling_method = scaling_method\n",
        "        self.handle_outliers = handle_outliers\n",
        "        self.scalers = {}\n",
        "        self.feature_names = None\n",
        "        self.categorical_features = []\n",
        "        self.numerical_features = []\n",
        "\n",
        "    def detect_feature_types(self, df):\n",
        "        \"\"\"Automatically detect categorical and numerical features\"\"\"\n",
        "        categorical = []\n",
        "        numerical = []\n",
        "\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
        "                # Check if it's actually numeric stored as object\n",
        "                try:\n",
        "                    pd.to_numeric(df[col].dropna())\n",
        "                    numerical.append(col)\n",
        "                except:\n",
        "                    categorical.append(col)\n",
        "            else:\n",
        "                numerical.append(col)\n",
        "\n",
        "        return categorical, numerical\n",
        "\n",
        "    def handle_missing_values(self, df):\n",
        "        \"\"\"Advanced missing value handling\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Handle negative Car.age values as missing\n",
        "        if 'Car.age' in df.columns:\n",
        "            df.loc[df['Car.age'] < 0, 'Car.age'] = np.nan\n",
        "            df['Car.age'].fillna(df['Car.age'].median(), inplace=True)\n",
        "\n",
        "        # Fill other missing values\n",
        "        for col in df.columns:\n",
        "            if df[col].isnull().sum() > 0:\n",
        "                if df[col].dtype == 'object' or df[col].dtype.name == 'category':\n",
        "                    df[col].fillna(df[col].mode().iloc[0] if len(df[col].mode()) > 0 else 'Unknown', inplace=True)\n",
        "                else:\n",
        "                    df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def clean_data_types(self, df):\n",
        "        \"\"\"Ensure proper data types\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        for col in df.columns:\n",
        "            if df[col].dtype == 'object':\n",
        "                # Try to convert to numeric\n",
        "                try:\n",
        "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "                    # Fill any new NaNs created from conversion\n",
        "                    if df[col].isnull().sum() > 0:\n",
        "                        df[col].fillna(df[col].median(), inplace=True)\n",
        "                except:\n",
        "                    # Keep as categorical\n",
        "                    pass\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_outliers(self, df, method='iqr', threshold=3):\n",
        "        \"\"\"Remove outliers using IQR or Z-score method\"\"\"\n",
        "        if not self.handle_outliers:\n",
        "            return df\n",
        "\n",
        "        df = df.copy()\n",
        "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "        if method == 'iqr':\n",
        "            for col in numerical_cols:\n",
        "                Q1 = df[col].quantile(0.25)\n",
        "                Q3 = df[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - 1.5 * IQR\n",
        "                upper_bound = Q3 + 1.5 * IQR\n",
        "                outlier_mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
        "                # Cap outliers instead of removing them\n",
        "                df.loc[df[col] < lower_bound, col] = lower_bound\n",
        "                df.loc[df[col] > upper_bound, col] = upper_bound\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_interaction_features(self, df):\n",
        "        \"\"\"Create interaction features for behavioral metrics\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Acceleration-Brake intensity combinations\n",
        "        accel_cols = [col for col in df.columns if 'Accel' in col and df[col].dtype in ['float64', 'int64']]\n",
        "        brake_cols = [col for col in df.columns if 'Brake' in col and df[col].dtype in ['float64', 'int64']]\n",
        "\n",
        "        for accel_col in accel_cols[:3]:  # Limit to avoid feature explosion\n",
        "            for brake_col in brake_cols[:3]:\n",
        "                if accel_col.split('.')[-1] == brake_col.split('.')[-1]:  # Same mile threshold\n",
        "                    new_col = f'{accel_col}_x_{brake_col}'\n",
        "                    df[new_col] = df[accel_col] * df[brake_col]\n",
        "\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        \"\"\"Fit preprocessor and transform data\"\"\"\n",
        "        df = X.copy()\n",
        "\n",
        "        # Store original feature names\n",
        "        self.feature_names = df.columns.tolist()\n",
        "\n",
        "        # Clean data types first\n",
        "        df = self.clean_data_types(df)\n",
        "\n",
        "        # Handle missing values\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Detect feature types after cleaning\n",
        "        self.categorical_features, self.numerical_features = self.detect_feature_types(df)\n",
        "\n",
        "        # Remove outliers\n",
        "        if y is not None:\n",
        "            original_len = len(df)\n",
        "            df = self.remove_outliers(df)\n",
        "            if len(df) < original_len:\n",
        "                # Filter y to match filtered df if indices changed\n",
        "                try:\n",
        "                    y = y.loc[df.index] if hasattr(y, 'loc') else y[df.index.tolist()]\n",
        "                except:\n",
        "                    print(\"Warning: Could not filter y after outlier removal\")\n",
        "\n",
        "        # Create interaction features (limited)\n",
        "        try:\n",
        "            df = self.create_interaction_features(df)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Could not create interaction features: {e}\")\n",
        "\n",
        "        # One-hot encode categorical features\n",
        "        if self.categorical_features:\n",
        "            try:\n",
        "                df = pd.get_dummies(df, columns=self.categorical_features, drop_first=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not encode categorical features: {e}\")\n",
        "\n",
        "        # Update numerical features after transformations\n",
        "        self.numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "        # Apply scaling\n",
        "        if self.scaling_method == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif self.scaling_method == 'robust':\n",
        "            scaler = RobustScaler()\n",
        "        elif self.scaling_method == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        else:\n",
        "            scaler = StandardScaler()\n",
        "\n",
        "        # Ensure all columns are numeric before scaling\n",
        "        for col in self.numerical_features:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "                df[col].fillna(df[col].median(), inplace=True)\n",
        "\n",
        "        df[self.numerical_features] = scaler.fit_transform(df[self.numerical_features])\n",
        "        self.scalers['numerical'] = scaler\n",
        "\n",
        "        return df, y\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"Transform new data using fitted preprocessor\"\"\"\n",
        "        df = X.copy()\n",
        "\n",
        "        # Clean data types\n",
        "        df = self.clean_data_types(df)\n",
        "\n",
        "        # Handle missing values\n",
        "        df = self.handle_missing_values(df)\n",
        "\n",
        "        # Create interaction features\n",
        "        try:\n",
        "            df = self.create_interaction_features(df)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # One-hot encode categorical features\n",
        "        if self.categorical_features:\n",
        "            try:\n",
        "                df = pd.get_dummies(df, columns=self.categorical_features, drop_first=True)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Ensure all required columns are present\n",
        "        for col in self.numerical_features:\n",
        "            if col not in df.columns:\n",
        "                df[col] = 0\n",
        "\n",
        "        # Select only the columns that were in training\n",
        "        df = df.reindex(columns=self.numerical_features, fill_value=0)\n",
        "\n",
        "        # Apply scaling\n",
        "        if 'numerical' in self.scalers:\n",
        "            df[self.numerical_features] = self.scalers['numerical'].transform(df[self.numerical_features])\n",
        "\n",
        "        return df\n",
        "\n",
        "# =============================================================================\n",
        "# Enhanced Feature Selector (Simplified)\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedFeatureSelector:\n",
        "    \"\"\"\n",
        "    Multi-method feature selection with ensemble approach\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features=20, methods=['mutual_info', 'rfe']):\n",
        "        self.n_features = n_features\n",
        "        self.methods = methods\n",
        "        self.selected_features = None\n",
        "        self.feature_scores = {}\n",
        "\n",
        "    def mutual_info_selection(self, X, y):\n",
        "        \"\"\"Feature selection using mutual information\"\"\"\n",
        "        try:\n",
        "            mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "            feature_scores = dict(zip(X.columns, mi_scores))\n",
        "            top_features = sorted(feature_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "            return [feat[0] for feat in top_features[:self.n_features]], feature_scores\n",
        "        except Exception as e:\n",
        "            print(f\"Mutual info selection failed: {e}\")\n",
        "            return X.columns.tolist()[:self.n_features], {}\n",
        "\n",
        "    def rfe_selection(self, X, y):\n",
        "        \"\"\"Recursive Feature Elimination\"\"\"\n",
        "        try:\n",
        "            estimator = GradientBoostingClassifier(n_estimators=50, random_state=42)\n",
        "            selector = RFE(estimator, n_features_to_select=min(self.n_features, X.shape[1]))\n",
        "            selector.fit(X, y)\n",
        "\n",
        "            feature_scores = dict(zip(X.columns, selector.ranking_))\n",
        "            selected_features = X.columns[selector.support_].tolist()\n",
        "            return selected_features, feature_scores\n",
        "        except Exception as e:\n",
        "            print(f\"RFE selection failed: {e}\")\n",
        "            return X.columns.tolist()[:self.n_features], {}\n",
        "\n",
        "    def ensemble_selection(self, X, y):\n",
        "        \"\"\"Ensemble feature selection combining multiple methods\"\"\"\n",
        "        all_selected_features = []\n",
        "        method_scores = {}\n",
        "\n",
        "        if 'mutual_info' in self.methods:\n",
        "            features, scores = self.mutual_info_selection(X, y)\n",
        "            all_selected_features.extend(features)\n",
        "            method_scores['mutual_info'] = scores\n",
        "\n",
        "        if 'rfe' in self.methods:\n",
        "            features, scores = self.rfe_selection(X, y)\n",
        "            all_selected_features.extend(features)\n",
        "            method_scores['rfe'] = scores\n",
        "\n",
        "        if not all_selected_features:\n",
        "            # Fallback: use all features up to n_features\n",
        "            self.selected_features = X.columns.tolist()[:self.n_features]\n",
        "        else:\n",
        "            # Count feature frequency across methods\n",
        "            feature_counts = defaultdict(int)\n",
        "            for feature in all_selected_features:\n",
        "                feature_counts[feature] += 1\n",
        "\n",
        "            # Select features that appear in multiple methods\n",
        "            ensemble_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "            self.selected_features = [feat[0] for feat in ensemble_features[:self.n_features]]\n",
        "\n",
        "        self.feature_scores = method_scores\n",
        "        return self.selected_features, method_scores\n",
        "\n",
        "# =============================================================================\n",
        "# Enhanced Neural Network (Fixed)\n",
        "# =============================================================================\n",
        "\n",
        "class EnhancedNeuralNetwork:\n",
        "    \"\"\"Enhanced Neural Network with LSTM integration\"\"\"\n",
        "\n",
        "    def __init__(self, include_lstm=True):\n",
        "        self.model = None\n",
        "        self.history = None\n",
        "        self.include_lstm = include_lstm\n",
        "        self.lstm_model = EnhancedLSTMModel()\n",
        "\n",
        "    def create_mlp_model(self, input_dim, hidden_layers=[128, 64, 32]):\n",
        "        \"\"\"Create Multi-Layer Perceptron model\"\"\"\n",
        "        inputs = Input(shape=(input_dim,))\n",
        "        x = inputs\n",
        "\n",
        "        for i, units in enumerate(hidden_layers):\n",
        "            x = Dense(units, activation='relu',\n",
        "                     kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Dropout(0.3)(x)\n",
        "\n",
        "        outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def prepare_lstm_data(self, X, feature_names):\n",
        "        \"\"\"Prepare data for LSTM processing\"\"\"\n",
        "        # Select features that could be sequential\n",
        "        sequential_candidates = []\n",
        "        for col in feature_names:\n",
        "            if any(keyword in col.lower() for keyword in ['speed', 'time', 'drive', 'mile', 'pct']):\n",
        "                sequential_candidates.append(col)\n",
        "\n",
        "        # Take first 5 features as sequence\n",
        "        seq_features = sequential_candidates[:5] if len(sequential_candidates) >= 5 else feature_names[:5]\n",
        "\n",
        "        # Create sequence data\n",
        "        seq_data = X[seq_features].values.reshape(-1, len(seq_features), 1)\n",
        "\n",
        "        return seq_data, seq_features\n",
        "\n",
        "    def compile_and_train(self, model, X_train, y_train, X_val, y_val,\n",
        "                         epochs=100, batch_size=32):\n",
        "        \"\"\"Compile and train the neural network\"\"\"\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        callbacks = [\n",
        "            EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, min_lr=1e-7)\n",
        "        ]\n",
        "\n",
        "        # Convert to numpy arrays to avoid dtype issues\n",
        "        if isinstance(X_train, list):\n",
        "            X_train = [arr.astype(np.float32) for arr in X_train]\n",
        "            X_val = [arr.astype(np.float32) for arr in X_val]\n",
        "        else:\n",
        "            X_train = X_train.astype(np.float32)\n",
        "            X_val = X_val.astype(np.float32)\n",
        "\n",
        "        y_train = y_train.astype(np.float32)\n",
        "        y_val = y_val.astype(np.float32)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        self.model = model\n",
        "        self.history = history\n",
        "        return model, history\n",
        "\n",
        "# =============================================================================\n",
        "# Main Framework Class (Simplified and Fixed)\n",
        "# =============================================================================\n",
        "\n",
        "class DualPathFramework:\n",
        "    \"\"\"\n",
        "    Main class implementing the dual-path machine learning framework\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or self._get_default_config()\n",
        "        self.preprocessor = None\n",
        "        self.imbalance_handler = None\n",
        "        self.feature_selector = None\n",
        "        self.gb_model = None\n",
        "        self.nn_model = None\n",
        "        self.results = {}\n",
        "\n",
        "    def _get_default_config(self):\n",
        "        \"\"\"Default configuration for the framework\"\"\"\n",
        "        return {\n",
        "            'preprocessing': {\n",
        "                'scaling_method': 'standard',\n",
        "                'handle_outliers': True\n",
        "            },\n",
        "            'imbalance_handling': {\n",
        "                'method': 'smote_tomek',\n",
        "                'random_state': 42\n",
        "            },\n",
        "            'feature_selection': {\n",
        "                'n_features': 20,\n",
        "                'methods': ['mutual_info', 'rfe']\n",
        "            },\n",
        "            'models': {\n",
        "                'gb_optimization': {\n",
        "                    'cv_folds': 3,\n",
        "                    'n_iter': 20\n",
        "                },\n",
        "                'nn_config': {\n",
        "                    'include_lstm': False,  # Simplified\n",
        "                    'epochs': 50,\n",
        "                    'batch_size': 64\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def load_and_preprocess_data(self, file_path):\n",
        "        \"\"\"Load and preprocess the telematics dataset\"\"\"\n",
        "        print(\"Loading and preprocessing data...\")\n",
        "\n",
        "        # Load data\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Loaded dataset with shape: {df.shape}\")\n",
        "\n",
        "        # Create target variable\n",
        "        if 'NB_Claim' in df.columns and 'AMT_Claim' in df.columns:\n",
        "            df['ClaimYN'] = ((df['NB_Claim'] >= 1) & (df['AMT_Claim'] > 1000)).astype(int)\n",
        "            df = df.drop(['NB_Claim', 'AMT_Claim'], axis=1)\n",
        "        elif 'ClaimYN' not in df.columns:\n",
        "            print(\"Warning: No target variable found. Creating dummy target.\")\n",
        "            df['ClaimYN'] = np.random.choice([0, 1], size=len(df), p=[0.95, 0.05])\n",
        "\n",
        "        # Separate features and target\n",
        "        X = df.drop('ClaimYN', axis=1, errors='ignore')\n",
        "        y = df['ClaimYN'] if 'ClaimYN' in df.columns else np.random.choice([0, 1], size=len(df), p=[0.95, 0.05])\n",
        "\n",
        "        print(f\"Features shape: {X.shape}\")\n",
        "        print(f\"Class distribution: {pd.Series(y).value_counts().to_dict()}\")\n",
        "\n",
        "        # Initialize and apply preprocessing\n",
        "        self.preprocessor = EnhancedDataPreprocessor(\n",
        "            scaling_method=self.config['preprocessing']['scaling_method'],\n",
        "            handle_outliers=self.config['preprocessing']['handle_outliers']\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            X_processed, y_processed = self.preprocessor.fit_transform(X, y)\n",
        "            print(f\"Processed dataset shape: {X_processed.shape}\")\n",
        "            return X_processed, y_processed\n",
        "        except Exception as e:\n",
        "            print(f\"Preprocessing failed: {e}\")\n",
        "            # Fallback: minimal preprocessing\n",
        "            X_numeric = X.select_dtypes(include=[np.number])\n",
        "            X_numeric = X_numeric.fillna(X_numeric.median())\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = pd.DataFrame(scaler.fit_transform(X_numeric),\n",
        "                                  columns=X_numeric.columns,\n",
        "                                  index=X_numeric.index)\n",
        "            return X_scaled, y\n",
        "\n",
        "    def split_data(self, X, y, test_size=0.2, val_size=0.2):\n",
        "        \"\"\"Split data into train, validation, and test sets\"\"\"\n",
        "        # First split: separate test set\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Second split: separate train and validation\n",
        "        val_size_adjusted = val_size / (1 - test_size)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        print(f\"Data split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "    def handle_class_imbalance(self, X_train, y_train):\n",
        "        \"\"\"Apply SMOTE for class imbalance\"\"\"\n",
        "        print(\"Handling class imbalance...\")\n",
        "\n",
        "        try:\n",
        "            smote = SMOTE(random_state=42)\n",
        "            X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "            print(f\"Original class distribution: {np.bincount(y_train)}\")\n",
        "            print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
        "\n",
        "            return X_resampled, y_resampled\n",
        "        except Exception as e:\n",
        "            print(f\"SMOTE failed: {e}\")\n",
        "            return X_train, y_train\n",
        "\n",
        "    def select_features(self, X_train, y_train):\n",
        "        \"\"\"Perform feature selection\"\"\"\n",
        "        print(\"Performing feature selection...\")\n",
        "\n",
        "        self.feature_selector = EnhancedFeatureSelector(\n",
        "            n_features=min(self.config['feature_selection']['n_features'], X_train.shape[1]),\n",
        "            methods=self.config['feature_selection']['methods']\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            selected_features, feature_scores = self.feature_selector.ensemble_selection(X_train, y_train)\n",
        "            print(f\"Selected {len(selected_features)} features\")\n",
        "            return selected_features, feature_scores\n",
        "        except Exception as e:\n",
        "            print(f\"Feature selection failed: {e}\")\n",
        "            # Fallback: use all features up to limit\n",
        "            n_features = min(20, X_train.shape[1])\n",
        "            selected_features = X_train.columns.tolist()[:n_features]\n",
        "            return selected_features, {}\n",
        "\n",
        "    def train_gradient_boosting(self, X_train, y_train):\n",
        "        \"\"\"Train gradient boosting model\"\"\"\n",
        "        print(\"Training Gradient Boosting model...\")\n",
        "\n",
        "        # Simple parameter grid for faster training\n",
        "        param_grid = {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.1, 0.2],\n",
        "            'max_depth': [3, 5],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'subsample': [0.8, 1.0]\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            gb_classifier = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "            search = RandomizedSearchCV(\n",
        "                estimator=gb_classifier,\n",
        "                param_distributions=param_grid,\n",
        "                n_iter=self.config['models']['gb_optimization']['n_iter'],\n",
        "                cv=self.config['models']['gb_optimization']['cv_folds'],\n",
        "                scoring='roc_auc',\n",
        "                n_jobs=-1,\n",
        "                random_state=42,\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            search.fit(X_train, y_train)\n",
        "            self.gb_model = search.best_estimator_\n",
        "\n",
        "            print(\"Best GB Parameters:\")\n",
        "            for param, value in search.best_params_.items():\n",
        "                print(f\"  {param}: {value}\")\n",
        "            print(f\"Best CV Score: {search.best_score_:.4f}\")\n",
        "\n",
        "            return self.gb_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"GB training failed: {e}\")\n",
        "            # Fallback: simple GB\n",
        "            gb = GradientBoostingClassifier(random_state=42)\n",
        "            gb.fit(X_train, y_train)\n",
        "            self.gb_model = gb\n",
        "            return gb\n",
        "\n",
        "    def train_neural_network(self, X_train, y_train, X_val, y_val):\n",
        "        \"\"\"Train neural network\"\"\"\n",
        "        print(\"Training Neural Network...\")\n",
        "\n",
        "        self.nn_model = EnhancedNeuralNetwork(\n",
        "            include_lstm=self.config['models']['nn_config']['include_lstm']\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            if self.config['models']['nn_config']['include_lstm']:\n",
        "                # LSTM model (simplified)\n",
        "                seq_data_train, seq_features = self.nn_model.prepare_lstm_data(X_train, X_train.columns)\n",
        "                seq_data_val, _ = self.nn_model.prepare_lstm_data(X_val, X_val.columns)\n",
        "\n",
        "                model = self.nn_model.lstm_model.create_lstm_enhanced_model(X_train.shape[1], len(seq_features))\n",
        "\n",
        "                trained_model, history = self.nn_model.compile_and_train(\n",
        "                    model, [X_train.values.astype(np.float32), seq_data_train], y_train,\n",
        "                    [X_val.values.astype(np.float32), seq_data_val], y_val,\n",
        "                    epochs=self.config['models']['nn_config']['epochs'],\n",
        "                    batch_size=self.config['models']['nn_config']['batch_size']\n",
        "                )\n",
        "\n",
        "                self.nn_model.seq_features = seq_features\n",
        "\n",
        "            else:\n",
        "                # Standard MLP\n",
        "                model = self.nn_model.create_mlp_model(X_train.shape[1])\n",
        "\n",
        "                trained_model, history = self.nn_model.compile_and_train(\n",
        "                    model, X_train.values, y_train, X_val.values, y_val,\n",
        "                    epochs=self.config['models']['nn_config']['epochs'],\n",
        "                    batch_size=self.config['models']['nn_config']['batch_size']\n",
        "                )\n",
        "\n",
        "            return trained_model, history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"NN training failed: {e}\")\n",
        "            # Fallback: simple sklearn MLP\n",
        "            from sklearn.neural_network import MLPClassifier\n",
        "            mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=200, random_state=42)\n",
        "            mlp.fit(X_train, y_train)\n",
        "\n",
        "            # Create a wrapper to match interface\n",
        "            class MLPWrapper:\n",
        "                def __init__(self, model):\n",
        "                    self.model = model\n",
        "\n",
        "                def predict(self, X):\n",
        "                    if hasattr(X, 'values'):\n",
        "                        X = X.values\n",
        "                    return self.model.predict(X)\n",
        "\n",
        "                def predict_proba(self, X):\n",
        "                    if hasattr(X, 'values'):\n",
        "                        X = X.values\n",
        "                    return self.model.predict_proba(X)\n",
        "\n",
        "            self.nn_model.model = MLPWrapper(mlp)\n",
        "            return mlp, None\n",
        "\n",
        "    def evaluate_models(self, X_test, y_test):\n",
        "        \"\"\"Evaluate both models\"\"\"\n",
        "        print(\"Evaluating models...\")\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        # Evaluate Gradient Boosting\n",
        "        if self.gb_model is not None:\n",
        "            try:\n",
        "                gb_pred_proba = self.gb_model.predict_proba(X_test)[:, 1]\n",
        "                gb_pred = self.gb_model.predict(X_test)\n",
        "\n",
        "                gb_metrics = {\n",
        "                    'accuracy': accuracy_score(y_test, gb_pred),\n",
        "                    'precision': precision_score(y_test, gb_pred, zero_division=0),\n",
        "                    'recall': recall_score(y_test, gb_pred, zero_division=0),\n",
        "                    'f1': f1_score(y_test, gb_pred, zero_division=0),\n",
        "                    'auc_roc': roc_auc_score(y_test, gb_pred_proba),\n",
        "                    'mcc': matthews_corrcoef(y_test, gb_pred)\n",
        "                }\n",
        "                results['Gradient Boosting'] = gb_metrics\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"GB evaluation failed: {e}\")\n",
        "\n",
        "        # Evaluate Neural Network\n",
        "        if self.nn_model is not None and self.nn_model.model is not None:\n",
        "            try:\n",
        "                if hasattr(self.nn_model, 'seq_features'):\n",
        "                    # LSTM model\n",
        "                    seq_data_test, _ = self.nn_model.prepare_lstm_data(X_test, X_test.columns)\n",
        "                    nn_pred_proba = self.nn_model.model.predict([X_test.values.astype(np.float32), seq_data_test])\n",
        "                    nn_pred_proba = nn_pred_proba.flatten()\n",
        "                else:\n",
        "                    # Standard model\n",
        "                    if hasattr(self.nn_model.model, 'predict_proba'):\n",
        "                        nn_pred_proba = self.nn_model.model.predict_proba(X_test)[:, 1]\n",
        "                    else:\n",
        "                        nn_pred_proba = self.nn_model.model.predict(X_test.values.astype(np.float32))\n",
        "                        if nn_pred_proba.ndim > 1:\n",
        "                            nn_pred_proba = nn_pred_proba.flatten()\n",
        "\n",
        "                nn_pred = (nn_pred_proba > 0.5).astype(int)\n",
        "\n",
        "                nn_metrics = {\n",
        "                    'accuracy': accuracy_score(y_test, nn_pred),\n",
        "                    'precision': precision_score(y_test, nn_pred, zero_division=0),\n",
        "                    'recall': recall_score(y_test, nn_pred, zero_division=0),\n",
        "                    'f1': f1_score(y_test, nn_pred, zero_division=0),\n",
        "                    'auc_roc': roc_auc_score(y_test, nn_pred_proba),\n",
        "                    'mcc': matthews_corrcoef(y_test, nn_pred)\n",
        "                }\n",
        "                results['Neural Network'] = nn_metrics\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"NN evaluation failed: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def plot_results(self, results):\n",
        "        \"\"\"Plot comparison results\"\"\"\n",
        "        if not results:\n",
        "            print(\"No results to plot\")\n",
        "            return\n",
        "\n",
        "        # Create comparison plot\n",
        "        metrics = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc', 'mcc']\n",
        "        models = list(results.keys())\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "        x = np.arange(len(metrics))\n",
        "        width = 0.35\n",
        "\n",
        "        for i, model in enumerate(models):\n",
        "            values = [results[model].get(metric, 0) for metric in metrics]\n",
        "            ax.bar(x + i * width, values, width, label=model, alpha=0.8)\n",
        "\n",
        "        ax.set_xlabel('Metrics')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Model Performance Comparison')\n",
        "        ax.set_xticks(x + width/2)\n",
        "        ax.set_xticklabels(metrics, rotation=45)\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(0, 1.1)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Print results table\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"MODEL EVALUATION RESULTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        df_results = pd.DataFrame(results).T\n",
        "        df_results = df_results.round(4)\n",
        "        print(df_results.to_string())\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def run_complete_pipeline(self, file_path, save_models=False):\n",
        "        \"\"\"\n",
        "        Run the complete dual-path machine learning pipeline\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the telematics dataset\n",
        "            save_models (bool): Whether to save trained models\n",
        "\n",
        "        Returns:\n",
        "            dict: Complete results from the pipeline\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "        print(\"=\"*80)\n",
        "        print(\"STARTING DUAL-PATH MACHINE LEARNING FRAMEWORK\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            # Step 1: Load and preprocess data\n",
        "            X, y = self.load_and_preprocess_data(file_path)\n",
        "\n",
        "            # Step 2: Split data\n",
        "            X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)\n",
        "\n",
        "            # Step 3: Handle class imbalance\n",
        "            X_train_balanced, y_train_balanced = self.handle_class_imbalance(X_train, y_train)\n",
        "\n",
        "            # Step 4: Feature selection\n",
        "            selected_features, feature_scores = self.select_features(X_train_balanced, y_train_balanced)\n",
        "\n",
        "            # Apply feature selection to all datasets\n",
        "            X_train_selected = X_train_balanced[selected_features]\n",
        "            X_val_selected = X_val[selected_features]\n",
        "            X_test_selected = X_test[selected_features]\n",
        "\n",
        "            # Step 5: Train Gradient Boosting model\n",
        "            gb_model = self.train_gradient_boosting(X_train_selected, y_train_balanced)\n",
        "\n",
        "            # Step 6: Train Neural Network model\n",
        "            nn_model, nn_history = self.train_neural_network(\n",
        "                X_train_selected, y_train_balanced, X_val_selected, y_val\n",
        "            )\n",
        "\n",
        "            # Step 7: Evaluate models\n",
        "            evaluation_results = self.evaluate_models(X_test_selected, y_test)\n",
        "\n",
        "            # Step 8: Plot results\n",
        "            self.plot_results(evaluation_results)\n",
        "\n",
        "            # Step 9: Save models if requested\n",
        "            if save_models:\n",
        "                try:\n",
        "                    import os\n",
        "                    save_path = 'models/'\n",
        "                    if not os.path.exists(save_path):\n",
        "                        os.makedirs(save_path)\n",
        "\n",
        "                    if self.gb_model is not None:\n",
        "                        joblib.dump(self.gb_model, os.path.join(save_path, 'gb_model.pkl'))\n",
        "\n",
        "                    if self.preprocessor is not None:\n",
        "                        joblib.dump(self.preprocessor, os.path.join(save_path, 'preprocessor.pkl'))\n",
        "\n",
        "                    if self.feature_selector is not None:\n",
        "                        joblib.dump(self.feature_selector, os.path.join(save_path, 'feature_selector.pkl'))\n",
        "\n",
        "                    print(f\"Models saved to {save_path}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to save models: {e}\")\n",
        "\n",
        "            end_time = time.time()\n",
        "            total_time = end_time - start_time\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\"DUAL-PATH FRAMEWORK COMPLETED SUCCESSFULLY\")\n",
        "            print(\"=\"*80)\n",
        "            print(f\"Total execution time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "\n",
        "            # Find best model\n",
        "            if evaluation_results:\n",
        "                best_model = max(evaluation_results.keys(),\n",
        "                               key=lambda x: evaluation_results[x].get('f1', 0))\n",
        "                best_f1 = evaluation_results[best_model]['f1']\n",
        "                best_auc = evaluation_results[best_model]['auc_roc']\n",
        "\n",
        "                print(f\"Best performing model: {best_model}\")\n",
        "                print(f\"  F1 Score: {best_f1:.4f}\")\n",
        "                print(f\"  AUC Score: {best_auc:.4f}\")\n",
        "\n",
        "            # Summary of results\n",
        "            pipeline_summary = {\n",
        "                'execution_time': total_time,\n",
        "                'selected_features': selected_features,\n",
        "                'feature_scores': feature_scores,\n",
        "                'model_results': evaluation_results,\n",
        "                'data_stats': {\n",
        "                    'original_shape': X.shape,\n",
        "                    'train_size': len(X_train_selected),\n",
        "                    'val_size': len(X_val_selected),\n",
        "                    'test_size': len(X_test_selected),\n",
        "                    'class_distribution': pd.Series(y).value_counts().to_dict()\n",
        "                }\n",
        "            }\n",
        "\n",
        "            if evaluation_results:\n",
        "                pipeline_summary['best_model'] = best_model\n",
        "                pipeline_summary['best_f1_score'] = best_f1\n",
        "                pipeline_summary['best_auc_score'] = best_auc\n",
        "\n",
        "            self.results = pipeline_summary\n",
        "            return pipeline_summary\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Pipeline failed with error: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def predict_new_data(self, new_data_path):\n",
        "        \"\"\"\n",
        "        Make predictions on new data using trained models\n",
        "\n",
        "        Args:\n",
        "            new_data_path (str): Path to new data file\n",
        "\n",
        "        Returns:\n",
        "            dict: Predictions from all models\n",
        "        \"\"\"\n",
        "        if self.gb_model is None and (self.nn_model is None or self.nn_model.model is None):\n",
        "            raise ValueError(\"Models not trained yet. Run the pipeline first.\")\n",
        "\n",
        "        print(\"Making predictions on new data...\")\n",
        "\n",
        "        try:\n",
        "            # Load and preprocess new data\n",
        "            new_df = pd.read_csv(new_data_path)\n",
        "            new_X_processed = self.preprocessor.transform(new_df)\n",
        "\n",
        "            # Apply feature selection\n",
        "            if self.feature_selector and self.feature_selector.selected_features:\n",
        "                # Ensure all selected features exist\n",
        "                missing_features = set(self.feature_selector.selected_features) - set(new_X_processed.columns)\n",
        "                for feat in missing_features:\n",
        "                    new_X_processed[feat] = 0\n",
        "                new_X_selected = new_X_processed[self.feature_selector.selected_features]\n",
        "            else:\n",
        "                new_X_selected = new_X_processed\n",
        "\n",
        "            predictions = {}\n",
        "\n",
        "            # Gradient Boosting predictions\n",
        "            if self.gb_model is not None:\n",
        "                try:\n",
        "                    gb_proba = self.gb_model.predict_proba(new_X_selected)[:, 1]\n",
        "                    gb_pred = self.gb_model.predict(new_X_selected)\n",
        "                    predictions['gradient_boosting'] = {\n",
        "                        'probabilities': gb_proba,\n",
        "                        'predictions': gb_pred\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"GB prediction failed: {e}\")\n",
        "\n",
        "            # Neural Network predictions\n",
        "            if self.nn_model is not None and self.nn_model.model is not None:\n",
        "                try:\n",
        "                    if hasattr(self.nn_model, 'seq_features'):\n",
        "                        # LSTM model\n",
        "                        seq_data_new, _ = self.nn_model.prepare_lstm_data(new_X_selected, new_X_selected.columns)\n",
        "                        nn_proba = self.nn_model.model.predict([new_X_selected.values.astype(np.float32), seq_data_new])\n",
        "                        nn_proba = nn_proba.flatten()\n",
        "                    else:\n",
        "                        # Standard model\n",
        "                        if hasattr(self.nn_model.model, 'predict_proba'):\n",
        "                            nn_proba = self.nn_model.model.predict_proba(new_X_selected)[:, 1]\n",
        "                        else:\n",
        "                            nn_proba = self.nn_model.model.predict(new_X_selected.values.astype(np.float32))\n",
        "                            if nn_proba.ndim > 1:\n",
        "                                nn_proba = nn_proba.flatten()\n",
        "\n",
        "                    nn_pred = (nn_proba > 0.5).astype(int)\n",
        "                    predictions['neural_network'] = {\n",
        "                        'probabilities': nn_proba,\n",
        "                        'predictions': nn_pred\n",
        "                    }\n",
        "                except Exception as e:\n",
        "                    print(f\"NN prediction failed: {e}\")\n",
        "\n",
        "            print(f\"Generated predictions for {len(new_X_selected)} samples\")\n",
        "\n",
        "            return predictions\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction failed: {e}\")\n",
        "            return None\n",
        "\n",
        "# =============================================================================\n",
        "# ADDITIONAL UTILITY FUNCTIONS\n",
        "# =============================================================================\n",
        "\n",
        "def load_models(load_path='models/'):\n",
        "    \"\"\"Load previously saved models\"\"\"\n",
        "    import os\n",
        "\n",
        "    framework = DualPathFramework()\n",
        "\n",
        "    # Load preprocessor\n",
        "    preprocessor_path = os.path.join(load_path, 'preprocessor.pkl')\n",
        "    if os.path.exists(preprocessor_path):\n",
        "        framework.preprocessor = joblib.load(preprocessor_path)\n",
        "        print(\"Preprocessor loaded\")\n",
        "\n",
        "    # Load feature selector\n",
        "    feature_selector_path = os.path.join(load_path, 'feature_selector.pkl')\n",
        "    if os.path.exists(feature_selector_path):\n",
        "        framework.feature_selector = joblib.load(feature_selector_path)\n",
        "        print(\"Feature selector loaded\")\n",
        "\n",
        "    # Load gradient boosting model\n",
        "    gb_model_path = os.path.join(load_path, 'gb_model.pkl')\n",
        "    if os.path.exists(gb_model_path):\n",
        "        framework.gb_model = joblib.load(gb_model_path)\n",
        "        print(\"Gradient Boosting model loaded\")\n",
        "\n",
        "    print(f\"Models loaded from {load_path}\")\n",
        "    return framework\n",
        "\n",
        "def run_driver_risk_assessment_pipeline(data_path, config=None, save_models=True):\n",
        "    \"\"\"\n",
        "    Convenience function to run the complete driver risk assessment pipeline\n",
        "\n",
        "    Args:\n",
        "        data_path (str): Path to the telematics dataset\n",
        "        config (dict): Optional configuration dictionary\n",
        "        save_models (bool): Whether to save trained models\n",
        "\n",
        "    Returns:\n",
        "        tuple: (framework_instance, results_summary)\n",
        "    \"\"\"\n",
        "    # Initialize framework\n",
        "    framework = DualPathFramework(config)\n",
        "\n",
        "    # Run complete pipeline\n",
        "    results = framework.run_complete_pipeline(data_path, save_models=save_models)\n",
        "\n",
        "    return framework, results\n",
        "\n",
        "def create_sample_config():\n",
        "    \"\"\"Create a sample configuration for the framework\"\"\"\n",
        "    return {\n",
        "        'preprocessing': {\n",
        "            'scaling_method': 'standard',  # 'standard', 'robust', 'minmax'\n",
        "            'handle_outliers': True\n",
        "        },\n",
        "        'imbalance_handling': {\n",
        "            'method': 'smote_tomek',  # 'smote', 'smote_tomek', 'adasyn'\n",
        "            'random_state': 42\n",
        "        },\n",
        "        'feature_selection': {\n",
        "            'n_features': 15,  # Number of features to select\n",
        "            'methods': ['mutual_info', 'rfe']  # Feature selection methods\n",
        "        },\n",
        "        'models': {\n",
        "            'gb_optimization': {\n",
        "                'cv_folds': 3,  # Number of CV folds\n",
        "                'n_iter': 15    # Number of random search iterations\n",
        "            },\n",
        "            'nn_config': {\n",
        "                'include_lstm': False,  # Whether to include LSTM\n",
        "                'epochs': 30,           # Training epochs\n",
        "                'batch_size': 64        # Batch size\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "# =============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# =============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage with error handling\n",
        "\n",
        "    # Your dataset path\n",
        "    DATA_PATH = 'telematics_syn.csv'  # Update this path\n",
        "\n",
        "    # Option 1: Use default configuration\n",
        "    print(\"Running with default configuration...\")\n",
        "    framework, results = run_driver_risk_assessment_pipeline(DATA_PATH)\n",
        "\n",
        "    if results is not None:\n",
        "        print(\"Pipeline completed successfully!\")\n",
        "        print(f\"Best model: {results.get('best_model', 'N/A')}\")\n",
        "        print(f\"Best F1 Score: {results.get('best_f1_score', 0):.4f}\")\n",
        "        print(f\"Best AUC Score: {results.get('best_auc_score', 0):.4f}\")\n",
        "    else:\n",
        "        print(\"Pipeline failed. Check the error messages above.\")\n",
        "\n",
        "    # Option 2: Use custom configuration\n",
        "    print(\"\\nRunning with custom configuration...\")\n",
        "    custom_config = create_sample_config()\n",
        "    custom_config['models']['nn_config']['include_lstm'] = True  # Enable LSTM\n",
        "\n",
        "    framework_custom, results_custom = run_driver_risk_assessment_pipeline(\n",
        "        DATA_PATH,\n",
        "        config=custom_config,\n",
        "        save_models=True\n",
        "    )\n",
        "\n",
        "    # Option 3: Make predictions on new data (if you have new data)\n",
        "    if framework.gb_model is not None:\n",
        "        try:\n",
        "            # predictions = framework.predict_new_data('new_telematics_data.csv')\n",
        "            # print(f\"Predictions generated: {len(predictions)}\")\n",
        "            pass\n",
        "        except Exception as e:\n",
        "            print(f\"Prediction example skipped: {e}\")\n",
        "\n",
        "print(\"\"\"\n",
        "=============================================================================\n",
        "ENHANCED DUAL-PATH MACHINE LEARNING FRAMEWORK - FIXED VERSION\n",
        "=============================================================================\n",
        "\n",
        "Key Improvements Made:\n",
        "1. ✅ Fixed data type handling and conversion issues\n",
        "2. ✅ Simplified xLSTM implementation to standard LSTM\n",
        "3. ✅ Added comprehensive error handling throughout\n",
        "4. ✅ Fixed neural network input data type issues\n",
        "5. ✅ Improved preprocessing with better categorical handling\n",
        "6. ✅ Reduced parameter search space for faster execution\n",
        "7. ✅ Added fallback mechanisms for failed components\n",
        "8. ✅ Simplified ensemble approach\n",
        "9. ✅ Better memory management and data handling\n",
        "10. ✅ Comprehensive evaluation and visualization\n",
        "\n",
        "Usage Examples:\n",
        "\n",
        "# Basic usage\n",
        "framework, results = run_driver_risk_assessment_pipeline('your_data.csv')\n",
        "\n",
        "# Custom configuration\n",
        "config = create_sample_config()\n",
        "config['feature_selection']['n_features'] = 25\n",
        "framework, results = run_driver_risk_assessment_pipeline('your_data.csv', config)\n",
        "\n",
        "# Load saved models\n",
        "loaded_framework = load_models('models/')\n",
        "predictions = loaded_framework.predict_new_data('new_data.csv')\n",
        "\n",
        "Features:\n",
        "- Robust data preprocessing with type handling\n",
        "- Advanced feature selection and engineering\n",
        "- Class imbalance handling with SMOTE\n",
        "- Gradient Boosting with hyperparameter optimization\n",
        "- Neural Network with optional LSTM integration\n",
        "- Comprehensive model evaluation and visualization\n",
        "- Model saving and loading capabilities\n",
        "- Error handling and fallback mechanisms\n",
        "\n",
        "=============================================================================\n",
        "\"\"\")>"
      ],
      "metadata": {
        "id": "hiXmU6lzhw0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}