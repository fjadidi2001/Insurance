{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "ClaimYN_Prediction_TabNet_80result.ipynb",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN7ivNa9UW1fG3ToEgDDFBR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/ClaimYN_Prediction_TabNet_80result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxre3ydMU1vF",
        "outputId": "a62830c8-7739-48f9-af33-fac97ec456b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/Insurance/telematics_syn.csv'\n",
        "\n",
        "# Import pandas (assuming you want to use it to read the CSV)\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "print(df.shape)  # Should print (100000, 52)\n",
        "\n",
        "print(df.head())  # Example: print the first few rows of the dataframe\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTxzMntBVCZ_",
        "outputId": "3a68a884-932e-4212-8e06-3b3443285fd3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(100000, 52)\n",
            "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
            "0       366           45        Male       -1  Married  Commute         609.0   \n",
            "1       182           44      Female        3  Married  Commute         575.0   \n",
            "2       184           48      Female        6  Married  Commute         847.0   \n",
            "3       183           71        Male        6  Married  Private         842.0   \n",
            "4       183           84        Male       10  Married  Private         856.0   \n",
            "\n",
            "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
            "0  Urban             6213.71              25  ...                    1.0   \n",
            "1  Urban            12427.42              20  ...                   58.0   \n",
            "2  Urban            12427.42              14  ...                    0.0   \n",
            "3  Urban             6213.71              43  ...                    0.0   \n",
            "4  Urban             6213.71              65  ...                    2.0   \n",
            "\n",
            "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "0                    0.0                    0.0                     3.0   \n",
            "1                   24.0                   11.0                  1099.0   \n",
            "2                    0.0                    0.0                     0.0   \n",
            "3                    0.0                    0.0                     0.0   \n",
            "4                    0.0                    0.0                   325.0   \n",
            "\n",
            "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "0                     1.0                     0.0                     0.0   \n",
            "1                   615.0                   219.0                   101.0   \n",
            "2                     0.0                     0.0                     0.0   \n",
            "3                     0.0                     0.0                     0.0   \n",
            "4                   111.0                    18.0                     4.0   \n",
            "\n",
            "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
            "0                     0.0         1  5100.171753  \n",
            "1                    40.0         1   883.554840  \n",
            "2                     0.0         0     0.000000  \n",
            "3                     0.0         0     0.000000  \n",
            "4                     2.0         0     0.000000  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#step 2"
      ],
      "metadata": {
        "id": "Ok2DoiD5fsPQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre1"
      ],
      "metadata": {
        "id": "nt77pX65f_ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical columns using one-hot encoding\n",
        "categorical_cols = ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']\n",
        "df = pd.get_dummies(df, columns=categorical_cols)\n",
        "\n",
        "# Creating the ClaimYN variable\n",
        "df['ClaimYN'] = df['NB_Claim'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Saving the preprocessed data to a new file\n",
        "preprocessed_file_path = '/content/drive/My Drive/pre_telematics_syn.csv'\n",
        "df.to_csv(preprocessed_file_path, index=False)\n",
        "print(df.shape)  # Should print (100000, number of columns after encoding)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4liZny1fp8V",
        "outputId": "fc40ce21-523c-4579-8762-c891ed4a9117"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 113)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre2"
      ],
      "metadata": {
        "id": "ZBcFKlvbgE2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Feature columns (excluding response columns)\n",
        "feature_cols = [col for col in df.columns if col not in ['NB_Claim', 'AMT_Claim', 'ClaimYN']]\n",
        "\n",
        "# Applying StandardScaler and MinMaxScaler\n",
        "scaler = StandardScaler()\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "df[feature_cols] = scaler.fit_transform(df[feature_cols])\n",
        "df[feature_cols] = minmax_scaler.fit_transform(df[feature_cols])\n",
        "\n",
        "# Save the preprocessed data to a new file\n",
        "preprocessed_file_path_scaled = '/content/drive/My Drive/pre_telematics_syn_scaled.csv'\n",
        "df.to_csv(preprocessed_file_path_scaled, index=False)\n",
        "print(df.shape)  # Should print (100000, number of columns after scaling and normalization)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qr_HBUAggGn8",
        "outputId": "75c75d52-be8a-4f3d-b132-a11201fbc117"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100000, 113)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre3"
      ],
      "metadata": {
        "id": "DfnrpjvvgLoB"
      }
    },
    {
      "source": [
        "# Advanced feature engineering steps, if any\n",
        "# For example, aggregating harsh driving events:\n",
        "# Check if the columns exist before using them\n",
        "if all(col in df.columns for col in ['Accel.xx.miles', 'Brake.xx.miles', 'Left.turn.intensityxx', 'Right.turn.intensityxx']):\n",
        "    df['HarshDriving'] = df['Accel.xx.miles'] + df['Brake.xx.miles'] + df['Left.turn.intensityxx'] + df['Right.turn.intensityxx']\n",
        "else:\n",
        "    print(\"Warning: One or more columns required for 'HarshDriving' calculation are missing.\")\n",
        "\n",
        "# Save the fully preprocessed data to a new file\n",
        "preprocessed_file_path_final = '/content/drive/My Drive/pre_telematics_syn_advanced.csv'\n",
        "df.to_csv(preprocessed_file_path_final, index=False)\n",
        "print(df.shape)  # Should print (100000, number of columns after feature engineering)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weJCxT8lgaIQ",
        "outputId": "4796525f-bc4a-49f4-cadd-81fc5a78a548"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: One or more columns required for 'HarshDriving' calculation are missing.\n",
            "(100000, 113)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the response and feature columns\n",
        "response_cols = ['NB_Claim', 'AMT_Claim', 'ClaimYN']\n",
        "feature_cols = [col for col in df.columns if col not in response_cols]\n",
        "\n",
        "# Split into 80% train and 20% test\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split train into train and validation\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "print(train_df.shape, val_df.shape, test_df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnc1tLTkdh3c",
        "outputId": "4a45f082-26e8-43f3-8f29-995271133752"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64000, 113) (16000, 113) (20000, 113)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIiUK8D5dj2p",
        "outputId": "ff94135e-d5d6-4eb6-c077-2b41808fbf00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n",
            "Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytorch-tabnet\n",
            "Successfully installed pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the library\n",
        "!pip install pytorch-tabnet\n",
        "\n",
        "# Import required libraries\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data for TabNet\n",
        "X_train = train_df[feature_cols].values\n",
        "y_train = train_df['ClaimYN'].values\n",
        "X_val = val_df[feature_cols].values\n",
        "y_val = val_df['ClaimYN'].values\n",
        "X_test = test_df[feature_cols].values\n",
        "y_test = test_df['ClaimYN'].values\n",
        "\n",
        "# Create and train the TabNet model\n",
        "clf = TabNetClassifier()\n",
        "\n",
        "clf.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "    eval_name=['train', 'val'],\n",
        "    eval_metric=['accuracy'],\n",
        "    max_epochs=50,\n",
        "    patience=10,\n",
        "    batch_size=1024,\n",
        "    virtual_batch_size=128,\n",
        "    num_workers=0,\n",
        "    weights=1,\n",
        "    drop_last=False\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsV9ZhfSgtBU",
        "outputId": "38fdbbeb-676e-4c2a-be45-f55dcae89969"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-tabnet in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.26.4)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.5.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.6)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.64251 | train_accuracy: 0.17378 | val_accuracy: 0.17856 |  0:00:08s\n",
            "epoch 1  | loss: 0.58654 | train_accuracy: 0.25545 | val_accuracy: 0.25669 |  0:00:12s\n",
            "epoch 2  | loss: 0.56904 | train_accuracy: 0.39398 | val_accuracy: 0.39744 |  0:00:17s\n",
            "epoch 3  | loss: 0.56708 | train_accuracy: 0.45408 | val_accuracy: 0.4575  |  0:00:23s\n",
            "epoch 4  | loss: 0.56379 | train_accuracy: 0.57734 | val_accuracy: 0.57788 |  0:00:28s\n",
            "epoch 5  | loss: 0.55983 | train_accuracy: 0.51839 | val_accuracy: 0.51962 |  0:00:34s\n",
            "epoch 6  | loss: 0.55754 | train_accuracy: 0.61255 | val_accuracy: 0.61325 |  0:00:39s\n",
            "epoch 7  | loss: 0.54772 | train_accuracy: 0.6257  | val_accuracy: 0.62606 |  0:00:44s\n",
            "epoch 8  | loss: 0.53942 | train_accuracy: 0.63492 | val_accuracy: 0.63256 |  0:00:50s\n",
            "epoch 9  | loss: 0.53525 | train_accuracy: 0.63497 | val_accuracy: 0.63038 |  0:00:55s\n",
            "epoch 10 | loss: 0.53837 | train_accuracy: 0.63761 | val_accuracy: 0.63319 |  0:01:00s\n",
            "epoch 11 | loss: 0.52921 | train_accuracy: 0.6837  | val_accuracy: 0.67631 |  0:01:06s\n",
            "epoch 12 | loss: 0.5331  | train_accuracy: 0.66447 | val_accuracy: 0.66069 |  0:01:11s\n",
            "epoch 13 | loss: 0.51984 | train_accuracy: 0.68077 | val_accuracy: 0.67381 |  0:01:17s\n",
            "epoch 14 | loss: 0.51894 | train_accuracy: 0.66611 | val_accuracy: 0.66069 |  0:01:22s\n",
            "epoch 15 | loss: 0.50466 | train_accuracy: 0.66267 | val_accuracy: 0.65562 |  0:01:27s\n",
            "epoch 16 | loss: 0.49814 | train_accuracy: 0.65109 | val_accuracy: 0.64381 |  0:01:33s\n",
            "epoch 17 | loss: 0.48641 | train_accuracy: 0.64384 | val_accuracy: 0.63375 |  0:01:38s\n",
            "epoch 18 | loss: 0.48089 | train_accuracy: 0.67875 | val_accuracy: 0.67    |  0:01:44s\n",
            "epoch 19 | loss: 0.47189 | train_accuracy: 0.69333 | val_accuracy: 0.68338 |  0:01:49s\n",
            "epoch 20 | loss: 0.46291 | train_accuracy: 0.6787  | val_accuracy: 0.6675  |  0:01:54s\n",
            "epoch 21 | loss: 0.46094 | train_accuracy: 0.6793  | val_accuracy: 0.6635  |  0:02:00s\n",
            "epoch 22 | loss: 0.44621 | train_accuracy: 0.69606 | val_accuracy: 0.68375 |  0:02:05s\n",
            "epoch 23 | loss: 0.44908 | train_accuracy: 0.73317 | val_accuracy: 0.71494 |  0:02:10s\n",
            "epoch 24 | loss: 0.42573 | train_accuracy: 0.72889 | val_accuracy: 0.70888 |  0:02:16s\n",
            "epoch 25 | loss: 0.41919 | train_accuracy: 0.71545 | val_accuracy: 0.69069 |  0:02:21s\n",
            "epoch 26 | loss: 0.42191 | train_accuracy: 0.67372 | val_accuracy: 0.65519 |  0:02:26s\n",
            "epoch 27 | loss: 0.42843 | train_accuracy: 0.72255 | val_accuracy: 0.70144 |  0:02:32s\n",
            "epoch 28 | loss: 0.40899 | train_accuracy: 0.76794 | val_accuracy: 0.75006 |  0:02:37s\n",
            "epoch 29 | loss: 0.39331 | train_accuracy: 0.73819 | val_accuracy: 0.7175  |  0:02:43s\n",
            "epoch 30 | loss: 0.39295 | train_accuracy: 0.74792 | val_accuracy: 0.72725 |  0:02:48s\n",
            "epoch 31 | loss: 0.38163 | train_accuracy: 0.77092 | val_accuracy: 0.74956 |  0:02:53s\n",
            "epoch 32 | loss: 0.37574 | train_accuracy: 0.76564 | val_accuracy: 0.73775 |  0:02:58s\n",
            "epoch 33 | loss: 0.3705  | train_accuracy: 0.77812 | val_accuracy: 0.75125 |  0:03:03s\n",
            "epoch 34 | loss: 0.3757  | train_accuracy: 0.75012 | val_accuracy: 0.71875 |  0:03:09s\n",
            "epoch 35 | loss: 0.36415 | train_accuracy: 0.76564 | val_accuracy: 0.73731 |  0:03:14s\n",
            "epoch 36 | loss: 0.36319 | train_accuracy: 0.77114 | val_accuracy: 0.743   |  0:03:19s\n",
            "epoch 37 | loss: 0.36308 | train_accuracy: 0.77677 | val_accuracy: 0.74656 |  0:03:25s\n",
            "epoch 38 | loss: 0.35596 | train_accuracy: 0.781   | val_accuracy: 0.75081 |  0:03:30s\n",
            "epoch 39 | loss: 0.34864 | train_accuracy: 0.77888 | val_accuracy: 0.7525  |  0:03:35s\n",
            "epoch 40 | loss: 0.34306 | train_accuracy: 0.79789 | val_accuracy: 0.76556 |  0:03:41s\n",
            "epoch 41 | loss: 0.33889 | train_accuracy: 0.79817 | val_accuracy: 0.76812 |  0:03:46s\n",
            "epoch 42 | loss: 0.33444 | train_accuracy: 0.80994 | val_accuracy: 0.77519 |  0:03:52s\n",
            "epoch 43 | loss: 0.32213 | train_accuracy: 0.80473 | val_accuracy: 0.77025 |  0:03:56s\n",
            "epoch 44 | loss: 0.33221 | train_accuracy: 0.81045 | val_accuracy: 0.77681 |  0:04:01s\n",
            "epoch 45 | loss: 0.32828 | train_accuracy: 0.81053 | val_accuracy: 0.778   |  0:04:07s\n",
            "epoch 46 | loss: 0.32011 | train_accuracy: 0.81075 | val_accuracy: 0.77888 |  0:04:12s\n",
            "epoch 47 | loss: 0.33092 | train_accuracy: 0.80211 | val_accuracy: 0.77256 |  0:04:18s\n",
            "epoch 48 | loss: 0.32503 | train_accuracy: 0.81344 | val_accuracy: 0.77794 |  0:04:23s\n",
            "epoch 49 | loss: 0.31221 | train_accuracy: 0.82202 | val_accuracy: 0.78638 |  0:04:28s\n",
            "Stop training because you reached max_epochs = 50 with best_epoch = 49 and best_val_accuracy = 0.78638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import ast\n",
        "import traceback\n",
        "\n",
        "# Function to evaluate functionality and handle errors\n",
        "def evaluate_functionality(code: str, *args):\n",
        "    try:\n",
        "        exec_globals = {}\n",
        "        exec(code, exec_globals)\n",
        "\n",
        "        if 'main' in exec_globals:\n",
        "            start_time = time.time()\n",
        "            result = exec_globals['main'](*args)  # Assuming a main function takes arguments\n",
        "            end_time = time.time()\n",
        "            return {\n",
        "                'result': result,\n",
        "                'execution_time': end_time - start_time,\n",
        "                'error': None\n",
        "            }\n",
        "        else:\n",
        "            return {'result': None, 'execution_time': None, 'error': 'No main function found'}\n",
        "    except Exception as e:\n",
        "        return {'result': None, 'execution_time': None, 'error': str(e)}\n",
        "\n",
        "# Function to evaluate readability (basic check)\n",
        "def evaluate_readability(code: str):\n",
        "    try:\n",
        "        # Check for indentation errors using AST\n",
        "        ast.parse(code)\n",
        "        lines = code.split('\\n')\n",
        "        length = sum(len(line) for line in lines)\n",
        "        avg_length = length / len(lines) if lines else 0\n",
        "        return {\n",
        "            'line_count': len(lines),\n",
        "            'average_length_per_line': avg_length,\n",
        "            'error': None\n",
        "        }\n",
        "    except SyntaxError as e:\n",
        "        return {'line_count': None, 'average_length_per_line': None, 'error': str(e)}\n",
        "\n",
        "# Main evaluation function\n",
        "def evaluate_code(code: str, *args):\n",
        "    print(\"Evaluating functionality...\")\n",
        "    functionality_results = evaluate_functionality(code, *args)\n",
        "\n",
        "    print(\"Evaluating readability...\")\n",
        "    readability_results = evaluate_readability(code)\n",
        "\n",
        "    return {\n",
        "        'functionality': functionality_results,\n",
        "        'readability': readability_results\n",
        "    }\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    sample_code = \"\"\"\n",
        "def main(x):\n",
        "    return x * x  # Simple function to square a number\n",
        "    \"\"\"\n",
        "\n",
        "    evaluation_results = evaluate_code(sample_code, 5)\n",
        "    print(evaluation_results)"
      ],
      "metadata": {
        "id": "nHQ0_IO1s3zM",
        "outputId": "3f48a15a-57c1-48c4-900d-c3ffb983abbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating functionality...\n",
            "Evaluating readability...\n",
            "{'functionality': {'result': 25, 'execution_time': 1.1920928955078125e-06, 'error': None}, 'readability': {'line_count': 4, 'average_length_per_line': 19.0, 'error': None}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import ast\n",
        "import traceback\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to evaluate functionality and handle errors\n",
        "def evaluate_functionality(code: str, *args):\n",
        "    try:\n",
        "        exec_globals = {}\n",
        "        exec(code, exec_globals)\n",
        "\n",
        "        # Assuming the code defines a function named 'model_predict' and 'true_labels'\n",
        "        predictions = exec_globals['model_predict'](*args)\n",
        "        true_labels = exec_globals['true_labels']\n",
        "\n",
        "        # Generate confusion matrix\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "\n",
        "        # Display the confusion matrix\n",
        "        disp.plot(cmap=plt.cm.Blues)\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "        return predictions  # Return predictions for further use if needed\n",
        "    except Exception as e:\n",
        "        print(\"An error occurred:\")\n",
        "        traceback.print_exc()\n",
        "\n",
        "# Sample code to evaluate\n",
        "sample_code = \"\"\"\n",
        "import numpy as np\n",
        "\n",
        "# Simulating true labels and a model prediction function\n",
        "true_labels = [0, 1, 0, 1, 0, 1, 0, 1]  # Example true labels\n",
        "\n",
        "def model_predict(*args):\n",
        "    # Simple mock prediction logic (replace with actual model prediction)\n",
        "    return [1 if i % 2 == 0 else 0 for i in range(len(args))]\n",
        "\"\"\"\n",
        "\n",
        "# Evaluate the sample code\n",
        "evaluation_results = evaluate_functionality(sample_code, 5)\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "id": "XKusHu0XuD9T",
        "outputId": "bb86db9a-8878-48ae-fc3a-5d4b48ab7be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred:\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-12-efab91576aff>\", line 18, in evaluate_functionality\n",
            "    cm = confusion_matrix(true_labels, predictions)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\", line 342, in confusion_matrix\n",
            "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\", line 103, in _check_targets\n",
            "    check_consistent_length(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\", line 457, in check_consistent_length\n",
            "    raise ValueError(\n",
            "ValueError: Found input variables with inconsistent numbers of samples: [8, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. load dataset from google drive\n",
        "2. create claimYN from NB claim and AMT-claim\n",
        "3. 3 level prepreocess\n",
        "4. simplr TabNet model\n",
        "\n",
        "result:\n",
        "<br>\n",
        "**max_epochs = 50 with best_epoch = 46 and best_val_accuracy = 0.78419**\n"
      ],
      "metadata": {
        "id": "sd22RVp5ny2q"
      }
    }
  ]
}