{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/NB_Claim_Prediction_FirstV_without_balance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac_RBikYk_NY"
      },
      "source": [
        "# Upload the dataset to Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "ZSiPXKBnivBl",
        "outputId": "9a6a1ec8-d5f2-4318-c86d-987cca5af07f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ac05130-8711-4c7d-b17e-97e8c3289220\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ac05130-8711-4c7d-b17e-97e8c3289220\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f395a7cd78e3>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Upload the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    161\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Upload the file\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8AaKfUzlFHq"
      },
      "source": [
        "## Read the uploaded CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "xs7yXH1PkNhi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('telematics_syn.csv')\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe1xOQcilH5z"
      },
      "source": [
        "## Perform Basic Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7x-FHPBfkVJe"
      },
      "outputs": [],
      "source": [
        "  print(\"Shape of the dataset:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sytx81jXz0CW"
      },
      "outputs": [],
      "source": [
        "# Adjust display options to show all columns\n",
        "pd.set_option('display.max_columns', None)  # Show all columns\n",
        "pd.set_option('display.expand_frame_repr', False)  # Prevent line wrapping\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6kcZS82lMcv"
      },
      "source": [
        "## Get summary statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cT1DJIN3ke9a"
      },
      "outputs": [],
      "source": [
        "print(\"Summary statistics:\\n\", df.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sC5asEpxlUBR"
      },
      "source": [
        "## Check for missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oN8VpFy9kh70"
      },
      "outputs": [],
      "source": [
        "print(\"Missing values:\\n\", df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLrTLDFO31kg"
      },
      "source": [
        "# Visualize the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKLwx2na35qt"
      },
      "source": [
        "### missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WthhmjWymJ9c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Plotting the missing values\n",
        "missing_values.plot(kind='bar', figsize=(15, 6))\n",
        "plt.title('Missing Values in Dataset')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Number of Missing Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyJFccR-3_PB"
      },
      "source": [
        "### numerical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PeCs53Ikkv-A"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Numerical columns\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Categorical columns\n",
        "categorical_columns = ['Insured.sex', 'Marital', 'Car.use', 'Region']\n",
        "\n",
        "# Subsample the data if necessary (e.g., 10% of the data)\n",
        "sample_size = int(len(df) * 0.1)\n",
        "df_sample = df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Plot histograms for numerical columns\n",
        "for column in numerical_columns[:5]:  # Limit to first 5 numerical columns\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.histplot(df_sample[column], kde=True)\n",
        "    plt.title(f'Histogram of {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n",
        "\n",
        "# Plot box plots for numerical columns\n",
        "for column in numerical_columns[:5]:  # Limit to first 5 numerical columns\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x=df_sample[column])\n",
        "    plt.title(f'Box Plot of {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.show()\n",
        "\n",
        "# Plot bar plots for categorical columns\n",
        "for column in categorical_columns:\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.countplot(y=df_sample[column], order=df_sample[column].value_counts().index)\n",
        "    plt.title(f'Bar Plot of {column}')\n",
        "    plt.xlabel('Frequency')\n",
        "    plt.ylabel(column)\n",
        "    plt.show()\n",
        "\n",
        "# Plot box plots for numerical columns grouped by categorical columns\n",
        "for num_col in numerical_columns[:5]:  # Limit to first 5 numerical columns\n",
        "    for cat_col in categorical_columns:\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        sns.boxplot(x=df_sample[cat_col], y=df_sample[num_col])\n",
        "        plt.title(f'Box Plot of {num_col} by {cat_col}')\n",
        "        plt.xlabel(cat_col)\n",
        "        plt.ylabel(num_col)\n",
        "        plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7gYlsiN3wGH"
      },
      "source": [
        "### heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oMlj8I693UuJ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Numerical columns\n",
        "numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Select a subset of numerical columns for the heatmap (e.g., first 10 columns)\n",
        "subset_columns = numerical_columns[:10]\n",
        "\n",
        "# Compute the correlation matrix for the subset\n",
        "correlation_matrix = df[subset_columns].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahaafCf42RCk"
      },
      "source": [
        "# Develop model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B4Rd71jFwxfF"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_tabnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oUX1UbH2F_7"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Wg7lc08azDgf"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHsLx7Gt2UHu"
      },
      "source": [
        "## Load and Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "La8UDKrjzHtb"
      },
      "outputs": [],
      "source": [
        "# Assuming 'df' is your DataFrame and 'NB_Claim' is the target column\n",
        "X = df.drop('NB_Claim', axis=1)\n",
        "y = df['NB_Claim']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK6G5VC72bmh"
      },
      "source": [
        "## Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e2rIXhcQzLYm"
      },
      "outputs": [],
      "source": [
        "# Identify numerical and categorical columns\n",
        "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# Create transformers\n",
        "numerical_transformer = StandardScaler()\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Create a preprocessor pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_preprocessed = preprocessor.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDNF4Z-CPZ6U"
      },
      "source": [
        "\n",
        "1. It identifies numerical and categorical columns in the dataset `X`.\n",
        "2. It creates transformers for standardizing numerical features and one-hot encoding categorical features.\n",
        "3. It constructs a preprocessor pipeline using the `ColumnTransformer` that applies the respective transformers to numerical and categorical features.\n",
        "4. It fits and transforms the training data `X_train` using the preprocessor pipeline.\n",
        "5. It transforms the testing data `X_test` using the preprocessor pipeline.\n",
        "\n",
        "In summary, the code preprocesses the data by standardizing numerical features and one-hot encoding categorical features for both the training and testing datasets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SowSXDZz0d25"
      },
      "source": [
        "## Explain the code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8zfvXe0TuH"
      },
      "source": [
        "The preprocessing is designed to handle both numerical and categorical features in a dataset, which is essential for preparing data for machine learning models. Here's a breakdown of why each step is necessary:\n",
        "\n",
        "1. **Identify Numerical and Categorical Columns**:\n",
        "   - `numerical_features = X.select_dtypes(include=['int64', 'float64']).columns`: This line selects columns that contain numerical data (integers and floating-point numbers). Numerical features are typically continuous values that can be scaled and normalized.\n",
        "   - `categorical_features = X.select_dtypes(include=['object', 'category']).columns`: This line selects columns that contain categorical data (strings or categorical types). Categorical features are typically discrete values that need to be encoded into numerical formats.\n",
        "\n",
        "2. **Create Transformers**:\n",
        "   - `numerical_transformer = StandardScaler()`: The `StandardScaler` is used to standardize numerical features by removing the mean and scaling to unit variance. This ensures that all numerical features have a similar scale, which is important for many machine learning algorithms.\n",
        "   - `categorical_transformer = OneHotEncoder(handle_unknown='ignore')`: The `OneHotEncoder` is used to convert categorical features into a format that can be provided to machine learning algorithms. It creates binary columns for each category, indicating the presence or absence of a category. The `handle_unknown='ignore'` parameter ensures that the encoder can handle unseen categories during testing without raising an error.\n",
        "\n",
        "3. **Create a Preprocessor Pipeline**:\n",
        "   - `preprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_features), ('cat', categorical_transformer, categorical_features)])`: The `ColumnTransformer` is used to apply different preprocessing steps to different columns of the dataset. It allows you to specify which transformer to use for which set of columns, making it easy to handle mixed data types.\n",
        "\n",
        "4. **Fit and Transform the Training Data**:\n",
        "   - `X_train_preprocessed = preprocessor.fit_transform(X_train)`: This line fits the preprocessor on the training data and then transforms it. The `fit_transform` method learns the parameters (e.g., mean and standard deviation for `StandardScaler`, and category mappings for `OneHotEncoder`) from the training data and applies the transformations.\n",
        "\n",
        "5. **Transform the Testing Data**:\n",
        "   - `X_test_preprocessed = preprocessor.transform(X_test)`: This line applies the same transformations to the testing data using the parameters learned from the training data. It is crucial to use the same preprocessing steps for both training and testing data to ensure consistency and avoid data leakage.\n",
        "\n",
        "By following these steps, the preprocessing code ensures that both numerical and categorical features are properly transformed and ready for machine learning models. This approach helps improve the performance and reliability of the models by handling different types of data appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74y_HB3a2haE"
      },
      "source": [
        "# Initialize and Train the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka5PtIrw2n5g"
      },
      "source": [
        "## TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mF9UUMRuQJhj"
      },
      "outputs": [],
      "source": [
        "# Initialize the TabNet model\n",
        "tabnet_model = TabNetClassifier()\n",
        "\n",
        "# Train the TabNet model\n",
        "tabnet_model.fit(\n",
        "    X_train_preprocessed, y_train,\n",
        "    eval_set=[(X_test_preprocessed, y_test)],\n",
        "    max_epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bnxdS6DQXMKK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Store the loss values\n",
        "tabnet_loss = tabnet_model.history['loss']\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = tabnet_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculate accuracy\n",
        "tabnet_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Plotting Loss and Accuracy for TabNet\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plotting Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(tabnet_loss, label='Loss')\n",
        "plt.title('TabNet Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.bar(['TabNet'], [tabnet_accuracy])\n",
        "plt.title('TabNet Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlpgOWs622LK"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NmBzspMg23ni"
      },
      "outputs": [],
      "source": [
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model.fit(X_train_preprocessed, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "W0KdBAdubrd-"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(objective='multi:softprob', random_state=42)\n",
        "\n",
        "# Train the XGBoost model with evaluation metrics tracking\n",
        "evals = [(X_train_preprocessed, y_train), (X_test_preprocessed, y_test)]  # Evaluation set\n",
        "xgb_model.fit(X_train_preprocessed, y_train, eval_metric=\"mlogloss\", eval_set=evals, verbose=True)\n",
        "\n",
        "# Plotting the training and validation loss\n",
        "results = xgb_model.evals_result()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(results['validation_0']['mlogloss'], label='Train Log Loss')\n",
        "plt.plot(results['validation_1']['mlogloss'], label='Test Log Loss')\n",
        "plt.title('XGBoost Log Loss')\n",
        "plt.xlabel('Boosting Iterations')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IrwHSvAzfHF3"
      },
      "outputs": [],
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize the XGBoost model\n",
        "xgb_model = xgb.XGBClassifier(objective='multi:softprob', random_state=42)\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgb_model.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "# Make predictions on the training and test sets\n",
        "y_train_pred = xgb_model.predict(X_train_preprocessed)\n",
        "y_test_pred = xgb_model.predict(X_test_preprocessed)\n",
        "\n",
        "# Calculate accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Optionally, plot the accuracies (if you want to visualize it)\n",
        "accuracies = [train_accuracy, test_accuracy]\n",
        "labels = ['Train Accuracy', 'Test Accuracy']\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, accuracies, color=['blue', 'orange'])\n",
        "plt.ylim(0, 1)\n",
        "plt.title('XGBoost Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RNSPMBU3IT0"
      },
      "source": [
        "## Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BETv4YYP3Jer"
      },
      "outputs": [],
      "source": [
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "lr_model.fit(X_train_preprocessed, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OyQuF6uBrDen"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Create lists to store loss and accuracy\n",
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "# Custom training loop to track loss and accuracy\n",
        "for epoch in range(100):  # Adjust the number of epochs as needed\n",
        "    lr_model.fit(X_train_preprocessed, y_train)\n",
        "\n",
        "    # Calculate predictions and loss\n",
        "    y_pred = lr_model.predict(X_train_preprocessed)\n",
        "    accuracy = accuracy_score(y_train, y_pred)\n",
        "    loss = np.mean((y_train - lr_model.predict_proba(X_train_preprocessed)[:, 1]) ** 2)  # Mean Squared Error as loss\n",
        "\n",
        "    # Store the loss and accuracy\n",
        "    losses.append(loss)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Plotting loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses, label='Loss', color='red')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(accuracies, label='Accuracy', color='blue')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyYN1wXO4jpM"
      },
      "source": [
        "# Evaluate the Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N9keUyy4o57"
      },
      "source": [
        "## TabNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "qSNfNcqE4lD2"
      },
      "outputs": [],
      "source": [
        "# Evaluate TabNet\n",
        "tabnet_preds = tabnet_model.predict(X_test_preprocessed)\n",
        "print(\"TabNet Accuracy:\", accuracy_score(y_test, tabnet_preds))\n",
        "print(\"TabNet Classification Report:\\n\", classification_report(y_test, tabnet_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCMa3tRSWo7c"
      },
      "source": [
        "In the classification report, the \"support\" column represents the number of instances (samples) in the test set for each class.\n",
        "\n",
        "Specifically, the classification report shows:\n",
        "\n",
        "- For class 0: 19,130 instances\n",
        "- For class 1: 821 instances\n",
        "- For class 2: 47 instances\n",
        "- For class 3: 2 instances\n",
        "\n",
        "The total number of instances in the test set is 20,000 (the sum of the support values for all classes).\n",
        "\n",
        "The \"support\" column is important because it provides context for interpreting the other metrics, such as precision, recall, and F1-score. These metrics are calculated based on the number of true positives, false positives, and false negatives, which are influenced by the number of instances (support) for each class.\n",
        "\n",
        "In a well-balanced dataset, the support values for each class would be similar. However, in real-world scenarios, class imbalance is common, and the support values can vary significantly between classes, which is important to consider when evaluating model performance.\n",
        "\n",
        "\n",
        "In the classification report, the \"macro avg\" and \"weighted avg\" rows provide additional summary metrics that aggregate the per-class metrics in different ways:\n",
        "\n",
        "1. **Macro Average**:\n",
        "   - The macro average is calculated by taking the unweighted mean of the per-class metrics (precision, recall, F1-score).\n",
        "   - It gives equal importance to each class, regardless of the number of instances (support) in the test set.\n",
        "   - The macro average is useful when you want to evaluate the overall performance of the model across all classes, treating each class equally.\n",
        "\n",
        "2. **Weighted Average**:\n",
        "   - The weighted average is calculated by taking the average of the per-class metrics, weighted by the number of instances (support) for each class.\n",
        "   - It gives more importance to the classes with a larger number of instances.\n",
        "   - The weighted average is useful when you want to evaluate the overall performance of the model, considering the relative importance of each class based on the number of instances.\n",
        "\n",
        "In the example you provided:\n",
        "\n",
        "- The macro average precision, recall, and F1-score are all 0.48, indicating that the model's performance is relatively poor when considering each class equally.\n",
        "- The weighted average precision, recall, and F1-score are all 0.99, indicating that the model's performance is very good when considering the relative importance of each class based on the number of instances.\n",
        "\n",
        "The choice of using macro average or weighted average depends on the specific problem and the importance you place on the performance of each class. Macro average is useful when you want to ensure the model performs well on all classes, while weighted average is useful when the class distribution in the test set is more important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFj2DitH40qB"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VpLAweVC42i9"
      },
      "outputs": [],
      "source": [
        "# Evaluate XGBoost\n",
        "xgb_preds = xgb_model.predict(X_test_preprocessed)\n",
        "print(\"XGBoost Accuracy:\", accuracy_score(y_test, xgb_preds))\n",
        "print(\"XGBoost Classification Report:\\n\", classification_report(y_test, xgb_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMz-pqdj477Y"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mYplWWft47GT"
      },
      "outputs": [],
      "source": [
        "# Evaluate Logistic Regression\n",
        "lr_preds = lr_model.predict(X_test_preprocessed)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, lr_preds))\n",
        "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, lr_preds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnQuUYM-50qe"
      },
      "source": [
        "# Evaluate all Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uAklB1EPClWU"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def evaluate_model(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
        "\n",
        "    print(f\"{model_name} Accuracy: {accuracy}\")\n",
        "    print(f\"{model_name} Precision: {precision}\")\n",
        "    print(f\"{model_name} Recall: {recall}\")\n",
        "    print(f\"{model_name} F1 Score: {f1}\")\n",
        "    print(f\"{model_name} Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=1))\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=True, yticklabels=True)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'{model_name} Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    try:\n",
        "        y_pred_proba = model.predict_proba(X_test)\n",
        "        auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "        print(f\"{model_name} AUC: {auc_score}\")\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba[:, 1], pos_label=1)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.figure()\n",
        "        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{model_name} ROC Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    except AttributeError:\n",
        "        print(f\"{model_name} does not support predict_proba, skipping AUC and ROC curve.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"{model_name} encountered an error with ROC curve calculation: {e}\")\n",
        "\n",
        "evaluate_model(tabnet_model, X_test_preprocessed, y_test, \"TabNet\")\n",
        "evaluate_model(xgb_model, X_test_preprocessed, y_test, \"XGBoost\")\n",
        "evaluate_model(lr_model, X_test_preprocessed, y_test, \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_9bA2TBpDUFG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef, roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_mcc_and_auc(model, X_test, y_test, model_name):\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate Matthews Correlation Coefficient (MCC)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    print(f\"{model_name} Matthews Correlation Coefficient: {mcc}\")\n",
        "\n",
        "    try:\n",
        "        # Calculate AUC for multi-class\n",
        "        y_pred_proba = model.predict_proba(X_test)\n",
        "        auc_score = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "        print(f\"{model_name} AUC: {auc_score}\")\n",
        "\n",
        "        # Plot ROC Curve for multi-class\n",
        "        n_classes = y_pred_proba.shape[1]\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(n_classes):\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "        plt.figure()\n",
        "        colors = ['blue', 'red', 'green']\n",
        "        for i, color in zip(range(n_classes), colors):\n",
        "            plt.plot(fpr[i], tpr[i], color=color, lw=2, label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n",
        "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
        "        plt.xlim([0.0, 1.0])\n",
        "        plt.ylim([0.0, 1.05])\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{model_name} ROC Curve')\n",
        "        plt.legend(loc=\"lower right\")\n",
        "        plt.show()\n",
        "    except AttributeError:\n",
        "        print(f\"{model_name} does not support predict_proba, skipping AUC and ROC curve.\")\n",
        "    except ValueError as e:\n",
        "        print(f\"{model_name} encountered an error with ROC curve calculation: {e}\")\n",
        "\n",
        "calculate_mcc_and_auc(tabnet_model, X_test_preprocessed, y_test, \"TabNet\")\n",
        "calculate_mcc_and_auc(xgb_model, X_test_preprocessed, y_test, \"XGBoost\")\n",
        "calculate_mcc_and_auc(lr_model, X_test_preprocessed, y_test, \"Logistic Regression\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PcuiJY2ftvTv"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall_curve(models, X_test, y_test, model_names):\n",
        "    n_classes = len(np.unique(y_test))\n",
        "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
        "\n",
        "    plt.figure()\n",
        "\n",
        "    # No skill line\n",
        "    no_skill = np.mean(y_test)\n",
        "    plt.plot([0, 1], [no_skill, no_skill], color='gray', linestyle='--', label='No Skill')\n",
        "\n",
        "    for model, model_name in zip(models, model_names):\n",
        "        try:\n",
        "            y_pred_proba = model.predict_proba(X_test)\n",
        "            precision = dict()\n",
        "            recall = dict()\n",
        "            average_precision = dict()\n",
        "            for i in range(n_classes):\n",
        "                precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "                average_precision[i] = average_precision_score(y_test[:, i], y_pred_proba[:, i])\n",
        "\n",
        "            # Averaging precision-recall curves\n",
        "            precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_pred_proba.ravel())\n",
        "            average_precision[\"micro\"] = average_precision_score(y_test, y_pred_proba, average=\"micro\")\n",
        "\n",
        "            plt.plot(recall[\"micro\"], precision[\"micro\"], label=f'{model_name} (AP = {average_precision[\"micro\"]:.2f})')\n",
        "        except AttributeError:\n",
        "            print(f\"{model_name} does not support predict_proba, skipping precision-recall curve.\")\n",
        "        except ValueError as e:\n",
        "            print(f\"{model_name} encountered an error with precision-recall curve calculation: {e}\")\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "models = [lr_model, tabnet_model, xgb_model]\n",
        "model_names = [\"Logistic Regression\", \"TabNet\", \"XGBoost\"]\n",
        "plot_precision_recall_curve(models, X_test_preprocessed, y_test, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U5vIUfHwgXM"
      },
      "source": [
        "it seems like there is an issue with the precision-recall curve plot. The plot does not look correct, as the precision values are not decreasing monotonically with increasing recall, which is the expected behavior of a proper precision-recall curve.\n",
        "\n",
        "Here are a few things you can try to troubleshoot and improve the plot:\n",
        "\n",
        "1. **Check the data and preprocessing**: Ensure that the `X_test`, `y_test`, and the binarized `y_test` are correct and consistent. Double-check the preprocessing steps, especially the label binarization.\n",
        "\n",
        "2. **Verify the `precision_recall_curve` and `average_precision_score` calculations**: Review the implementation of the `plot_precision_recall_curve` function to ensure that the precision-recall curve and average precision score are being calculated correctly for each model.\n",
        "\n",
        "3. **Inspect the model predictions**: Print or inspect the model predictions (`y_pred`) to see if they are as expected. Verify that the models are making reasonable predictions on the test set.\n",
        "\n",
        "4. **Try a simpler example**: Start with a simpler example, such as a binary classification problem, and ensure that the precision-recall curve is plotted correctly. Once you have a working example, gradually increase the complexity to your multi-class problem.\n",
        "\n",
        "5. **Consult the scikit-learn documentation**: The scikit-learn documentation provides detailed information and examples on how to use the `precision_recall_curve` and `average_precision_score` functions. Refer to the documentation to ensure that you are using them correctly.\n",
        "\n",
        "6. **Seek community support**: If you're still unable to resolve the issue, consider posting your question on a relevant online forum or community, such as the scikit-learn GitHub repository or the Machine Learning subreddit. The community may be able to provide more specific guidance based on your code and the problem you're trying to solve.\n",
        "\n",
        "The goal is to identify the root cause of the issue and make the necessary corrections to the code to generate a correct and meaningful precision-recall curve plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TQMvmPo9zIws"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_precision_recall_curve(models, X_test, y_test, model_names):\n",
        "    n_classes = len(np.unique(y_test))\n",
        "    y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
        "\n",
        "    plt.figure(figsize=(10, 7))\n",
        "\n",
        "    # No skill line\n",
        "    no_skill = np.mean(y_test, axis=0)\n",
        "    for i in range(n_classes):\n",
        "        plt.plot([0, 1], [no_skill[i], no_skill[i]], color='gray', linestyle='--', label=f'No Skill Class {i}')\n",
        "\n",
        "    for model, model_name in zip(models, model_names):\n",
        "        try:\n",
        "            y_pred_proba = model.predict_proba(X_test)\n",
        "            precision = dict()\n",
        "            recall = dict()\n",
        "            average_precision = dict()\n",
        "            for i in range(n_classes):\n",
        "                precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "                average_precision[i] = average_precision_score(y_test[:, i], y_pred_proba[:, i])\n",
        "                plt.plot(recall[i], precision[i], label=f'{model_name} Class {i} (AP = {average_precision[i]:.2f})')\n",
        "\n",
        "            # Averaging precision-recall curves\n",
        "            precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_pred_proba.ravel())\n",
        "            average_precision[\"micro\"] = average_precision_score(y_test, y_pred_proba, average=\"micro\")\n",
        "            plt.plot(recall[\"micro\"], precision[\"micro\"], label=f'{model_name} Micro-Average (AP = {average_precision[\"micro\"]:.2f})')\n",
        "        except AttributeError:\n",
        "            print(f\"{model_name} does not support predict_proba, skipping precision-recall curve.\")\n",
        "        except ValueError as e:\n",
        "            print(f\"{model_name} encountered an error with precision-recall curve calculation: {e}\")\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(bbox_to_anchor=(0.75, 1.15), ncol=2)\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "models = [lr_model, tabnet_model, xgb_model]\n",
        "model_names = [\"Logistic Regression\", \"TabNet\", \"XGBoost\"]\n",
        "plot_precision_recall_curve(models, X_test_preprocessed, y_test, model_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbCM4YqD1OTA"
      },
      "source": [
        "\n",
        "### Line-by-Line Explanation:\n",
        "\n",
        "1. **Imports**:\n",
        "   ```python\n",
        "   from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "   from sklearn.preprocessing import label_binarize\n",
        "   import numpy as np\n",
        "   import matplotlib.pyplot as plt\n",
        "   ```\n",
        "   - Import necessary functions from `sklearn.metrics` and `sklearn.preprocessing`.\n",
        "   - Import `numpy` for numerical operations.\n",
        "   - Import `matplotlib.pyplot` for plotting.\n",
        "\n",
        "2. **Function Definition**:\n",
        "   ```python\n",
        "   def plot_precision_recall_curve(models, X_test, y_test, model_names):\n",
        "   ```\n",
        "   - Define the function `plot_precision_recall_curve` that takes a list of models, test data (`X_test`, `y_test`), and model names as inputs.\n",
        "\n",
        "3. **Determine Number of Classes**:\n",
        "   ```python\n",
        "   n_classes = len(np.unique(y_test))\n",
        "   ```\n",
        "   - Determine the number of unique classes in `y_test`.\n",
        "\n",
        "4. **Binarize Target Labels**:\n",
        "   ```python\n",
        "   y_test = label_binarize(y_test, classes=np.unique(y_test))\n",
        "   ```\n",
        "   - Convert the target labels `y_test` into a binary format suitable for multi-class precision-recall curve calculation.\n",
        "\n",
        "5. **Create Plot Figure**:\n",
        "   ```python\n",
        "   plt.figure(figsize=(10, 7))\n",
        "   ```\n",
        "   - Create a plot figure with a specified size of 10x7 inches.\n",
        "\n",
        "6. **Plot No Skill Line**:\n",
        "   ```python\n",
        "   no_skill = np.mean(y_test, axis=0)\n",
        "   for i in range(n_classes):\n",
        "       plt.plot([0, 1], [no_skill[i], no_skill[i]], color='gray', linestyle='--', label=f'No Skill Class {i}')\n",
        "   ```\n",
        "   - Calculate the no-skill line as the mean of the binary target labels for each class.\n",
        "   - Plot the no-skill line for each class.\n",
        "\n",
        "7. **Loop Through Models**:\n",
        "   ```python\n",
        "   for model, model_name in zip(models, model_names):\n",
        "   ```\n",
        "   - Loop through each model and corresponding model name.\n",
        "\n",
        "8. **Try Block for Precision-Recall Curve Calculation**:\n",
        "   ```python\n",
        "   try:\n",
        "       y_pred_proba = model.predict_proba(X_test)\n",
        "       precision = dict()\n",
        "       recall = dict()\n",
        "       average_precision = dict()\n",
        "       for i in range(n_classes):\n",
        "           precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred_proba[:, i])\n",
        "           average_precision[i] = average_precision_score(y_test[:, i], y_pred_proba[:, i])\n",
        "           plt.plot(recall[i], precision[i], label=f'{model_name} Class {i} (AP = {average_precision[i]:.2f})')\n",
        "   ```\n",
        "   - Try to get the predicted probabilities for each class using `model.predict_proba(X_test)`.\n",
        "   - Initialize dictionaries to store precision, recall, and average precision for each class.\n",
        "   - Calculate precision-recall curves and average precision scores for each class and plot them.\n",
        "\n",
        "9. **Micro-Average Precision-Recall Curve**:\n",
        "   ```python\n",
        "       precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(y_test.ravel(), y_pred_proba.ravel())\n",
        "       average_precision[\"micro\"] = average_precision_score(y_test, y_pred_proba, average=\"micro\")\n",
        "       plt.plot(recall[\"micro\"], precision[\"micro\"], label=f'{model_name} Micro-Average (AP = {average_precision[\"micro\"]:.2f})')\n",
        "   ```\n",
        "   - Calculate the micro-average precision-recall curve and average precision score by aggregating all classes.\n",
        "   - Plot the micro-average precision-recall curve.\n",
        "\n",
        "10. **Exception Handling**:\n",
        "    ```python\n",
        "   except AttributeError:\n",
        "       print(f\"{model_name} does not support predict_proba, skipping precision-recall curve.\")\n",
        "   except ValueError as e:\n",
        "       print(f\"{model_name} encountered an error with precision-recall curve calculation: {e}\")\n",
        "    ```\n",
        "    - Handle exceptions if the model does not support `predict_proba` or if there is an error in the precision-recall curve calculation.\n",
        "\n",
        "11. **Plot Labels and Title**:\n",
        "    ```python\n",
        "   plt.xlabel('Recall')\n",
        "   plt.ylabel('Precision')\n",
        "   plt.title('Precision-Recall Curve')\n",
        "   plt.legend(loc=\"lower left\")\n",
        "   plt.show()\n",
        "    ```\n",
        "    - Set the labels for the x-axis and y-axis.\n",
        "    - Set the title of the plot.\n",
        "    - Add a legend to the plot.\n",
        "    - Display the plot.\n",
        "\n",
        "This function plots the precision-recall curves for each class and the micro-average curve for each model, along with a no-skill line for comparison. The curves are plotted in one chart for easy comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40dsVT5e2ARM"
      },
      "source": [
        "In the context of the precision-recall curve, \"AP\" stands for \"Average Precision.\" Average Precision is a single-value metric that summarizes the precision-recall curve by calculating the average of the precision values obtained for each threshold.\n",
        "\n",
        "### Definition and Calculation:\n",
        "- **Precision**: The ratio of true positive predictions to the total number of positive predictions (true positives + false positives).\n",
        "- **Recall**: The ratio of true positive predictions to the total number of actual positives (true positives + false negatives).\n",
        "\n",
        "The precision-recall curve plots precision against recall for different probability thresholds. The Average Precision (AP) is calculated as the area under this curve. It provides a measure of the model's performance across all thresholds, with higher values indicating better performance.\n",
        "\n",
        "### Interpretation:\n",
        "- **AP = 1**: Perfect precision and recall.\n",
        "- **AP = 0**: Poor precision and recall.\n",
        "\n",
        "In the context of your code, the `average_precision_score` function from `sklearn.metrics` is used to compute the AP for each class and the micro-average AP for all classes combined. The micro-average AP aggregates the contributions of all classes to compute a single AP value.\n",
        "\n",
        "### Example in Code:\n",
        "```python\n",
        "average_precision[i] = average_precision_score(y_test[:, i], y_pred_proba[:, i])\n",
        "plt.plot(recall[i], precision[i], label=f'{model_name} Class {i} (AP = {average_precision[i]:.2f})')\n",
        "```\n",
        "- For each class `i`, the `average_precision_score` function computes the AP based on the true labels `y_test[:, i]` and the predicted probabilities `y_pred_proba[:, i]`.\n",
        "- The AP value is then included in the label for the precision-recall curve plot.\n",
        "\n",
        "In summary, AP is a useful metric for evaluating the performance of classification models, especially in imbalanced datasets where the focus is on the positive class. It provides a concise summary of the model's ability to correctly identify positive instances across different probability thresholds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlYwI-bm4L-a"
      },
      "source": [
        " showcasing the performance of multiple models (Logistic Regression, TabNet, XGBoost) across various classes.\n",
        "\n",
        "Here are a few observations:\n",
        "\n",
        "1. **Class-specific AP (Average Precision)**:\n",
        "    - Logistic Regression shows high AP for Class 0 and Class 1 but significantly lower for Class 2 and Class 3.\n",
        "    - XGBoost shows high AP for Class 0, Class 1, and Class 3 but lower for Class 2.\n",
        "    - TabNet shows a perfect AP for Class 0 and lower values for the other classes.\n",
        "\n",
        "2. **Micro-Average AP**:\n",
        "    - Logistic Regression, XGBoost, and TabNet all have a perfect (1.00) micro-average AP, indicating strong overall performance when all classes are considered together.\n",
        "\n",
        "3. **PR Curves**:\n",
        "    - The PR curve for Logistic Regression Class 0 is very close to the top-right corner, indicating high precision and recall.\n",
        "    - The PR curve for Logistic Regression Class 2 and Class 3 shows lower performance with considerable drops.\n",
        "    - XGBoost PR curves for Class 0 and Class 1 are also near the top-right, indicating strong performance.\n",
        "    - TabNet shows similar behavior for Class 0 but significantly poorer for Class 2 and Class 3.\n",
        "\n",
        "The graph indicates that while certain models perform exceptionally well for specific classes, they might struggle with others. This could be due to class imbalance or the inherent difficulty in distinguishing those classes.\n",
        "\n",
        "To ensure correctness:\n",
        "- Verify that the labels and predictions fed into the calculation of precision-recall metrics are accurate.\n",
        "- Confirm that the AP values and PR curves align with what you would expect from the model performance on your dataset.\n",
        "\n",
        "If everything aligns, then this output should be considered correct. If there are discrepancies, you may need to revisit the data preprocessing or the metric calculation steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ScLSUvcc1fTh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNUm+G4+/rmL72IphbXwEg7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}