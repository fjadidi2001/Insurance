{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9RTqKPQJZnW1DmlDsf13Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/Nov21Insur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNZAFKgdEKYS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "# 1. Handle missing values\n",
        "# For numerical columns, we'll use median imputation\n",
        "# For categorical columns, we'll use mode imputation\n",
        "numeric_features = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# 2. Feature engineering\n",
        "df['Age_Group'] = pd.cut(df['Insured.age'], bins=[0, 25, 35, 45, 55, 65, 100], labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
        "df['Car_Age_Group'] = pd.cut(df['Car.age'], bins=[-1, 0, 3, 5, 10, 20, 100], labels=['New', '1-3', '4-5', '6-10', '11-20', '20+'])\n",
        "\n",
        "# 3. Create preprocessing pipelines\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# 4. Combine preprocessing steps\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# 5. Fit and transform the data\n",
        "X = df.drop(['NB_Claim', 'AMT_Claim'], axis=1)  # Features\n",
        "y_nb = df['NB_Claim']  # Target for classification\n",
        "y_amt = df['AMT_Claim']  # Target for regression\n",
        "\n",
        "X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "# 6. Convert to DataFrame for better interpretability\n",
        "feature_names = (numeric_features.tolist() +\n",
        "                 preprocessor.named_transformers_['cat'].named_steps['onehot'].get_feature_names(categorical_features).tolist())\n",
        "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=feature_names)\n",
        "\n",
        "# 7. Print info about the preprocessed data\n",
        "print(\"Preprocessed data shape:\", X_preprocessed_df.shape)\n",
        "print(\"\\nFirst few rows of preprocessed data:\")\n",
        "print(X_preprocessed_df.head())\n",
        "\n",
        "# 8. Handle target variables\n",
        "# For NB_Claim (classification)\n",
        "y_nb = (y_nb > 0).astype(int)  # Convert to binary (0 for no claim, 1 for claim)\n",
        "\n",
        "# For AMT_Claim (regression)\n",
        "# We'll keep it as is, but you might want to consider log transformation if the distribution is skewed\n",
        "\n",
        "print(\"\\nUnique values in NB_Claim (after binarization):\", y_nb.unique())\n",
        "print(\"AMT_Claim statistics:\")\n",
        "print(y_amt.describe())"
      ]
    }
  ]
}