{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjadidi2001/Insurance/blob/main/ClaimYN_Prediction_TabNet_Bad_Result.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbHLksJUkNON",
        "outputId": "7f77a30f-2067-42f3-8cb3-e2945a8e9876"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "(100000, 52)\n",
            "   Duration  Insured.age Insured.sex  Car.age  Marital  Car.use  Credit.score  \\\n",
            "0       366           45        Male       -1  Married  Commute         609.0   \n",
            "1       182           44      Female        3  Married  Commute         575.0   \n",
            "2       184           48      Female        6  Married  Commute         847.0   \n",
            "3       183           71        Male        6  Married  Private         842.0   \n",
            "4       183           84        Male       10  Married  Private         856.0   \n",
            "\n",
            "  Region  Annual.miles.drive  Years.noclaims  ...  Left.turn.intensity10  \\\n",
            "0  Urban             6213.71              25  ...                    1.0   \n",
            "1  Urban            12427.42              20  ...                   58.0   \n",
            "2  Urban            12427.42              14  ...                    0.0   \n",
            "3  Urban             6213.71              43  ...                    0.0   \n",
            "4  Urban             6213.71              65  ...                    2.0   \n",
            "\n",
            "   Left.turn.intensity11  Left.turn.intensity12  Right.turn.intensity08  \\\n",
            "0                    0.0                    0.0                     3.0   \n",
            "1                   24.0                   11.0                  1099.0   \n",
            "2                    0.0                    0.0                     0.0   \n",
            "3                    0.0                    0.0                     0.0   \n",
            "4                    0.0                    0.0                   325.0   \n",
            "\n",
            "   Right.turn.intensity09  Right.turn.intensity10  Right.turn.intensity11  \\\n",
            "0                     1.0                     0.0                     0.0   \n",
            "1                   615.0                   219.0                   101.0   \n",
            "2                     0.0                     0.0                     0.0   \n",
            "3                     0.0                     0.0                     0.0   \n",
            "4                   111.0                    18.0                     4.0   \n",
            "\n",
            "   Right.turn.intensity12  NB_Claim    AMT_Claim  \n",
            "0                     0.0         1  5100.171753  \n",
            "1                    40.0         1   883.554840  \n",
            "2                     0.0         0     0.000000  \n",
            "3                     0.0         0     0.000000  \n",
            "4                     2.0         0     0.000000  \n",
            "\n",
            "[5 rows x 52 columns]\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "\n",
        "# Import pandas (assuming you want to use it to read the CSV)\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.shape)  # Should print (100000, 52)\n",
        "print(df.head()) # To check the first few rows\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# pre-process"
      ],
      "metadata": {
        "id": "mphEoc3iOfOr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UNDVweYpQFO",
        "outputId": "4d3f741e-9eef-4a2e-f983-e2a6428564f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "(100000, 52)\n",
            "Index(['Duration', 'Insured.age', 'Insured.sex', 'Car.age', 'Marital',\n",
            "       'Car.use', 'Credit.score', 'Region', 'Annual.miles.drive',\n",
            "       'Years.noclaims', 'Territory', 'Annual.pct.driven',\n",
            "       'Total.miles.driven', 'Pct.drive.mon', 'Pct.drive.tue', 'Pct.drive.wed',\n",
            "       'Pct.drive.thr', 'Pct.drive.fri', 'Pct.drive.sat', 'Pct.drive.sun',\n",
            "       'Pct.drive.2hrs', 'Pct.drive.3hrs', 'Pct.drive.4hrs', 'Pct.drive.wkday',\n",
            "       'Pct.drive.wkend', 'Pct.drive.rush am', 'Pct.drive.rush pm',\n",
            "       'Avgdays.week', 'Accel.06miles', 'Accel.08miles', 'Accel.09miles',\n",
            "       'Accel.11miles', 'Accel.12miles', 'Accel.14miles', 'Brake.06miles',\n",
            "       'Brake.08miles', 'Brake.09miles', 'Brake.11miles', 'Brake.12miles',\n",
            "       'Brake.14miles', 'Left.turn.intensity08', 'Left.turn.intensity09',\n",
            "       'Left.turn.intensity10', 'Left.turn.intensity11',\n",
            "       'Left.turn.intensity12', 'Right.turn.intensity08',\n",
            "       'Right.turn.intensity09', 'Right.turn.intensity10',\n",
            "       'Right.turn.intensity11', 'Right.turn.intensity12', 'NB_Claim',\n",
            "       'AMT_Claim'],\n",
            "      dtype='object')\n",
            "(100000, 113)\n",
            "(100000, 113)\n",
            "(100000, 114)\n",
            "(64000, 113) (16000, 113) (20000, 113)\n",
            "(64000, 113) (16000, 113) (20000, 113)\n",
            "(64000, 114) (16000, 114) (20000, 114)\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/telematics_syn.csv'\n",
        "\n",
        "# Import pandas (assuming you want to use it to read the CSV)\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.shape)  # Should print (100000, 52)\n",
        "print(df.columns) # To check the column names\n",
        "\n",
        "# Encoding categorical columns using one-hot encoding\n",
        "categorical_cols = ['Marital', 'Insured.sex', 'Car.use', 'Region', 'Territory']\n",
        "df_level1 = pd.get_dummies(df, columns=categorical_cols)\n",
        "\n",
        "# Creating the ClaimYN variable\n",
        "df_level1['ClaimYN'] = df_level1['NB_Claim'].apply(lambda x: 1 if x > 0 else 0)\n",
        "\n",
        "# Save the preprocessed data to a new file\n",
        "preprocessed_file_path_level1 = '/content/drive/My Drive/pre_telematics_syn_level1.csv'\n",
        "df_level1.to_csv(preprocessed_file_path_level1, index=False)\n",
        "print(df_level1.shape)  # Should print (100000, number of columns after encoding)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "# Copy the DataFrame from Level 1\n",
        "df_level2 = df_level1.copy()\n",
        "\n",
        "# Feature columns (excluding response columns)\n",
        "feature_cols = [col for col in df_level2.columns if col not in ['NB_Claim', 'AMT_Claim', 'ClaimYN']]\n",
        "\n",
        "# Applying StandardScaler and MinMaxScaler\n",
        "scaler = StandardScaler()\n",
        "minmax_scaler = MinMaxScaler()\n",
        "\n",
        "df_level2[feature_cols] = scaler.fit_transform(df_level2[feature_cols])\n",
        "df_level2[feature_cols] = minmax_scaler.fit_transform(df_level2[feature_cols])\n",
        "\n",
        "# Save the preprocessed data to a new file\n",
        "preprocessed_file_path_level2 = '/content/drive/My Drive/pre_telematics_syn_level2.csv'\n",
        "df_level2.to_csv(preprocessed_file_path_level2, index=False)\n",
        "print(df_level2.shape)  # Should print (100000, number of columns after scaling and normalization)\n",
        "\n",
        "# Copy the DataFrame from Level 2\n",
        "df_level3 = df_level2.copy()\n",
        "\n",
        "# Use provided columns for feature engineering\n",
        "feature_cols_to_sum = ['Annual.pct.driven', 'Annual.miles.drive', 'Pct.drive.rush am']\n",
        "\n",
        "# Advanced feature engineering steps\n",
        "df_level3['DrivingIntensity'] = df_level3[feature_cols_to_sum].sum(axis=1)\n",
        "\n",
        "# Save the fully preprocessed data to a new file\n",
        "preprocessed_file_path_level3 = '/content/drive/My Drive/pre_telematics_syn_level3.csv'\n",
        "df_level3.to_csv(preprocessed_file_path_level3, index=False)\n",
        "print(df_level3.shape)  # Should print (100000, number of columns after feature engineering)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the response and feature columns\n",
        "response_cols = ['NB_Claim', 'AMT_Claim', 'ClaimYN']\n",
        "\n",
        "\n",
        "def split_data(df):\n",
        "    feature_cols = [col for col in df.columns if col not in response_cols]\n",
        "\n",
        "    # Split into 80% train and 20% test\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Further split train into train and validation\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    return train_df, val_df, test_df, feature_cols\n",
        "\n",
        "train_df_level1, val_df_level1, test_df_level1, feature_cols_level1 = split_data(df_level1)\n",
        "train_df_level2, val_df_level2, test_df_level2, feature_cols_level2 = split_data(df_level2)\n",
        "train_df_level3, val_df_level3, test_df_level3, feature_cols_level3 = split_data(df_level3)\n",
        "\n",
        "print(train_df_level1.shape, val_df_level1.shape, test_df_level1.shape)\n",
        "print(train_df_level2.shape, val_df_level2.shape, test_df_level2.shape)\n",
        "print(train_df_level3.shape, val_df_level3.shape, test_df_level3.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the library\n",
        "!pip install pytorch-tabnet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0PTXTvJRoa2",
        "outputId": "1b1e17b8-99ac-43c9-ef8c-9b5771538cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-tabnet\n",
            "  Downloading pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.5/44.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.25.2)\n",
            "Requirement already satisfied: scikit_learn>0.21 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.2.2)\n",
            "Requirement already satisfied: scipy>1.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (1.11.4)\n",
            "Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (2.3.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.10/dist-packages (from pytorch-tabnet) (4.66.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn>0.21->pytorch-tabnet) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.3->pytorch-tabnet)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3->pytorch-tabnet) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3->pytorch-tabnet)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3->pytorch-tabnet) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3->pytorch-tabnet) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, pytorch-tabnet\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 pytorch-tabnet-4.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def train_tabnet(train_df, val_df, feature_cols, response_col='ClaimYN', epochs=50):\n",
        "    # Convert boolean columns to integers\n",
        "    train_df = train_df.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
        "    val_df = val_df.applymap(lambda x: int(x) if isinstance(x, bool) else x)\n",
        "\n",
        "    # Ensure all data is numeric and convert to numpy arrays\n",
        "    X_train = train_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "    y_train = train_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "    X_val = val_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "    y_val = val_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
        "\n",
        "    # Print shapes of the arrays\n",
        "    print(\"Shapes of the datasets:\")\n",
        "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "\n",
        "    # Verify the types of the arrays\n",
        "    print(\"Data types:\")\n",
        "    print(f\"X_train type: {type(X_train)}, y_train type: {type(y_train)}\")\n",
        "    print(f\"X_val type: {type(X_val)}, y_val type: {type(y_val)}\")\n",
        "\n",
        "    # Print sample data\n",
        "    print(\"Sample data:\")\n",
        "    print(f\"X_train sample: {X_train[:5]}\")\n",
        "    print(f\"y_train sample: {y_train[:5]}\")\n",
        "    print(f\"X_val sample: {X_val[:5]}\")\n",
        "    print(f\"y_val sample: {y_val[:5]}\")\n",
        "\n",
        "    # Assert that the arrays contain only numeric data\n",
        "    assert np.issubdtype(X_train.dtype, np.number), \"X_train contains non-numeric data\"\n",
        "    assert np.issubdtype(y_train.dtype, np.number), \"y_train contains non-numeric data\"\n",
        "    assert np.issubdtype(X_val.dtype, np.number), \"X_val contains non-numeric data\"\n",
        "    assert np.issubdtype(y_val.dtype, np.number), \"y_val contains non-numeric data\"\n",
        "\n",
        "    clf = TabNetClassifier()\n",
        "\n",
        "    clf.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "        eval_name=['train', 'val'],\n",
        "        eval_metric=['accuracy'],\n",
        "        max_epochs=10,\n",
        "        patience=100,\n",
        "        batch_size=1024,\n",
        "        virtual_batch_size=128,\n",
        "        num_workers=0,\n",
        "        weights=1,\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    return clf\n",
        "\n",
        "# Assuming train_df_level1, val_df_level1, feature_cols_level1, etc., are already defined\n",
        "clf_level1 = train_tabnet(train_df_level1, val_df_level1, feature_cols_level1)\n",
        "clf_level2 = train_tabnet(train_df_level2, val_df_level2, feature_cols_level2)\n",
        "clf_level3 = train_tabnet(train_df_level3, val_df_level3, feature_cols_level3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "440_zDLWRjhA",
        "outputId": "0d8f8651-ca88-4bb5-a66c-888bac384c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the datasets:\n",
            "X_train: (64000, 110), y_train: (64000,)\n",
            "X_val: (16000, 110), y_val: (16000,)\n",
            "Data types:\n",
            "X_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
            "X_val type: <class 'numpy.ndarray'>, y_val type: <class 'numpy.ndarray'>\n",
            "Sample data:\n",
            "X_train sample: [[3.66000000e+02 6.40000000e+01 2.00000000e+00 6.95000000e+02\n",
            "  1.24274200e+04 8.00000000e+00 4.82191781e-01 3.25681330e+03\n",
            "  1.51102077e-01 1.67734049e-01 1.68922553e-01 1.75627304e-01\n",
            "  1.71069433e-01 1.31185908e-01 3.43586750e-02 2.19352600e-03\n",
            "  0.00000000e+00 0.00000000e+00 8.38064741e-01 1.61935259e-01\n",
            "  2.23870517e-01 2.12878450e-02 6.31935259e+00 1.10000000e+01\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 3.40000000e+01 2.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.10000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  5.80000000e+01 1.70000000e+01 2.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [3.65000000e+02 4.00000000e+01 2.00000000e+00 8.63000000e+02\n",
            "  1.24274200e+04 2.20000000e+01 2.93150685e-01 1.52216078e+03\n",
            "  1.61433545e-01 2.23668297e-01 1.94913279e-01 1.36081388e-01\n",
            "  1.27065831e-01 6.02825760e-02 9.65550840e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.47309517e-01 1.52690483e-01\n",
            "  3.31928550e-01 2.29402115e-01 5.83722206e+00 1.07000000e+02\n",
            "  4.00000000e+00 1.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 3.18000000e+02 3.00000000e+01 1.10000000e+01\n",
            "  4.00000000e+00 1.00000000e+00 1.00000000e+00 7.60000000e+01\n",
            "  3.70000000e+01 2.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.22000000e+02 4.30000000e+01 1.00000000e+01 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [3.66000000e+02 5.40000000e+01 5.00000000e+00 7.87000000e+02\n",
            "  6.21371000e+03 2.40000000e+01 9.04109589e-01 7.10045454e+03\n",
            "  1.56547235e-01 1.49720028e-01 1.37540583e-01 1.24756803e-01\n",
            "  1.45322931e-01 1.38513004e-01 1.47599416e-01 2.37388400e-03\n",
            "  1.24925600e-03 0.00000000e+00 7.16261158e-01 2.83738842e-01\n",
            "  7.37090880e-02 1.50000000e-01 6.21246281e+00 1.86000000e+02\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 5.80000000e+01 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.07000000e+02\n",
            "  2.04000000e+02 4.30000000e+01 1.20000000e+01 3.00000000e+00\n",
            "  5.23000000e+02 2.36000000e+02 4.70000000e+01 1.50000000e+01\n",
            "  7.00000000e+00 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [3.65000000e+02 4.50000000e+01 1.10000000e+01 5.62000000e+02\n",
            "  6.21371000e+03 1.20000000e+01 6.00000000e-01 1.00649719e+03\n",
            "  1.33933591e-01 1.06856154e-01 7.79606660e-02 2.46001783e-01\n",
            "  2.28300366e-01 1.79100010e-01 2.78474310e-02 2.77996500e-03\n",
            "  0.00000000e+00 0.00000000e+00 7.96332754e-01 2.03667246e-01\n",
            "  1.46689850e-02 2.37531363e-01 2.81538318e+00 5.00000000e+01\n",
            "  1.40000000e+01 6.00000000e+00 4.00000000e+00 2.00000000e+00\n",
            "  1.00000000e+00 7.30000000e+01 1.60000000e+01 7.00000000e+00\n",
            "  4.00000000e+00 4.00000000e+00 2.00000000e+00 4.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.10000000e+01 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [3.65000000e+02 3.90000000e+01 9.00000000e+00 8.68000000e+02\n",
            "  5.59233900e+03 2.10000000e+01 6.65753425e-01 2.82347975e+03\n",
            "  1.21512271e-01 1.53928355e-01 1.73023920e-01 1.65669613e-01\n",
            "  1.44707674e-01 1.33125939e-01 1.08032229e-01 7.58538400e-03\n",
            "  1.58028800e-03 0.00000000e+00 7.60000000e-01 2.40000000e-01\n",
            "  7.15548080e-02 1.94197116e-01 5.38861540e+00 1.40000000e+01\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 5.90000000e+01 4.00000000e+00 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.00000000e+00 2.13000000e+02\n",
            "  4.90000000e+01 5.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  5.82000000e+02 1.65000000e+02 1.90000000e+01 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "y_train sample: [0 0 0 0 0]\n",
            "X_val sample: [[ 3.65000000e+02  3.40000000e+01  4.00000000e+00  7.89000000e+02\n",
            "   7.45645200e+03  1.30000000e+01  5.23287671e-01  1.14759416e+04\n",
            "   1.32058537e-01  1.43728185e-01  1.52066370e-01  1.30492342e-01\n",
            "   1.65778693e-01  1.45362760e-01  1.30513113e-01  1.72065600e-03\n",
            "   0.00000000e+00  0.00000000e+00  7.27206559e-01  2.72793441e-01\n",
            "   1.79272144e-01  1.72511495e-01  6.67206559e+00  3.00000000e+01\n",
            "   2.00000000e+00  1.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  3.00000000e+01  2.00000000e+00  1.00000000e+00\n",
            "   1.00000000e+00  1.00000000e+00  0.00000000e+00  3.60000000e+01\n",
            "   1.40000000e+01  3.00000000e+00  1.00000000e+00  1.00000000e+00\n",
            "   9.60000000e+01  2.90000000e+01  4.00000000e+00  1.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  1.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 2.12000000e+02  5.40000000e+01  1.40000000e+01  8.49000000e+02\n",
            "   6.21371000e+03  3.80000000e+01  2.35616438e-01  1.31122997e+02\n",
            "   1.00086050e-01  2.39464479e-01  1.23006605e-01  2.80466681e-01\n",
            "   2.32915750e-01  2.40604360e-02 -8.34000000e-10  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  9.73348457e-01  2.66515430e-02\n",
            "   3.26764972e-01  2.43393829e-01  1.40000000e+00  7.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  5.20000000e+01  4.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  8.49000000e+02\n",
            "   3.19000000e+02  3.50000000e+01  5.00000000e+00  0.00000000e+00\n",
            "   1.17700000e+03  6.52000000e+02  2.81000000e+02  9.90000000e+01\n",
            "   3.50000000e+01  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 1.83000000e+02  7.30000000e+01  2.00000000e+00  8.49000000e+02\n",
            "   1.11846780e+04  5.70000000e+01  4.98630137e-01  7.71924570e+03\n",
            "   1.39007394e-01  1.19617491e-01  1.47182711e-01  1.60819588e-01\n",
            "   1.37971831e-01  1.79996341e-01  1.15404644e-01  1.50554960e-02\n",
            "   0.00000000e+00  0.00000000e+00  7.05833512e-01  2.94166488e-01\n",
            "   1.00000000e-02  1.45833512e-01  4.73889008e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  2.00000000e+01  2.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  2.50000000e+01\n",
            "   1.00000000e+01  2.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   1.60000000e+01  1.00000000e+01  2.00000000e+00  1.00000000e+00\n",
            "   1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 3.65000000e+02  7.40000000e+01  1.10000000e+01  5.98000000e+02\n",
            "   6.21371000e+03  5.80000000e+01  1.64383562e-01  5.15040281e+02\n",
            "   2.15970544e-01  1.13488507e-01  1.23033338e-01  1.71054756e-01\n",
            "   2.16495747e-01  1.00312843e-01  5.96442660e-02  3.90336300e-03\n",
            "   0.00000000e+00  0.00000000e+00  8.35892912e-01  1.64107088e-01\n",
            "   3.90336260e-02  5.85504400e-02  5.68886703e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  1.40000000e+01  2.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  8.30000000e+01\n",
            "   4.40000000e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.21000000e+02  4.50000000e+01  2.20000000e+01  7.00000000e+00\n",
            "   7.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   1.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [ 3.65000000e+02  4.40000000e+01  7.00000000e+00  8.83000000e+02\n",
            "   1.24274200e+04  2.00000000e+00  1.00000000e+00  1.08518681e+04\n",
            "   1.62783879e-01  1.40086460e-01  1.39289343e-01  1.37837726e-01\n",
            "   1.52943024e-01  1.56192820e-01  1.10866747e-01  5.92557200e-03\n",
            "   2.09255700e-03  1.00000000e-03  7.31851144e-01  2.68148856e-01\n",
            "   6.55534320e-02  1.45372140e-01  6.23520996e+00  5.70000000e+01\n",
            "   9.00000000e+00  5.00000000e+00  2.00000000e+00  1.00000000e+00\n",
            "   1.00000000e+00  8.30000000e+01  6.00000000e+00  2.00000000e+00\n",
            "   1.00000000e+00  1.00000000e+00  1.00000000e+00  2.31000000e+02\n",
            "   8.30000000e+01  1.70000000e+01  6.00000000e+00  1.00000000e+00\n",
            "   2.81000000e+02  1.03000000e+02  1.40000000e+01  4.00000000e+00\n",
            "   1.00000000e+00  1.00000000e+00  0.00000000e+00  1.00000000e+00\n",
            "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  1.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]]\n",
            "y_val sample: [0 0 0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.6325  | train_accuracy: 0.78583 | val_accuracy: 0.78012 |  0:00:03s\n",
            "epoch 1  | loss: 0.57592 | train_accuracy: 0.73548 | val_accuracy: 0.73225 |  0:00:09s\n",
            "epoch 2  | loss: 0.56626 | train_accuracy: 0.6738  | val_accuracy: 0.67338 |  0:00:13s\n",
            "epoch 3  | loss: 0.56547 | train_accuracy: 0.60909 | val_accuracy: 0.61262 |  0:00:17s\n",
            "epoch 4  | loss: 0.55606 | train_accuracy: 0.67339 | val_accuracy: 0.673   |  0:00:22s\n",
            "epoch 5  | loss: 0.54594 | train_accuracy: 0.65309 | val_accuracy: 0.6515  |  0:00:27s\n",
            "epoch 6  | loss: 0.5377  | train_accuracy: 0.70662 | val_accuracy: 0.7035  |  0:00:32s\n",
            "epoch 7  | loss: 0.52202 | train_accuracy: 0.65916 | val_accuracy: 0.65575 |  0:00:36s\n",
            "epoch 8  | loss: 0.51118 | train_accuracy: 0.70066 | val_accuracy: 0.69644 |  0:00:40s\n",
            "epoch 9  | loss: 0.50141 | train_accuracy: 0.69269 | val_accuracy: 0.68444 |  0:00:45s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 0 and best_val_accuracy = 0.78012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the datasets:\n",
            "X_train: (64000, 110), y_train: (64000,)\n",
            "X_val: (16000, 110), y_val: (16000,)\n",
            "Data types:\n",
            "X_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
            "X_val type: <class 'numpy.ndarray'>, y_val type: <class 'numpy.ndarray'>\n",
            "Sample data:\n",
            "X_train sample: [[1.00000000e+00 5.51724138e-01 1.81818182e-01 5.71129707e-01\n",
            "  2.19058050e-01 1.01265823e-01 4.80769231e-01 6.88778598e-02\n",
            "  1.51378842e-01 1.67734049e-01 1.68922553e-01 1.75996906e-01\n",
            "  1.71306277e-01 1.38587015e-01 3.52012753e-02 4.81308685e-03\n",
            "  0.00000000e+00 0.00000000e+00 8.38064741e-01 1.61935259e-01\n",
            "  2.26579899e-01 2.14340683e-02 8.99891509e-01 1.77133655e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 5.47504026e-02 3.22061192e-03 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38410046e-05\n",
            "  1.25837448e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  6.89483007e-05 2.02090567e-05 2.37755587e-06 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.97050147e-01 2.75862069e-01 1.81818182e-01 9.22594142e-01\n",
            "  2.19058050e-01 2.78481013e-01 2.91208791e-01 3.21908784e-02\n",
            "  1.61729234e-01 2.23668297e-01 1.94913279e-01 1.36367767e-01\n",
            "  1.27241753e-01 6.36835341e-02 9.89229641e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.47309517e-01 1.52690483e-01\n",
            "  3.35945700e-01 2.30977847e-01 8.28980552e-01 1.72302738e-01\n",
            "  6.44122383e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
            "  0.00000000e+00 5.12077295e-01 4.83091787e-02 1.77133655e-02\n",
            "  6.44122383e-03 1.61030596e-03 1.61030596e-03 9.56287591e-05\n",
            "  4.65598558e-05 2.51768675e-06 0.00000000e+00 0.00000000e+00\n",
            "  1.45029184e-04 5.11170259e-05 1.18877794e-05 1.18881185e-06\n",
            "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 4.36781609e-01 3.18181818e-01 7.63598326e-01\n",
            "  1.09529025e-01 3.03797468e-01 9.03846154e-01 1.50168835e-01\n",
            "  1.56833974e-01 1.49720028e-01 1.37540583e-01 1.25019350e-01\n",
            "  1.45524129e-01 1.46327483e-01 1.51219087e-01 5.20883266e-03\n",
            "  3.85773525e-03 0.00000000e+00 7.16261158e-01 2.83738842e-01\n",
            "  7.46011488e-02 1.51030330e-01 8.84170338e-01 2.99516908e-01\n",
            "  1.61030596e-03 1.61030596e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 9.33977456e-02 1.61030596e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.37944485e-04\n",
            "  2.56708394e-04 5.41302651e-05 1.51147588e-05 3.78229131e-06\n",
            "  6.21723470e-04 2.80549258e-04 5.58725630e-05 1.78321778e-05\n",
            "  8.32199956e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.97050147e-01 3.33333333e-01 5.90909091e-01 2.92887029e-01\n",
            "  1.09529025e-01 1.51898734e-01 5.98901099e-01 2.12848667e-02\n",
            "  1.34178910e-01 1.06856154e-01 7.79606660e-02 2.46519486e-01\n",
            "  2.28616446e-01 1.89204283e-01 2.85303522e-02 6.09986523e-03\n",
            "  0.00000000e+00 0.00000000e+00 7.96332754e-01 2.03667246e-01\n",
            "  1.48465157e-02 2.39162934e-01 3.84533512e-01 8.05152979e-02\n",
            "  2.25442834e-02 9.66183575e-03 6.44122383e-03 3.22061192e-03\n",
            "  1.61030596e-03 1.17552335e-01 2.57648953e-02 1.12721417e-02\n",
            "  6.44122383e-03 6.44122383e-03 3.22061192e-03 5.03309258e-06\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.30764018e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.97050147e-01 2.64367816e-01 5.00000000e-01 9.33054393e-01\n",
            "  9.85761227e-02 2.65822785e-01 6.64835165e-01 5.97130850e-02\n",
            "  1.21734838e-01 1.53928355e-01 1.73023920e-01 1.66018259e-01\n",
            "  1.44908021e-01 1.40636496e-01 1.10681570e-01 1.66440297e-02\n",
            "  4.87997074e-03 0.00000000e+00 7.60000000e-01 2.40000000e-01\n",
            "  7.24207967e-02 1.95531030e-01 7.63000231e-01 2.25442834e-02\n",
            "  1.61030596e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 9.50080515e-02 6.44122383e-03 1.61030596e-03\n",
            "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.68012180e-04\n",
            "  6.16603496e-05 6.29421687e-06 1.25956323e-06 0.00000000e+00\n",
            "  6.91860534e-04 1.96146727e-04 2.25867808e-05 1.18881185e-06\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "y_train sample: [0 0 0 0 0]\n",
            "X_val sample: [[9.97050147e-01 2.06896552e-01 2.72727273e-01 7.67782427e-01\n",
            "  1.31434830e-01 1.64556962e-01 5.21978022e-01 2.42708067e-01\n",
            "  1.32300421e-01 1.43728185e-01 1.52066370e-01 1.30766959e-01\n",
            "  1.66008212e-01 1.53563681e-01 1.33713766e-01 3.77550426e-03\n",
            "  0.00000000e+00 0.00000000e+00 7.27206559e-01 2.72793441e-01\n",
            "  1.81441777e-01 1.73696453e-01 9.51767951e-01 4.83091787e-02\n",
            "  3.22061192e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
            "  0.00000000e+00 4.83091787e-02 3.22061192e-03 1.61030596e-03\n",
            "  1.61030596e-03 1.61030596e-03 0.00000000e+00 4.52978333e-05\n",
            "  1.76172428e-05 3.77653012e-06 1.25956323e-06 1.26076377e-06\n",
            "  1.14121325e-04 3.44742733e-05 4.75511175e-06 1.18881185e-06\n",
            "  1.18885708e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [5.45722714e-01 4.36781609e-01 7.27272727e-01 8.93305439e-01\n",
            "  1.09529025e-01 4.81012658e-01 2.33516483e-01 2.77116639e-03\n",
            "  1.00269372e-01 2.39464479e-01 1.23006605e-01 2.81056914e-01\n",
            "  2.33238220e-01 2.54178520e-02 1.07165168e-09 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 9.73348457e-01 2.66515430e-02\n",
            "  3.30719630e-01 2.45065668e-01 1.76361306e-01 1.12721417e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 8.37359098e-02 6.44122383e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.06827390e-03\n",
            "  4.01421460e-04 4.40595181e-05 6.29781617e-06 0.00000000e+00\n",
            "  1.39917500e-03 7.75076765e-04 3.34046600e-04 1.17692374e-04\n",
            "  4.16099978e-05 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [4.60176991e-01 6.55172414e-01 1.81818182e-01 8.93305439e-01\n",
            "  1.97152245e-01 7.21518987e-01 4.97252747e-01 1.63255940e-01\n",
            "  1.39262006e-01 1.19617491e-01 1.47182711e-01 1.61158028e-01\n",
            "  1.38162852e-01 1.90151183e-01 1.18234783e-01 3.30351269e-02\n",
            "  0.00000000e+00 0.00000000e+00 7.05833512e-01 2.94166488e-01\n",
            "  1.01210245e-02 1.46835223e-01 6.67439710e-01 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 3.22061192e-02 3.22061192e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.14568286e-05\n",
            "  1.25837448e-05 2.51768675e-06 1.25956323e-06 0.00000000e+00\n",
            "  1.90202209e-05 1.18876804e-05 2.37755587e-06 1.18881185e-06\n",
            "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.97050147e-01 6.66666667e-01 5.90909091e-01 3.68200837e-01\n",
            "  1.09529025e-01 7.34177215e-01 1.62087912e-01 1.08908135e-02\n",
            "  2.16366125e-01 1.13488507e-01 1.23033338e-01 1.71414735e-01\n",
            "  2.16795484e-01 1.05972186e-01 6.11069600e-02 8.56485181e-03\n",
            "  0.00000000e+00 0.00000000e+00 8.35892912e-01 1.64107088e-01\n",
            "  3.95060286e-02 5.89526150e-02 8.07160742e-01 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 2.25442834e-02 3.22061192e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.04436671e-04\n",
            "  5.53684772e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.43840420e-04 5.34945620e-05 2.61531146e-05 8.32168298e-06\n",
            "  8.32199956e-06 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]\n",
            " [9.97050147e-01 3.21839080e-01 4.09090909e-01 9.64435146e-01\n",
            "  2.19058050e-01 2.53164557e-02 1.00000000e+00 2.29509244e-01\n",
            "  1.63082041e-01 1.40086460e-01 1.39289343e-01 1.38127801e-01\n",
            "  1.53154772e-01 1.65004740e-01 1.13585601e-01 1.30020308e-02\n",
            "  6.46187083e-03 3.76099569e-03 7.31851144e-01 2.68148856e-01\n",
            "  6.63467893e-02 1.46370682e-01 8.87515952e-01 9.17874396e-02\n",
            "  1.44927536e-02 8.05152979e-03 3.22061192e-03 1.61030596e-03\n",
            "  1.61030596e-03 1.33655395e-01 9.66183575e-03 3.22061192e-03\n",
            "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.90661097e-04\n",
            "  1.04445082e-04 2.14003374e-05 7.55737940e-06 1.26076377e-06\n",
            "  3.34042629e-04 1.22443109e-04 1.66428911e-05 4.75524742e-06\n",
            "  1.18885708e-06 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00]]\n",
            "y_val sample: [0 0 0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.64251 | train_accuracy: 0.17378 | val_accuracy: 0.17856 |  0:00:04s\n",
            "epoch 1  | loss: 0.58654 | train_accuracy: 0.25545 | val_accuracy: 0.25669 |  0:00:08s\n",
            "epoch 2  | loss: 0.56904 | train_accuracy: 0.39398 | val_accuracy: 0.39744 |  0:00:13s\n",
            "epoch 3  | loss: 0.56708 | train_accuracy: 0.45408 | val_accuracy: 0.4575  |  0:00:17s\n",
            "epoch 4  | loss: 0.56379 | train_accuracy: 0.57734 | val_accuracy: 0.57788 |  0:00:22s\n",
            "epoch 5  | loss: 0.55983 | train_accuracy: 0.51839 | val_accuracy: 0.51962 |  0:00:26s\n",
            "epoch 6  | loss: 0.55754 | train_accuracy: 0.61255 | val_accuracy: 0.61325 |  0:00:30s\n",
            "epoch 7  | loss: 0.54772 | train_accuracy: 0.6257  | val_accuracy: 0.62606 |  0:00:35s\n",
            "epoch 8  | loss: 0.53942 | train_accuracy: 0.63492 | val_accuracy: 0.63256 |  0:00:39s\n",
            "epoch 9  | loss: 0.53525 | train_accuracy: 0.63497 | val_accuracy: 0.63038 |  0:00:43s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 8 and best_val_accuracy = 0.63256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the datasets:\n",
            "X_train: (64000, 111), y_train: (64000,)\n",
            "X_val: (16000, 111), y_val: (16000,)\n",
            "Data types:\n",
            "X_train type: <class 'numpy.ndarray'>, y_train type: <class 'numpy.ndarray'>\n",
            "X_val type: <class 'numpy.ndarray'>, y_val type: <class 'numpy.ndarray'>\n",
            "Sample data:\n",
            "X_train sample: [[1.00000000e+00 5.51724138e-01 1.81818182e-01 5.71129707e-01\n",
            "  2.19058050e-01 1.01265823e-01 4.80769231e-01 6.88778598e-02\n",
            "  1.51378842e-01 1.67734049e-01 1.68922553e-01 1.75996906e-01\n",
            "  1.71306277e-01 1.38587015e-01 3.52012753e-02 4.81308685e-03\n",
            "  0.00000000e+00 0.00000000e+00 8.38064741e-01 1.61935259e-01\n",
            "  2.26579899e-01 2.14340683e-02 8.99891509e-01 1.77133655e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 5.47504026e-02 3.22061192e-03 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.38410046e-05\n",
            "  1.25837448e-06 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  6.89483007e-05 2.02090567e-05 2.37755587e-06 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 9.26407181e-01]\n",
            " [9.97050147e-01 2.75862069e-01 1.81818182e-01 9.22594142e-01\n",
            "  2.19058050e-01 2.78481013e-01 2.91208791e-01 3.21908784e-02\n",
            "  1.61729234e-01 2.23668297e-01 1.94913279e-01 1.36367767e-01\n",
            "  1.27241753e-01 6.36835341e-02 9.89229641e-02 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.47309517e-01 1.52690483e-01\n",
            "  3.35945700e-01 2.30977847e-01 8.28980552e-01 1.72302738e-01\n",
            "  6.44122383e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
            "  0.00000000e+00 5.12077295e-01 4.83091787e-02 1.77133655e-02\n",
            "  6.44122383e-03 1.61030596e-03 1.61030596e-03 9.56287591e-05\n",
            "  4.65598558e-05 2.51768675e-06 0.00000000e+00 0.00000000e+00\n",
            "  1.45029184e-04 5.11170259e-05 1.18877794e-05 1.18881185e-06\n",
            "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.46212541e-01]\n",
            " [1.00000000e+00 4.36781609e-01 3.18181818e-01 7.63598326e-01\n",
            "  1.09529025e-01 3.03797468e-01 9.03846154e-01 1.50168835e-01\n",
            "  1.56833974e-01 1.49720028e-01 1.37540583e-01 1.25019350e-01\n",
            "  1.45524129e-01 1.46327483e-01 1.51219087e-01 5.20883266e-03\n",
            "  3.85773525e-03 0.00000000e+00 7.16261158e-01 2.83738842e-01\n",
            "  7.46011488e-02 1.51030330e-01 8.84170338e-01 2.99516908e-01\n",
            "  1.61030596e-03 1.61030596e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 9.33977456e-02 1.61030596e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 6.37944485e-04\n",
            "  2.56708394e-04 5.41302651e-05 1.51147588e-05 3.78229131e-06\n",
            "  6.21723470e-04 2.80549258e-04 5.58725630e-05 1.78321778e-05\n",
            "  8.32199956e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.08797633e+00]\n",
            " [9.97050147e-01 3.33333333e-01 5.90909091e-01 2.92887029e-01\n",
            "  1.09529025e-01 1.51898734e-01 5.98901099e-01 2.12848667e-02\n",
            "  1.34178910e-01 1.06856154e-01 7.79606660e-02 2.46519486e-01\n",
            "  2.28616446e-01 1.89204283e-01 2.85303522e-02 6.09986523e-03\n",
            "  0.00000000e+00 0.00000000e+00 7.96332754e-01 2.03667246e-01\n",
            "  1.48465157e-02 2.39162934e-01 3.84533512e-01 8.05152979e-02\n",
            "  2.25442834e-02 9.66183575e-03 6.44122383e-03 3.22061192e-03\n",
            "  1.61030596e-03 1.17552335e-01 2.57648953e-02 1.12721417e-02\n",
            "  6.44122383e-03 6.44122383e-03 3.22061192e-03 5.03309258e-06\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.30764018e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 7.23276640e-01]\n",
            " [9.97050147e-01 2.64367816e-01 5.00000000e-01 9.33054393e-01\n",
            "  9.85761227e-02 2.65822785e-01 6.64835165e-01 5.97130850e-02\n",
            "  1.21734838e-01 1.53928355e-01 1.73023920e-01 1.66018259e-01\n",
            "  1.44908021e-01 1.40636496e-01 1.10681570e-01 1.66440297e-02\n",
            "  4.87997074e-03 0.00000000e+00 7.60000000e-01 2.40000000e-01\n",
            "  7.24207967e-02 1.95531030e-01 7.63000231e-01 2.25442834e-02\n",
            "  1.61030596e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 9.50080515e-02 6.44122383e-03 1.61030596e-03\n",
            "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.68012180e-04\n",
            "  6.16603496e-05 6.29421687e-06 1.25956323e-06 0.00000000e+00\n",
            "  6.91860534e-04 1.96146727e-04 2.25867808e-05 1.18881185e-06\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.35832085e-01]]\n",
            "y_train sample: [0 0 0 0 0]\n",
            "X_val sample: [[9.97050147e-01 2.06896552e-01 2.72727273e-01 7.67782427e-01\n",
            "  1.31434830e-01 1.64556962e-01 5.21978022e-01 2.42708067e-01\n",
            "  1.32300421e-01 1.43728185e-01 1.52066370e-01 1.30766959e-01\n",
            "  1.66008212e-01 1.53563681e-01 1.33713766e-01 3.77550426e-03\n",
            "  0.00000000e+00 0.00000000e+00 7.27206559e-01 2.72793441e-01\n",
            "  1.81441777e-01 1.73696453e-01 9.51767951e-01 4.83091787e-02\n",
            "  3.22061192e-03 1.61030596e-03 1.61030596e-03 0.00000000e+00\n",
            "  0.00000000e+00 4.83091787e-02 3.22061192e-03 1.61030596e-03\n",
            "  1.61030596e-03 1.61030596e-03 0.00000000e+00 4.52978333e-05\n",
            "  1.76172428e-05 3.77653012e-06 1.25956323e-06 1.26076377e-06\n",
            "  1.14121325e-04 3.44742733e-05 4.75511175e-06 1.18881185e-06\n",
            "  1.18885708e-06 0.00000000e+00 1.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 8.34854629e-01]\n",
            " [5.45722714e-01 4.36781609e-01 7.27272727e-01 8.93305439e-01\n",
            "  1.09529025e-01 4.81012658e-01 2.33516483e-01 2.77116639e-03\n",
            "  1.00269372e-01 2.39464479e-01 1.23006605e-01 2.81056914e-01\n",
            "  2.33238220e-01 2.54178520e-02 1.07165168e-09 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 9.73348457e-01 2.66515430e-02\n",
            "  3.30719630e-01 2.45065668e-01 1.76361306e-01 1.12721417e-02\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 8.37359098e-02 6.44122383e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.06827390e-03\n",
            "  4.01421460e-04 4.40595181e-05 6.29781617e-06 0.00000000e+00\n",
            "  1.39917500e-03 7.75076765e-04 3.34046600e-04 1.17692374e-04\n",
            "  4.16099978e-05 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 6.73765138e-01]\n",
            " [4.60176991e-01 6.55172414e-01 1.81818182e-01 8.93305439e-01\n",
            "  1.97152245e-01 7.21518987e-01 4.97252747e-01 1.63255940e-01\n",
            "  1.39262006e-01 1.19617491e-01 1.47182711e-01 1.61158028e-01\n",
            "  1.38162852e-01 1.90151183e-01 1.18234783e-01 3.30351269e-02\n",
            "  0.00000000e+00 0.00000000e+00 7.05833512e-01 2.94166488e-01\n",
            "  1.01210245e-02 1.46835223e-01 6.67439710e-01 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 3.22061192e-02 3.22061192e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.14568286e-05\n",
            "  1.25837448e-05 2.51768675e-06 1.25956323e-06 0.00000000e+00\n",
            "  1.90202209e-05 1.18876804e-05 2.37755587e-06 1.18881185e-06\n",
            "  1.18885708e-06 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 7.04526017e-01]\n",
            " [9.97050147e-01 6.66666667e-01 5.90909091e-01 3.68200837e-01\n",
            "  1.09529025e-01 7.34177215e-01 1.62087912e-01 1.08908135e-02\n",
            "  2.16366125e-01 1.13488507e-01 1.23033338e-01 1.71414735e-01\n",
            "  2.16795484e-01 1.05972186e-01 6.11069600e-02 8.56485181e-03\n",
            "  0.00000000e+00 0.00000000e+00 8.35892912e-01 1.64107088e-01\n",
            "  3.95060286e-02 5.89526150e-02 8.07160742e-01 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 2.25442834e-02 3.22061192e-03 1.61030596e-03\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.04436671e-04\n",
            "  5.53684772e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.43840420e-04 5.34945620e-05 2.61531146e-05 8.32168298e-06\n",
            "  8.32199956e-06 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.11122966e-01]\n",
            " [9.97050147e-01 3.21839080e-01 4.09090909e-01 9.64435146e-01\n",
            "  2.19058050e-01 2.53164557e-02 1.00000000e+00 2.29509244e-01\n",
            "  1.63082041e-01 1.40086460e-01 1.39289343e-01 1.38127801e-01\n",
            "  1.53154772e-01 1.65004740e-01 1.13585601e-01 1.30020308e-02\n",
            "  6.46187083e-03 3.76099569e-03 7.31851144e-01 2.68148856e-01\n",
            "  6.63467893e-02 1.46370682e-01 8.87515952e-01 9.17874396e-02\n",
            "  1.44927536e-02 8.05152979e-03 3.22061192e-03 1.61030596e-03\n",
            "  1.61030596e-03 1.33655395e-01 9.66183575e-03 3.22061192e-03\n",
            "  1.61030596e-03 1.61030596e-03 1.61030596e-03 2.90661097e-04\n",
            "  1.04445082e-04 2.14003374e-05 7.55737940e-06 1.26076377e-06\n",
            "  3.34042629e-04 1.22443109e-04 1.66428911e-05 4.75524742e-06\n",
            "  1.18885708e-06 1.00000000e+00 0.00000000e+00 1.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.28540484e+00]]\n",
            "y_val sample: [0 0 0 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0  | loss: 0.68455 | train_accuracy: 0.04211 | val_accuracy: 0.04488 |  0:00:04s\n",
            "epoch 1  | loss: 0.59187 | train_accuracy: 0.0422  | val_accuracy: 0.04494 |  0:00:08s\n",
            "epoch 2  | loss: 0.58674 | train_accuracy: 0.05048 | val_accuracy: 0.05219 |  0:00:13s\n",
            "epoch 3  | loss: 0.58849 | train_accuracy: 0.05091 | val_accuracy: 0.0525  |  0:00:17s\n",
            "epoch 4  | loss: 0.58329 | train_accuracy: 0.393   | val_accuracy: 0.397   |  0:00:21s\n",
            "epoch 5  | loss: 0.57294 | train_accuracy: 0.51016 | val_accuracy: 0.51081 |  0:00:26s\n",
            "epoch 6  | loss: 0.56885 | train_accuracy: 0.61344 | val_accuracy: 0.61469 |  0:00:30s\n",
            "epoch 7  | loss: 0.54927 | train_accuracy: 0.60753 | val_accuracy: 0.611   |  0:00:34s\n",
            "epoch 8  | loss: 0.54174 | train_accuracy: 0.68238 | val_accuracy: 0.68562 |  0:00:39s\n",
            "epoch 9  | loss: 0.53716 | train_accuracy: 0.70473 | val_accuracy: 0.70706 |  0:00:43s\n",
            "Stop training because you reached max_epochs = 10 with best_epoch = 9 and best_val_accuracy = 0.70706\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the model\n",
        "with open('clf_level1.pkl', 'wb') as file:\n",
        "    pickle.dump(clf_level1, file)\n",
        "\n",
        "# Load the model\n",
        "with open('clf_level1.pkl', 'rb') as file:\n",
        "    clf_level1 = pickle.load(file)\n"
      ],
      "metadata": {
        "id": "vOECGDG7uBre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(clf, test_df, feature_cols, response_col='ClaimYN'):\n",
        "    # Ensure all data is numeric\n",
        "    X_test = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "    y_test = test_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "\n",
        "    # Calculate predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'F1-Score: {f1}')\n",
        "    print(f'MCC: {mcc}')\n",
        "    print(f'AUC: {auc}')\n",
        "\n",
        "    # Plot loss and accuracy if available\n",
        "    if hasattr(clf, 'history_'):\n",
        "        history = clf.history_\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        if 'loss' in history:\n",
        "            plt.plot(history['loss'], label='train_loss')\n",
        "        if 'val_loss' in history:\n",
        "            plt.plot(history['val_loss'], label='val_loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if 'accuracy' in history:\n",
        "            plt.plot(history['accuracy'], label='train_accuracy')\n",
        "        if 'val_accuracy' in history:\n",
        "            plt.plot(history['val_accuracy'], label='val_accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Accuracy')\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No training history available for this model.\")\n",
        "\n",
        "def save_model(clf, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(clf, file)\n",
        "\n",
        "def load_model(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "# Save the models\n",
        "save_model(clf_level1, 'clf_level1.pkl')\n",
        "save_model(clf_level2, 'clf_level2.pkl')\n",
        "save_model(clf_level3, 'clf_level3.pkl')\n",
        "\n",
        "# Load and evaluate the models\n",
        "print(\"Evaluation for Level 1:\")\n",
        "clf_level1_loaded = load_model('clf_level1.pkl')\n",
        "evaluate_model(clf_level1_loaded, test_df_level1, feature_cols_level1)\n",
        "\n",
        "print(\"Evaluation for Level 2:\")\n",
        "clf_level2_loaded = load_model('clf_level2.pkl')\n",
        "evaluate_model(clf_level2_loaded, test_df_level2, feature_cols_level2)\n",
        "\n",
        "print(\"Evaluation for Level 3:\")\n",
        "clf_level3_loaded = load_model('clf_level3.pkl')\n",
        "evaluate_model(clf_level3_loaded, test_df_level3, feature_cols_level3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkG0RW2puFmJ",
        "outputId": "d6a571bf-3d7c-4822-f751-4f7c271aebca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for Level 1:\n",
            "Accuracy: 0.7876\n",
            "Recall: 0.5160919540229885\n",
            "Precision: 0.1050046772684752\n",
            "F1-Score: 0.17450446949086668\n",
            "MCC: 0.1572383069760196\n",
            "AUC: 0.7487822881554519\n",
            "No training history available for this model.\n",
            "Evaluation for Level 2:\n",
            "Accuracy: 0.6396\n",
            "Recall: 0.7770114942528735\n",
            "Precision: 0.08790637191157347\n",
            "F1-Score: 0.15794392523364484\n",
            "MCC: 0.17206491317898748\n",
            "AUC: 0.7735803726469227\n",
            "No training history available for this model.\n",
            "Evaluation for Level 3:\n",
            "Accuracy: 0.7061\n",
            "Recall: 0.7160919540229885\n",
            "Precision: 0.0996162456028142\n",
            "F1-Score: 0.17490174059517125\n",
            "MCC: 0.18556337394284583\n",
            "AUC: 0.7767039493844295\n",
            "No training history available for this model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries for evaluation\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(clf, test_df, feature_cols, response_col='ClaimYN'):\n",
        "    # Ensure all data is numeric\n",
        "    X_test = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "    y_test = test_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "\n",
        "    # Calculate predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'F1-Score: {f1}')\n",
        "    print(f'MCC: {mcc}')\n",
        "    print(f'AUC: {auc}')\n",
        "\n",
        "    # Plot loss and accuracy if available\n",
        "    if hasattr(clf, 'history_'):\n",
        "        history = clf.history_\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        if 'loss' in history:\n",
        "            plt.plot(history['loss'], label='train_loss')\n",
        "        if 'val_loss' in history:\n",
        "            plt.plot(history['val_loss'], label='val_loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if 'accuracy' in history:\n",
        "            plt.plot(history['accuracy'], label='train_accuracy')\n",
        "        if 'val_accuracy' in history:\n",
        "            plt.plot(history['val_accuracy'], label='val_accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Accuracy')\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No training history available for this model.\")\n",
        "\n",
        "print(\"Evaluation for Level 1:\")\n",
        "evaluate_model(clf_level1, test_df_level1, feature_cols_level1)\n",
        "\n",
        "print(\"Evaluation for Level 2:\")\n",
        "evaluate_model(clf_level2, test_df_level2, feature_cols_level2)\n",
        "\n",
        "print(\"Evaluation for Level 3:\")\n",
        "evaluate_model(clf_level3, test_df_level3, feature_cols_level3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmbmaOrBtjHb",
        "outputId": "22f6cf7d-c041-4ecf-bae2-3696d436b188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for Level 1:\n",
            "Accuracy: 0.8547\n",
            "Recall: 0.5333333333333333\n",
            "Precision: 0.15654520917678813\n",
            "F1-Score: 0.24204486176317164\n",
            "MCC: 0.23116406883165821\n",
            "AUC: 0.7831592371613461\n",
            "No training history available for this model.\n",
            "Evaluation for Level 2:\n",
            "Accuracy: 0.8496\n",
            "Recall: 0.5264367816091954\n",
            "Precision: 0.14996725605762934\n",
            "F1-Score: 0.2334352701325178\n",
            "MCC: 0.22157964434883623\n",
            "AUC: 0.7805994376047731\n",
            "No training history available for this model.\n",
            "Evaluation for Level 3:\n",
            "Accuracy: 0.84325\n",
            "Recall: 0.5436781609195402\n",
            "Precision: 0.14730613516038618\n",
            "F1-Score: 0.23180592991913748\n",
            "MCC: 0.22255822273699005\n",
            "AUC: 0.7800172143410783\n",
            "No training history available for this model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries for evaluation\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, matthews_corrcoef, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_model(clf, test_df, feature_cols, response_col='ClaimYN'):\n",
        "    # Ensure all data is numeric\n",
        "    X_test = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "    y_test = test_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "\n",
        "    # Calculate predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'F1-Score: {f1}')\n",
        "    print(f'MCC: {mcc}')\n",
        "    print(f'AUC: {auc}')\n",
        "\n",
        "    # Plot loss and accuracy if available\n",
        "    if hasattr(clf, 'history_'):\n",
        "        history = clf.history_\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        if 'loss' in history:\n",
        "            plt.plot(history['loss'], label='train_loss')\n",
        "        if 'val_loss' in history:\n",
        "            plt.plot(history['val_loss'], label='val_loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if 'train_accuracy' in history:\n",
        "            plt.plot(history['train_accuracy'], label='train_accuracy')\n",
        "        if 'val_accuracy' in history:\n",
        "            plt.plot(history['val_accuracy'], label='val_accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Accuracy')\n",
        "\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No training history available for this model.\")\n",
        "\n",
        "print(\"Evaluation for Level 1:\")\n",
        "evaluate_model(clf_level1, test_df_level1, feature_cols_level1)\n",
        "\n",
        "print(\"Evaluation for Level 2:\")\n",
        "evaluate_model(clf_level2, test_df_level2, feature_cols_level2)\n",
        "\n",
        "print(\"Evaluation for Level 3:\")\n",
        "evaluate_model(clf_level3, test_df_level3, feature_cols_level3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uXLYZM2VdG4",
        "outputId": "2a971bd1-74d7-44c3-814a-6e1796ac89d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation for Level 1:\n",
            "Accuracy: 0.8547\n",
            "Recall: 0.5333333333333333\n",
            "Precision: 0.15654520917678813\n",
            "F1-Score: 0.24204486176317164\n",
            "MCC: 0.23116406883165821\n",
            "AUC: 0.7831592371613461\n",
            "No training history available for this model.\n",
            "Evaluation for Level 2:\n",
            "Accuracy: 0.8496\n",
            "Recall: 0.5264367816091954\n",
            "Precision: 0.14996725605762934\n",
            "F1-Score: 0.2334352701325178\n",
            "MCC: 0.22157964434883623\n",
            "AUC: 0.7805994376047731\n",
            "No training history available for this model.\n",
            "Evaluation for Level 3:\n",
            "Accuracy: 0.84325\n",
            "Recall: 0.5436781609195402\n",
            "Precision: 0.14730613516038618\n",
            "F1-Score: 0.23180592991913748\n",
            "MCC: 0.22255822273699005\n",
            "AUC: 0.7800172143410783\n",
            "No training history available for this model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "def evaluate_model(clf, test_df, feature_cols, response_col='ClaimYN', model_save_path=None, metrics_save_path=None):\n",
        "    # Ensure all data is numeric\n",
        "    X_test = test_df[feature_cols].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "    y_test = test_df[response_col].apply(pd.to_numeric, errors='coerce').fillna(0).values.astype(np.float32)\n",
        "\n",
        "    # Calculate predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n",
        "\n",
        "    # Create a DataFrame for metrics\n",
        "    metrics_df = pd.DataFrame({\n",
        "        'Metric': ['Accuracy', 'Recall', 'Precision', 'F1-Score', 'MCC', 'AUC'],\n",
        "        'Value': [accuracy, recall, precision, f1, mcc, auc]\n",
        "    })\n",
        "\n",
        "    # Save metrics to Google Drive\n",
        "    if metrics_save_path:\n",
        "        metrics_df.to_csv(metrics_save_path, index=False)\n",
        "        print(f\"Metrics saved to {metrics_save_path}\")\n",
        "\n",
        "    # Save the model to Google Drive\n",
        "    if model_save_path:\n",
        "        with open(model_save_path, 'wb') as model_file:\n",
        "            pickle.dump(clf, model_file)\n",
        "        print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "    # Print metrics\n",
        "    print(f'Accuracy: {accuracy}')\n",
        "    print(f'Recall: {recall}')\n",
        "    print(f'Precision: {precision}')\n",
        "    print(f'F1-Score: {f1}')\n",
        "    print(f'MCC: {mcc}')\n",
        "    print(f'AUC: {auc}')\n",
        "\n",
        "    # Plot loss and accuracy if available\n",
        "    if hasattr(clf, 'history_'):\n",
        "        history = clf.history_\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        if 'loss' in history:\n",
        "            plt.plot(history['loss'], label='train_loss')\n",
        "        if 'val_loss' in history:\n",
        "            plt.plot(history['val_loss'], label='val_loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Loss Over Epochs')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        if 'train_accuracy' in history:\n",
        "            plt.plot(history['train_accuracy'], label='train_accuracy')\n",
        "        if 'val_accuracy' in history:\n",
        "            plt.plot(history['val_accuracy'], label='val_accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.title('Accuracy Over Epochs')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No training history available for this model.\")\n",
        "\n",
        "# Example usage\n",
        "metrics_save_path_level1 = '/content/drive/My Drive/metrics_level1.csv'\n",
        "model_save_path_level1 = '/content/drive/My Drive/model_level1.pkl'\n",
        "evaluate_model(clf_level1, test_df_level1, feature_cols_level1, metrics_save_path=metrics_save_path_level1, model_save_path=model_save_path_level1)\n",
        "\n",
        "metrics_save_path_level2 = '/content/drive/My Drive/metrics_level2.csv'\n",
        "model_save_path_level2 = '/content/drive/My Drive/model_level2.pkl'\n",
        "evaluate_model(clf_level2, test_df_level2, feature_cols_level2, metrics_save_path=metrics_save_path_level2, model_save_path=model_save_path_level2)\n",
        "\n",
        "metrics_save_path_level3 = '/content/drive/My Drive/metrics_level3.csv'\n",
        "model_save_path_level3 = '/content/drive/My Drive/model_level3.pkl'\n",
        "evaluate_model(clf_level3, test_df_level3, feature_cols_level3, metrics_save_path=metrics_save_path_level3, model_save_path=model_save_path_level3)"
      ],
      "metadata": {
        "id": "SNcDUPQ_bAmx",
        "outputId": "4e7a184c-edc7-4e25-84ee-d8a2c8db4255",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to /content/drive/My Drive/metrics_level1.csv\n",
            "Model saved to /content/drive/My Drive/model_level1.pkl\n",
            "Accuracy: 0.7876\n",
            "Recall: 0.5160919540229885\n",
            "Precision: 0.1050046772684752\n",
            "F1-Score: 0.17450446949086668\n",
            "MCC: 0.1572383069760196\n",
            "AUC: 0.7487822881554519\n",
            "No training history available for this model.\n",
            "Metrics saved to /content/drive/My Drive/metrics_level2.csv\n",
            "Model saved to /content/drive/My Drive/model_level2.pkl\n",
            "Accuracy: 0.7916\n",
            "Recall: 0.6045977011494252\n",
            "Precision: 0.12091954022988506\n",
            "F1-Score: 0.20153256704980846\n",
            "MCC: 0.20010176978151709\n",
            "AUC: 0.7745766113284184\n",
            "No training history available for this model.\n",
            "Metrics saved to /content/drive/My Drive/metrics_level3.csv\n",
            "Model saved to /content/drive/My Drive/model_level3.pkl\n",
            "Accuracy: 0.798\n",
            "Recall: 0.6011494252873564\n",
            "Precision: 0.12405123339658444\n",
            "F1-Score: 0.20566260322453794\n",
            "MCC: 0.20409240530069214\n",
            "AUC: 0.7800603853849344\n",
            "No training history available for this model.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8990/C0k2OapqfW+z8CSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}